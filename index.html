<!DOCTYPE html>
<html prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article# " lang="en">
<head>
<meta charset="utf-8">
<meta name="description" content="lvh's blog">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>lvh</title>
<link href="assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<meta content="#5670d4" name="theme-color">
<link rel="alternate" type="application/rss+xml" title="RSS" href="rss.xml">
<link rel="canonical" href="https://www.lvh.io/index.html">
<link rel="next" href="index-5.html" type="text/html">
<!--[if lt IE 9]><script src="assets/js/html5.js"></script><![endif]--><link rel="prefetch" href="posts/2016-rmbp-caveats.html" type="text/html">
</head>
<body>
<a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>

<!-- Menubar -->

<nav class="navbar navbar-inverse navbar-static-top"><div class="container">
<!-- This keeps the margins nice -->
        <div class="navbar-header">
            <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-navbar" aria-controls="bs-navbar" aria-expanded="false">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="https://www.lvh.io/">
                <img src="pinchy.svg" alt="lvh" id="logo"><span id="blog-title">lvh</span>
            </a>
        </div>
<!-- /.navbar-header -->
        <div class="collapse navbar-collapse" id="bs-navbar" aria-expanded="false">
            <ul class="nav navbar-nav">
<li>
<a href="about.html">About</a>
                </li>
<li>
<a href="archive.html">Archive</a>
                </li>
<li>
<a href="categories/index.html">Tags</a>
                </li>
<li>
<a href="talks.html">Talks</a>
                </li>
<li>
<a href="rss.xml">RSS feed</a>

                
            </li>
</ul>
<!-- Custom search --><form method="get" id="search" action="//duckduckgo.com/" class="navbar-form pull-right">
<input type="hidden" name="sites" value="https://www.lvh.io/"><input type="hidden" name="k8" value="#444444"><input type="hidden" name="k9" value="#D51920"><input type="hidden" name="kt" value="h"><input type="text" name="q" maxlength="255" placeholder="Search…" class="span2" style="margin-top: 4px;"><input type="submit" value="DuckDuckGo Search" style="display: none;">
</form>
<!-- End of custom search -->


            <ul class="nav navbar-nav navbar-right"></ul>
</div>
<!-- /.navbar-collapse -->
    </div>
<!-- /.container -->
</nav><!-- End of Menubar --><div class="container" id="content" role="main">
    <div class="body-content">
        <!--Body content-->
        <div class="row">
            
            

    
<div class="postindex">
    <article class="h-entry post-text"><header><h1 class="p-name entry-title"><a href="posts/2016-rmbp-caveats.html" class="u-url">2016 rMBP caveats</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn">
                lvh
            </span></p>
            <p class="dateline"><a href="posts/2016-rmbp-caveats.html" rel="bookmark"><time class="published dt-published" datetime="2016-11-22T07:49:16-08:00" title="2016-11-22 07:49">2016-11-22 07:49</time></a></p>
        </div>
    </header><div class="e-content entry-content">
    <div>
<p>I bought the 2016 15" retina MacBook Pro as soon as it became available. I've
had it for a week now, and there have been some issues you might want to be
aware of if you'd like to get one.</p>
<p>(There are a bunch of links to Amazon in this article. They're not affiliate
links.)</p>
<h2>System Integrity Protection is often disabled</h2>
<p>I noticed <a href="https://twitter.com/schwa/status/799160866209828864">via Twitter</a> that some people were reporting that <a href="https://en.wikipedia.org/wiki/System_Integrity_Protection">System Integrity
Protection (SIP)</a> was disabled by default on their Macs. SIP is a mechanism via
which macOS protects critical system files from being overwritten.</p>
<p>You can check if SIP is enabled on your system by running <code>csrutil status</code> in a
terminal. Sure enough, SIP was disabled for both me and my wife's new rMBPs. To
enable SIP, boot into the recovery mode (hold ⌘-R when booting), open a
terminal, type <code>csrutil enable</code> and reboot.</p>
<p>Perhaps unrelatedly, different out-of-the-box rMBPs appear to have different
builds of OS X Sierra 10.12.1.</p>
<h2>Thunderbolt 2 dongle doesn't work with external screens</h2>
<p>I have a Dell 27" 4k montior (P2715Q). I used it with my previous-generation
rMBP with a DisplayPort-to-mDP2 cable to connect it to its Thunderbolt 2 port.
When buying my laptop, it suggested I get a Thunderbolt 3 to Thunderbolt 2
dongle. I was expecting to get a Thunderbolt 2 port like the one on my previous
Mac. When I plugged it in to my monitor, it told me that there was a cable
plugged in, but no signal coming from the computer.</p>
<p>My understanding was that the Thunderbolt spec implies PCIe lanes and other
protocols over the same port. Specifically, Thunderbolt 2 means 4 PCI Express
2.0 lanes with DisplayPort 1.2; at a cursory glance, <a href="https://en.wikipedia.org/wiki/Thunderbolt_(interface)">Wikipedia agrees</a>.
(Thunderbolt 3 adds HDMI 2.0 and USB 3.1 gen 2.)</p>
<p>I spent about an hour and a half on the phone with AppleCare folks. The Apple
support people were very friendly. (I'm guessing their instructions tell them to
never, under any circumstances, interrupt a customer. It was a little weird.) I
was redirected a few times. They had a variety of suggestions, including:</p>
<ul>
<li>Changing my monitor to MST mode, which shouldn't be necessary for DisplayPort
  1.2-supporting devices, and did nothing but make my monitor not work with my old
  rMBP either. Fortunately I was able to recover via HDMI to my old laptop.</li>
<li>Buying the Apple Digital AV Adapter instead. That adapter used HDMI instead of
  mDP2. That's a significant downgrade; my use of DisplayPort was intentional,
  because DisplayPort 1.2 is the only way I can power the 4K display at 60Hz.
  (The new adapter does not support HDMI 2.0, which is necessary for 4K@60Hz.)</li>
<li>Buying a third-party DisplayPort adapter or dock. This is precarious at best.
  Most existing devices <a href="https://9to5mac.com/2016/11/03/2016-macbook-pro-thunderbolt-compatibility-issues/">don't work with the new rMBP</a>, because they
  use a previous-generation TI chip. There are plenty of docks that wont work,
  by <a href="https://www.amazon.com/StarTech-com-Thunderbolt-Dual-4K-Docking-Station">StarTech</a>, <a href="https://www.amazon.com/Dell-Dock-WD15-Adapter-Type-C">Dell</a>, <a href="https://www.amazon.com/Kensington-Delivery-DisplayPort-Microphone-K38231WW">Kensington</a>
  and <a href="https://www.amazon.com/Plugable-Display-Docking-Charging-Delivery">Plugable</a>. I found one Dock by <a href="https://www.amazon.com/CalDigit-USB-C-Docking-Station-DisplayPort">CalDigit</a> that will
  ostensibly work with the new rMBP, but doesn't supply enough power to charge
  it.</li>
</ul>
<p>Eventually, we found <a href="https://support.apple.com/en-us/HT207266">a KB article</a> that spells out that the Thunderbolt
dongle doesn't work for DisplayPort displays:</p>
<blockquote>
<p>The Thunderbolt 3 (USB-C) to Thunderbolt 2 Adapter doesn't support connections to these devices:</p>
<ul>
<li>Apple DisplayPort display</li>
<li>DisplayPort devices or accessories, such as Mini DisplayPort to HDMI or Mini DisplayPort to VGA adapters</li>
<li>4K Mini DisplayPort displays that don’t have Thunderbolt</li>
</ul>
</blockquote>
<p>I'm a little vindicated by the <a href="http://www.apple.com/shop/reviews/MMEL2AM/A/thunderbolt-3-usb-c-to-thunderbolt-2-adapter">Mac Store</a> review page for the dongle;
apparently I wasn't the only person to expect that. (I was unable to see the
reviews before my purchase, because I purchased it with my Mac, which doesn't
show reviews. Also, the product was brand new at the time, and didn't have these
reviews yet.)</p>
<p><a href="http://www.belkin.com/us/p/P-F4U095/">Belkin</a> and <a href="https://9to5mac.com/2016/11/03/owc-announces-thunderbolt-3-dock-adds-13-ports-of-legacy-io-to-the-new-macbook-pros-over-a-single-cable/">OWC</a> will be shipping docks that allegedly work with
the new rMBP, but Belkin's is currently unavailable with no ship date mentioned,
and OWC claims February 2017.</p>
<h2>WiFi failing with USB-C devices plugged in</h2>
<p>Just as I was going to start writing this post, I noticed that I wasn't able
to sync my blog repository from GitHub:</p>
<pre class="code literal-block"><span></span>Get https://api.github.com/repos/lvh/lvh.github.io: dial tcp 192.30.253.116:443: connect: network is unreachable
</pre>


<p>It didn't click at first what was going on. I restarted my router, connected to
different networks, tried a different machine -- all telling me it was this
laptop that was misbehaving. I started trying everything, and realized I had
recently plugged in my WD backup drive from which I was copying over an SSH key.
It's a USB 3.0 drive that I'm connecting via an AUKEY USB 3 to USB-C converter.
I removed the drive, and my WiFi starts working again. Plugging it back in does
not instantly, but eventually, break WiFi again.</p>
<p>After searching, I was able <a href="https://www.youtube.com/watch?v=NYVjIjBMx6o">to find someone with the same problem</a>.
It is unclear to me if this issue is related to the first-gen TI chip issue
mentioned above. In that video, the authors are also using a USB 3.0 to USB-C
plug, albeit a different one from mine. I don't have a reference USB-C machine
that isn't a new 2016 rMBP to test with. However, this seems plausible, because
the USB 3.0 dongle I purchased from Apple ostensibly works fine.</p>
<p>This does not seem like a reasonable failure mode.</p>
<h2>The escape key, and the new keyboard</h2>
<p>I spend most of my day in Emacs. I'm perfectly happy with the new keyboard. I've
also used the regular MacBook butterfly keyboard, and the new version is
significantly better. I've never had a problem with not having an escape key;
every app where I would've cared to press it had an escape key drawn on the new
Touch Bar. However, not having tactile feedback for the escape key is annoying.
When I was setting up my box and quickly editing a file in vim, I successfully
pressed Escape to exit insert mode -- but I ended up pressing it five times
because I thought I didn't hit it. Apparently the visual feedback vim gives me
that I've exited insert mode is not, actually, what my brain relies on. I'll let
you know if I get used to it.</p>
<h2>Charging</h2>
<p>I'll miss the safety of Magsafe, but being able to plug in your charger on
either side is an unexpected nice benefit.</p>
<h2>Conclusion</h2>
<p>I was ready to accept a transition period of dongles; I bought into it,
literally and figuratively. However, most of the dongles don't actually work,
and that sucks. So, maybe wait for the refresh, or at least until the
high-quality docks are available.</p>
</div>
    </div>
    </article><article class="h-entry post-text"><header><h1 class="p-name entry-title"><a href="posts/crypto-apis-and-jvm-byte-types.html" class="u-url">Crypto APIs and JVM byte types</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn">
                lvh
            </span></p>
            <p class="dateline"><a href="posts/crypto-apis-and-jvm-byte-types.html" rel="bookmark"><time class="published dt-published" datetime="2016-07-11T14:00:00-07:00" title="2016-07-11 14:00">2016-07-11 14:00</time></a></p>
        </div>
    </header><div class="e-content entry-content">
    <div>
<p>In a previous post, I talked about <a href="posts/tradeoffs-in-cryptographic-api-design.html">crypto API tradeoffs</a>. In this
post, I'll go into a specific API design case in <a href="https://github.com/lvh/caesium"><code>caesium</code></a>, my
cryptographic library for Clojure, a language that runs on the Java Virtual
Machine.</p>
<h3>JVM byte types</h3>
<p>The JVM has several standard byte types. For one-shot cryptographic APIs, the
two most relevant ones are byte arrays (also known as <code>byte[]</code>) and
<code>java.nio.ByteBuffer</code>.  Unfortunately, they have different pros and cons, so
there is no unambiguously superior choice.</p>
<p><code>ByteBuffer</code> can produce slices of byte arrays and other byte buffers with
zero-copy semantics. This makes a useful tool when want to place an encrypted
message in a pre-allocated binary format. One example of this is my
<a href="https://github.com/lvh/caesium/blob/master/src/caesium/magicnonce/secretbox.clj">experimental NMR suite</a>. Another use case is generating more than
one key out of a single call to a key derivation function. The call produces
one (long) output, and <code>ByteBuffer</code> lets you slice it into different keys.</p>
<p>Byte arrays are easily serializable, but <code>ByteBuffer</code> is not. Even if you
teach your serialization library about <code>ByteBuffer</code>, this usually results in
extra copying during serialization.</p>
<p>Byte arrays are constant length, and that length is stored with the array, so
it's cheap to access. Figuring out how much to read from a <code>ByteBuffer</code>
requires a (trivial) amount of math by calling <code>remaining</code>. This is because
the <code>ByteBuffer</code> is a view, and it can be looking at a different part of the
underlying memory at different times. For a byte array, this is all fixed: a
byte array's starting and stopping points remain constant. Computing the
remaining length of a <code>ByteBuffer</code> may not always be constant time, although
it probably is. Even if it isn't, it's probably not in a way that is relevant
to the security of the scheme (in <code>caesium</code>, only cryptographic hashes,
detached signatures and detached MACs don't publicly specify the message
length).</p>
<p><code>ByteBuffer</code> has a public API for allocating <em>direct</em> buffers. This means they
are not managed by the JVM. Therefore they won't be copied around by the
garbage collector, and memory pinning is free. "Memory pinning" means that you
notify the JVM that some external C code is using this object, so it should
not be moved around or garbage collected until that code is done using that
buffer. You can't pass "regular" (non-direct) buffers to C code. When you do
that, the buffer is first copied under the hood. Directly allocated buffers
let you securely manage the entire lifecycle of the buffer. For example, they
can be securely zeroed out after use. Directly allocated <code>ByteBuffer</code>
instances might have underlying arrays; this is explicitly unspecified.
Therefore, going back to an array <em>might</em> be zero-copy. In my experiments,
these byte buffers never have underlying arrays, so copying is always
required. I have not yet done further research to determine if this generally
the case. In addition to <code>ByteBuffer</code>, the<code>sun.misc.Unsafe</code> class does have
options for allocating memory directly, but it's pretty clear that use of that
class is strongly discouraged. Outside of the JDK, the <code>Pointer</code> API in
<code>jnr-ffi</code> works identically to <code>ByteBuffer</code>.</p>
<h3>Design decisions</h3>
<p>As a brief recap from my previous post, it's important that we design an API
that makes common things easy and hard things possible while remaining secure
and performant. For the cryptographic APIs in <code>caesium</code>, there are a number of
variables to consider:</p>
<ul>
<li>Are the return types and arguments <code>ByteBuffer</code> instances, byte arrays
   (<code>[B</code>), <code>Pointer</code> instances, or something else?</li>
<li>Is the return type fixed per exposed function, or is the return
   type based on the input types, like Clojure's <a href="https://clojure.github.io/clojure/clojure.core-api.html#clojure.core/empty"><code>empty</code></a>?</li>
<li>Are the APIs "C style" (which passes in the output buffer as an argument)
   or "functional style" (which allocates the output buffer for you)?</li>
<li>Does the implementation convert to the appropriate type (which might
   involve copying), does it use reflection to find the appropriate type, does
   it explicitly dispatch on argument types, or does it assume you give
   it some specific types?</li>
</ul>
<p>Many of these choices are orthogonal, meaning we can choose them
independently. With dozens of exposed functions, half a dozen or so arguments
per function with 2-4 argument types each, two function styles, four argument
conversion styles, and two ways of picking the return type, this easily turns
into a combinatorial explosion of many thousands of exposed functions.</p>
<p>All of these choices pose trade-offs. We've already discussed the differences
between the different byte types, so I won't repeat them here. Having the
function manage the output buffer for you is the most convenient option, but
it also precludes using direct byte buffers effectively. Type conversion is
most convenient, but type dispatch is faster, and statically resolvable
dispatching to the right implementation is faster still. The correct return
value depends on context. Trying to divine what the user really wanted is
tricky, and, as we discussed before, the differences between those types are
significant.</p>
<p>The functions exposed in caesium live on the inside of a bigger system, in the
same sense that IO libraries like <a href="https://twistedmatrix.com/">Twisted</a> and <a href="https://github.com/ztellman/manifold">manifold</a>
live on the edges. Something gives you some bytes, you perform some
cryptographic operations on them, and then the resulting bytes go somewhere
else. This is important, because it reduces the number of contexts in which
people end up with particular types.</p>
<h3>Implementing the API</h3>
<p>One easy decision is that the underlying binding should support every
permutation, regardless of what the API exposes. This would most likely
involve annoying code generation in a regular Java/jnr-ffi project, but
caesium is written in Clojure. The information on how to bind libsodium is a
Clojure data structure that gets compiled into an interface, which is what
jnr-ffi consumes. This makes it easy to expose every permutation, since it's
just some code that operates on a value. You can see this at work in the
<a href="https://github.com/lvh/caesium/blob/master/src/caesium/binding.clj#L13"><code>caesium.binding</code> namespace</a>. As a consequence, an expert
implementer (who knows exactly which underlying function they want to call
with no "smart" APIs or performance overhead) can always just drop down to the
binding layer.</p>
<p>Another easy call is that all APIs should raise exceptions, instead of
returning success codes. Success codes make sense for a C API, because there's
no reasonable exception mechanism available. However, problems like failed
decryption should definitely just raise exceptions.</p>
<p>It gets tricky when we compare APIs that take an output buffer versus APIs
that build the output buffer for you. The latter are clearly the easiest to
use, but the former are necessary for explicit buffer life cycle
management. You can also easily build the managed version from the unmanaged
version, but you can't do the converse. As a consequence, we should expose
both.</p>
<p>Having to expose both has the downside that we haven't put a dent in that
combinatorial explosion of APIs yet. Let's consider the cases in which someone
might have a byte buffer:</p>
<ul>
<li>They're using them as a slice of memory, where the underlying memory could
   be another byte buffer (direct or indirect) or a byte array -- usually a
   byte array wrapping a byte buffer.</li>
<li>They're managing their own (presumably direct) output buffers.</li>
</ul>
<p>In the former case, the byte buffers primarily act as inputs. In the latter,
they exclusively act as outputs. Because both byte buffers and byte arrays can
act as inputs, any API should be flexible in what it accepts. However, this
asymmetry in how the types are used, and how they can be converted, has
consequences for APIs where the caller manages the output buffer versus APIs
that manage it for you.</p>
<p>When the API that manages the output buffer for you, the most reasonable
return type is a byte array. There is no difference between byte arrays
created by the API and those created by the caller, and there's no reasonable
way to reuse them. If you do really need a byte buffer for some reason,
wrapping that output array is simple and cheap. Conversely, APIs where the
caller manages the output buffer should use output byte buffers. Callers who
are managing their own byte buffer need to call an API that supports that, and
there's nothing to be gained from managing your own byte arrays (only direct
byte buffers). This is fine for internal use within <code>caesium</code> — the byte array
producing API can just wrap it in a byte buffer view.</p>
<p>This means we've reduced the surface significantly: APIs with caller-managed
buffers output to <code>ByteBuffer</code>, and APIs that manage it themselves return byte
arrays. This takes care of the output types, but not the input types.</p>
<p>Keys, salts, nonces, messages et cetera will usually be byte arrays, since
they're typically just read directly from a file or made on the spot. However
rare, there can be good reasons for having any of these as byte buffers. For
example, a key might have been generated from a different key using a key
derivation function; a nonce might be synthetically generated (as with
deterministic or nonce-misuse resistant schemes); either might be randomly
generated but just into a pre-existing buffer.</p>
<p>The easiest way for this to work by default is reflection. That mostly works,
until it doesn't. Firstly, reflecting can be brittle. For example, if all of
your byte sequence types are known but a buffer length isn't, Clojure's
reflection will fail to find the appropriate method, even if it is
unambiguous. Secondly, unannotated Clojure fns always take boxed objects, not
primitives, which is what we want for calling into C. Annotating is imperfect,
too, because it moves the onus of producing a primitive to the caller. These
aren't really criticisms of Clojure. At this point we're well into weird edge
case territory which this system wasn't designed for.</p>
<p>We can't do static dispatch for the public API, because we've established that
we should be flexible in our input types. We can work around the unknown type
problems with reflection using explicitly annotated call sites. That means
we're dispatching on types, which comes with its own set of issues. In the
next blog post, I'll go into more detail on how that works, with a bunch of
benchmarks. Stay tuned!</p>
</div>
    </div>
    </article><article class="h-entry post-text"><header><h1 class="p-name entry-title"><a href="posts/tradeoffs-in-cryptographic-api-design.html" class="u-url">Tradeoffs in cryptographic API design</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn">
                lvh
            </span></p>
            <p class="dateline"><a href="posts/tradeoffs-in-cryptographic-api-design.html" rel="bookmark"><time class="published dt-published" datetime="2016-06-18T13:45:35-07:00" title="2016-06-18 13:45">2016-06-18 13:45</time></a></p>
        </div>
    </header><div class="e-content entry-content">
    <div>
<p>Producing cryptographic software is a difficult and specialized endeavor. One
of the pitfalls is that getting it wrong looks exactly like getting it
right. Much like a latent memory corruption bug or a broken distributed
consensus algorithm, a piece of cryptographic software can appear to be
functioning perfectly, while being subtly broken in a way that only comes to
light years later. As the adage goes, attacks never get worse; they only get
better. Implementation concerns like timing attacks can be fiendishly
complicated to solve, involving problems like division instructions on modern
Intel CPUs taking a variable number of cycles depending on the size of the
input. Implementation concerns aren't the only problem; just designing the
APIs themselves is a complex task as well.</p>
<p>Like all API design, cryptographic API design is a user experience
exercise. It doesn't matter how strong or fast your cryptographic software is
if no one uses it. The people who end up with ECB mode didn't end up with it
because they understood what that meant. They got stuck with it because it was
the default and it didn't require thinking about scary parameters like IVs,
nonces, salts and tweaks. Even if someone ended up with CTR or CBC, these APIs
are still precarious; they'll still be vulnerable to issues like nonce
reuse, fixed IV, key-as-IV, unauthenticated encryption...</p>
<p>User experience design always means deep consideration of who your users
are. A particular API might be necessary for a cryptographic engineer to build
new protocols, but that API is probably not a reasonable default encryption
API. An explicit-nonce encryption scheme is great for a record layer protocol
between two peers like TLS, but it's awful for someone trying to encrypt a
session cookie. We can't keep complaining about people getting it wrong when
we keep giving them no chances at getting it right. This is why I'm building
educational material like <a href="https://www.crypto101.io/">Crypto 101</a> and why I care about
cryptography like <a href="posts/nonce-misuse-resistance-101.html">nonce-misuse resistance</a> that's easier to use
correctly.  (The blog post on my new nonce-misuse resistant schemes for
libsodium is coming soon, I promise!)</p>
<p>Before you can make your API easy to use, first you have to worry about
getting it to work at all.</p>
<p>An underlying cryptographic library might expose an unfortunate API. It might
be unwieldy because of historical reasons, backwards compatibility, language
limitations, or even simple oversight. Regardless of why the API is the way it
is, even minute changes to it—a nicer type, an implied parameter—might have
subtle but catastrophic consequences for the security of the final
product. Figuring out if an arbitrary-length integer in your programming
language is interchangeable with other representations, like the
implementation in your crypto library or a <code>char *</code>, has many complex
facets. It doesn't just have to be true under some conditions; ideally, it's
true for every platform your users will run your software on, in perpetuity.</p>
<p>There might be an easy workaround to an annoying API. C APIs often take a
<code>char *</code> together with a length parameter, because C doesn't have a standard
way of passing a byte sequence together with its length. Most higher level
languages, including Java and Python, have byte sequence types that know their
own length. Therefore, you can specify the <code>char *</code> and its associated length
in a single parameter on the high-level side. That's just the moral equivalent
of building a small C struct that holds both. (Whether or not you can trust C
compilers to get anything right at all is a point of contention.)</p>
<p>These problems compound when you are binding libraries in languages and
environments with wildly different semantics. For example, your runtime might
have a relocating garbage collector.  Pointers in C and objects in CPython
stay put, but objects move around all the time in environments like the JVM
(HotSpot) or PyPy. That implies copying to or from a buffer whenever you call
C code, unless the underlying virtual machine supports "memory pinning":
forcing the object to stay put for the duration of the call.</p>
<p>Programmers normally operate in a drastically simplified model of the
world. We praise programming designs for their ability to separate concerns,
so that programmers can deal with one problem at a time. The modern CPU your
code runs on is always an intricate beast, but you don't worry about cache
lines when you're writing a Python program. Only a fraction of programmers
ever has to worry about them at all. Those that do typically only do so after
the program already works so they can still focus on one part of the problem.</p>
<p>When designing cryptographic software, these simplified models we normally
program in don't generally work.  A cryptographic engineer often needs to
worry about concerns all the way up and down the stack simultaneously: from
application layer concerns, to runtime semantics like the
<a href="https://docs.oracle.com/javase/specs/jls/se8/html/index.html">Java Language Specification</a>, to FFI semantics and the C ABI on all
relevant platforms, to the underlying CPU, to the mathematical underpinnings
themselves. The engineer has to manage all of those, often while being
hamstrung by flawed designs like TLS' MAC-then-pad-then-encrypt mess.</p>
<p>In future blog posts, I'll go into more detail about particular cryptographic
API design concerns, starting with JVM byte types. If you're interested, you
should <a href="https://twitter.com/lvh">follow me on Twitter</a> or <a href="rss.xml">subscribe to my blog's feed</a>.</p>
<p><em>Footnote:</em> I'm happy to note that <a href="https://bitbucket.org/cffi/cffi/commits/61e03368485cb78471f701adbfd1bde69a6eaa31">cffi</a> now also has
support for memory pinning since PyPy will support it in the upcoming
5.2 release, although that means I'll no longer be able to make
<a href="https://github.com/reaperhulk">Paul Kehrer of PyCA fame</a> jealous with the pinning support in
<a href="https://github.com/lvh/caesium">caesium</a>.</p>
</div>
    </div>
    </article><article class="h-entry post-text"><header><h1 class="p-name entry-title"><a href="posts/nonce-misuse-resistance-101.html" class="u-url">Nonce misuse resistance 101</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn">
                lvh
            </span></p>
            <p class="dateline"><a href="posts/nonce-misuse-resistance-101.html" rel="bookmark"><time class="published dt-published" datetime="2016-05-19T12:25:44-07:00" title="2016-05-19 12:25">2016-05-19 12:25</time></a></p>
        </div>
    </header><div class="e-content entry-content">
    <div>
<p><em>This post is an introduction to nonce-misused resistant cryptosystems and why
I think they matter. The first part of this post is about nonce-based
authenticated encryption schemes: how they work, and how they fail. If you're
already familiar with them, you can skip to the section on
<a href="posts/nonce-misuse-resistance-101.html#proto">protocol design</a>. If you're completely new to cryptography, you might
like my free introductory course to cryptography, <a href="https://www.crypto101.io">Crypto 101</a>. In a
future blog post, I'll talk about some nonce-misuse resistant schemes I've
implemented using libsodium.</em></p>
<p>Many stream ciphers and stream cipher-like constructions such as CTR,
GCM, (X)Salsa20... take a nonce. You can think of it as a pointer that lets
you jump to a particular point in the keystream. This makes these ciphers
"seekable", meaning that you can decrypt a small part of a big ciphertext,
instead of having to decrypt everything up to that point first. (That ends up
being trickier than it seems, because you still want to authenticate that
small chunk of ciphertext, but that's a topic for another time.)</p>
<p>The critical security property of a nonce is that it's never repeated under
the same key. You can remember this by the mnemonic that a <em>nonce</em> is a
"number used once". If you were to repeat the nonce, the keystream would also
repeat. That means that an attacker can take the two ciphertexts and XOR them
to compute the XOR of the plaintexts. If <code>C_n</code> are ciphertexts, <code>P_n</code>
plaintexts, <code>K_n</code> keystreams, and <code>^</code> is bitwise exclusive or:</p>
<pre class="code literal-block"><span></span>C_1 = K_1 ^ P_1
C_2 = K_2 ^ P_2
</pre>


<p>The attacker just XORs <code>C_1</code> and <code>C_2</code> together:</p>
<pre class="code literal-block"><span></span>C_1 ^ C_2 = K_1 ^ P_1 ^ K_2 ^ P_2
</pre>


<p>Since XOR is commutative (you can rearrange the order), <code>K_1 = K_2</code>, and
XOR'ing two equal values cancels them out:</p>
<pre class="code literal-block"><span></span>C_1 ^ C_2 = P_1 ^ P_2
</pre>


<p>That tells an attacker a lot about the plaintext, especially if some of one of
the plaintexts is predictable. If the attacker has access to an encryption
oracle, meaning that they can get encryptions for plaintexts of their
choosing, they can even get perfect decryptions. That is not an unrealistic
scenario. For example, if you're encrypting session cookies that contain the
user name and e-mail, I can register using a name and e-mail address that has
a lot of <code>Z</code> characters, and then I know that just XORing with <code>Z</code> will reveal
most of the plaintext. For an idea of the state of the art in attacking
two-time pads (the usual term for two ciphertexts with a reused keystream),
see <a href="https://www.cs.jhu.edu/~jason/papers/mason+al.ccs06.pdf">Mason06</a>.</p>
<p><a id="proto"></a></p>
<h3>Protocol design</h3>
<p>For many on-line protocols like TLS, the explicit nonce provides a convenient
way to securely send many messages under a per-session key. Because the
critical security property for a nonce is that it is never repeated with the
same key, it's safe to use a counter. In protocols where both peers send
messages to each other, you can just have one peer use odd nonces and have the
other use even ones. There are some caveats here: for example, if the nonce
size is sufficiently small, an attacker might try to make that counter
overflow, resulting in a repeated nonce.</p>
<p>For off-line (or at-rest) protocols, it's a little trickier. You don't have a
live communication channel to negotiate a new ephemeral key over, so you're
stuck with longer-term keys or keys derived from them. If multiple systems are
participating, you need to decide ahead of time which systems own which
nonces. Even then, systems need to keep track of which nonces they've
used. That doesn't work well, especially not in a distributed system where
nodes and connections can fail at any time. This is why some cryptosystems
like <a href="https://cryptography.io/en/latest/fernet/">Fernet</a> provide an API that doesn't require you to specify
anything besides a key and a message.</p>
<p>One solution is to use randomized nonces. Since nonces can't repeat, random
nonces should be large: if they're too small, you might randomly select the
same nonce twice, per the birthday bound. That is the only difference between
Salsa20 and XSalsa20: Salsa20 has a 64 bit nonce, whereas XSalsa20 has a 192
bit nonce. That change exists explicitly to make random nonces secure.</p>
<p>Picking a random nonce and just prepending it to the secretbox ciphertext is
secure, but there are a few problems with this approach. It's not clear to
practitioners that that's a secure construct. Doing this may seem obvious to a
cryptographer, but not to someone who just wants to encrypt a
message. Prepending a nonce doesn't feel much different from e.g. appending a
MAC. A somewhat knowledgeable practitioner knows that there's plenty of ways
to use MACs that are insecure, and they don't immediately see that the
prefix-nonce construction is secure. Not wanting to design your own
cryptosystems is a good reflex which we should be encouraging.</p>
<p>Random nonces also mean that any system sending messages needs access to
high-quality random number generators while they're sending a message. That's
often, but not always true. Bugs around random number generation, especially
userspace CSPRNGs, <a href="http://sockpuppet.org/blog/2014/02/25/safely-generate-random-numbers/">keep popping up</a>. This is often a consequence of
poor programming practice, but it can also be a consequence of
poorly-configured VMs or limitations of embedded hardware.</p>
<h3>Nonce-misuse resistant systems</h3>
<p>To recap, not all protocols have the luxury of an obvious nonce choice, and
through circumstances or poor practices, nonces might repeat
anyway. Regardless of how cryptographers feel about how important nonce misuse
is, we can anecdotally and empirically verify that such issues are real and
common. This is true even for systems like TLS where there is an "obvious"
nonce available (<a href="https://eprint.iacr.org/2016/475.pdf">Böck et al, 2016</a>). It's easy to point fingers, but
it's better to produce cryptosystems that fail gracefully.</p>
<p><a href="http://web.cs.ucdavis.edu/~rogaway/papers/keywrap.pdf">Rogaway and Shrimpton (2006)</a> defined a new model called nonce-misuse
resistance. Informally, nonce-misuse resistance schemes ensure that a repeated
random nonce doesn't result in plaintext compromise. In the case of a broken
system where the attacker can cause repeated nonces, an attacker will only be
able to discern if a particular message repeated, but they will not be able
to decrypt the message.</p>
<p>Rogaway and Shrimpton also later developed a mode of operation called SIV
(synthetic IV), which Gueron and Lindell are refined to GCM-SIV, a SIV-like
that takes advantage of fast GCM hardware implementations. Those two authors
are currently working with Adam Langley to standardize the AES-GCM-SIV
construction through CFRG. AEZ and HS1-SIV, two entries in the CAESAR
competition, also feature nonce-misuse resistance. CAESAR is an ongoing
competition, and GCM-SIV is not officially finished yet, so this is clearly
a field that is still evolving.</p>
<p>There are parallels between nonce-misuse resistance and length extension
attacks. Both address issues that arguably only affected systems that were
doing it wrong to begin with. (Note, however, in the embedded case above, it
might not be a software design flaw but a hardware limitation.) Fortunately,
the SHA-3 competition showed that you can have increased performance and
still be immune to a class of problems. I'm hopeful that CAESAR will consider
nonce-misuse resistance an important property of an authenticated encryption
standard.</p>
<h3>Repeated messages</h3>
<p>Repeated messages are suboptimal, and in some protocols they might be
unacceptable. However, they're a fail-safe failure mode for nonce
misuse. You're not choosing to have a repeated ciphertext, you're just getting
a repeated ciphertext instead of a plaintext disclosure (where the attacker
would also know that you repeated a message). In the case of a secure random
nonce, a nonce-misuse resistant scheme is just as secure, at the cost of a
performance hit.</p>
<p>In a context where attackers can see individual messages to detect repeated
ciphertexts, it makes sense to also consider a model where attackers can
replay messages. If replaying messages (which presumably have side effects) is
a problem, a common approach is to add a validity timestamp. This is a feature
of <a href="https://cryptography.io/en/latest/fernet/">Fernet</a>, for example. A device that doesn't have access to
sufficient entropy will still typically have access to a reasonably
high-resolution clock, which is still more than good enough to make sure the
synthetic IVs don't repeat either.</p>
<h3>OK, but how does it work?</h3>
<p>Being able to trade plaintext disclosure for attackers being able to detect
repeated messages sounds like magic, but it makes sense once you realize how
they work. As demonstrated in the start of this post, nonce re-use normally
allows an attacker to have two keystreams cancel out. That only makes sense if
two <em>distinct</em> messages are encrypted using the same (key, nonce) pair. NMR
solves this by making the nonce also depend on the message itself. Informally,
it means that a nonce should never repeat for two distinct
messages. Therefore, an attacker can't cancel out the keystreams without
cancelling out the messages themselves as well.</p>
<p>This model does imply off-line operation, in that the entire message has to be
scanned before the nonce can be computed. For some protocols, that may not be
acceptable, although plenty of protocols work around this assumption by simply
making individual messages sufficiently small.</p>
<p><em>Thanks to Aaron Zauner and Kurt Griffiths for proofreading this post.</em></p>
</div>
    </div>
    </article><article class="h-entry post-text"><header><h1 class="p-name entry-title"><a href="posts/supersingular-isogeny-diffie-hellman-101.html" class="u-url">Supersingular isogeny Diffie-Hellman 101</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn">
                lvh
            </span></p>
            <p class="dateline"><a href="posts/supersingular-isogeny-diffie-hellman-101.html" rel="bookmark"><time class="published dt-published" datetime="2016-04-30T09:00:28-07:00" title="2016-04-30 09:00">2016-04-30 09:00</time></a></p>
        </div>
    </header><div class="e-content entry-content">
    <div>
<p>Craig Costello, Patrick Longa and Michael Naehrig, three cryptographers at
Microsoft Research, recently published a <a href="https://eprint.iacr.org/2016/413">paper</a> on supersingular
isogeny Diffie-Hellman. This paper garnered a lot of interest in the security
community and even made it to the front page of Hacker News. Most of the
discussion around it seemed to be how no one understands isogenies, even
within cryptography-literate communities. This article aims to give you a
high-level understanding of what this cryptosystem is and why it works.</p>
<p>This post assumes that you already know how Diffie-Hellman works in the
abstract, and that you know elliptic curves are a mathematical construct that
you can use to perform Diffie-Hellman operations, just like you can with the
integers <em>mod p</em> (that would be "regular" Diffie-Hellman). If that was
gibberish to you and you'd like to know more, check out <a href="https://www.crypto101.io">Crypto 101</a>, my
free introductory book on cryptography. You don't need a math background to
understand those concepts at a high level. The main difference is that Crypto
101 sticks to production cryptography, while this is still experimental.</p>
<p>It's not surprising that isogeny-based cryptography is so confusing. Up until
recently, it was unambiguously in the realm of research, not even close to
being practically applicable. Its mathematical underpinnings are much more
complex than regular elliptic curves, let alone integers <em>mod p</em>. It also
looks superficially similar to elliptic curve Diffie-Hellman, which only adds
to the confusion.</p>
<p>With that, let's begin!</p>
<p><strong>What is this paper about?</strong></p>
<p>Supersingular isogeny Diffie-Hellman (SIDH) is one of a handful of
"post-quantum" cryptosystems. Those are cryptosystems that will remain secure
even if the attacker has access to a large quantum computer. This has nothing
to do with quantum cryptography (for example, quantum key distribution)
beyond their shared quantum mechanical underpinning.</p>
<p><strong>Why should I care about quantum computers?</strong></p>
<p>General quantum computers are not useful as general-purpose computing devices,
but they can solve some problems much faster than classical
computers. Classical computers can emulate quantum computers, but only with
exponential slowdown. A sufficiently large quantum computer could break most
production cryptography, including cryptosystems based on the difficulty of
factoring large numbers (like RSA), taking discrete logs over the integers
<em>mod p</em> (like regular DH), or taking discrete logs over elliptic curves (like
ECDH and ECDSA). To quantify that, consider the following table:</p>
<p><img alt="quantum computer attack cost versus classical" src="img/post-quantum/quantum-computer-relative-cost.png"></p>
<p>In this table, n refers to the modulus size for RSA, and the field size for
ECC. Look at the rightmost column, which represents time taken by the
classical algorithm, and compare it to the "time" columns, which represent how
much a quantum computer would take. As <em>n</em> increases, the amount of time the
quantum computer would take stays in the same ballpark, whereas, for a
classical computer, it increases (almost) exponentially. Therefore, increasing
n is an effective strategy for keeping up with ever-faster classical
computers, but it is ineffective at increasing the run time for a quantum
computer.</p>
<p><strong>Aah! Why isn't everyone panicking about this?!</strong></p>
<p>The good news is that these large quantum computers don't exist yet.</p>
<p>If you look at the qubits column, you'll see that these attacks require large
universal quantum computers. The state of the art in those only has a handful
of qubits. In 2011, IBM successfully factored 143 using a 4-qubit quantum
computer. Scaling the number of qubits up is troublesome. In that light,
larger key sizes may prove effective after all; we simply don't know yet how
hard it is to build quantum computers that big.</p>
<p>D-wave, a quantum computing company, has produced computers with 128 and 512
qubits and even &gt;1000 qubits. While there is some discussion if D-waves
provide quantum speedup or are even real quantum computers at all; there is no
discussion that they are not <em>universal</em> quantum computers. Specifically, they
only claim to solve one particular problem called quantum annealing. The 1000
qubit D-Wave 2X cannot factor RSA moduli of ~512 bits or solve discrete logs
on curves of ~120 bits.</p>
<p>The systems at risk implement asymmetric encryption, signatures, and
Diffie-Hellman key exchanges. That's no accident: all post-quantum
alternatives are asymmetric algorithms. Post-quantum secure symmetric
cryptography is easier: we can just use bigger key sizes, which are still
small enough to be practical and result in fast primitives. Quantum computers
simply halve the security level, so all we need to do to maintain a 128 bit
security level is to use ciphers with 256 bit keys, like Salsa20.</p>
<p>Quantum computers also have an advantage against SIDH, but both are still
exponential in the field size. The SIDH scheme in the new paper has 192 bits
of security against a classical attacker, but still has 128 bits of security
against a quantum attacker. That's in the same ballpark as most symmetric
cryptography, and better than the 2048-bit RSA certificates that underpin the
security of the Internet.</p>
<p><strong>What makes this paper special?</strong></p>
<p>Post-quantum cryptography has been firmly in the realm of academic research
and experiments. This paper makes significant advancements in how practically
applicable SIDH is.</p>
<p><strong>Being future-proof sounds good. If this makes it practical, why don't we
start using it right now?</strong></p>
<p>SIDH is a young cryptosystem in a young field, and hasn't had the same level
of scrutiny as some of the other post-quantum cryptosystems, let alone the
"regular" cryptosystems we use daily. Attacks only get better, they never get
worse. It's possible that SIDH is insecure, and we just don't know how to
break it yet. It does have a good argument for why quantum algorithms wouldn't
be able to crack it (more on that later), but that's a hypothesis, not a
proof.</p>
<p>The new performance figures from this paper are impressive, but this system is
still much slower than the ones we use today. Key generation and key exchange
take a good 50 million cycles or so each. That's about a thousand times slower
than Curve25519, a curve designed about 10 years ago. Key sizes are also much
larger: SIDH public keys are 751 bytes, whereas Curve25519 keys are only 32
bytes. For on-line protocols like HTTPS operating over TCP, that's a
significant cost.</p>
<p>Finally, there are issues with implementing SIDH safely. Systems like
Diffie-Hellman over integers <em>mod p</em> are much less complex than elliptic curve
Diffie-Hellman (ECDH), let alone SIDH. With ECDH and ECC in general, we've
seen new implementation difficulties, especially with early curves. Point
addition formulas would work, unless you were adding a point to itself. You
have to check that input points are on the curve, or leak the secret key
modulo some small order. These are real implementation problems, even though
we know how to solve them.</p>
<p>This is nothing compared to the difficulties implementing SIDH. Currently,
SIDH security arguments rely on honest peers. A peer that gives you a
pathological input can utterly break the security of the scheme. To make
matters worse, while we understand how to verify inputs for elliptic curve
Diffie-Hellman, we don't have a way to verify inputs for isogeny-based
cryptography at all. We don't have much research to fall back on here
either. This isn't a SIDH-specific problem; post-quantum cryptography isn't
mature enough yet to have implementation issues like these nailed down
yet. (For an example from lattice-based cryptography, see the recent paper by
<a href="https://eprint.iacr.org/2016/415">Bindel et al</a>.)</p>
<p>I don't want to diminish the importance of this paper in any way!  Just
because it's not something that your browser is going to be doing tomorrow
doesn't mean it's not an impressive accomplishment. It's just a step on the
path that might lead to production crypto one day.</p>
<p><strong>OK, fine. Why is this so different from elliptic curve Diffie-Hellman?</strong></p>
<p>While SIDH and ECDH both use elliptic curves, they're different beasts. SIDH
generates new curves to perform a DH exchange, whereas ECDH uses points on one
fixed curve. These supersingular curves also have different properties from
regular curves. Using a supersingular curve for regular elliptic curve
operations would be horribly insecure. If you have some background in elliptic
curves: supersingular curves have a tiny embedding degree, meaning that
solving the ECDLP over <code>F(p)</code> can easily be transformed into solving the DLP
over <code>F(p^n)</code> where <code>n</code> is that small embedding degree. Most curves have large
embedding degrees, meaning that solving the ECDLP directly is easier than
translating it into a DLP and then solving that.  You generally have to go out
of your way to find a curve with a small embedding degree. That is only done
in specialized systems, like for pairing-based cryptography, or, as in this
case, supersingular isogeny-based Diffie-Hellman.</p>
<p>Let's recap ECDH. Public keys are points on a curve, and secret keys are
numbers. Alice and Bob agree on the parameters of the exchange ahead of time,
such as the curve <em>E</em> and a generator point <em>P</em> on that curve. Alice picks a
secret integer <em>a</em> and computes her public key <em>aP</em>. Bob picks a secret
integer <em>b</em> and computes his public key <em>bP</em>. Alice and Bob send each other
their public keys, and multiply their secret key by the other peer's public
key. Since <em>abP = baP</em>, they compute the same secret. Since an attacker has
neither secret key, they can't compute the shared secret.</p>
<p>SIDH is different. Secret keys are isogenies...</p>
<p><strong>Whoa whoa whoa. What the heck are isogenies?</strong></p>
<p>An isogeny between elliptic curves is a function from one elliptic curve to
another that preserves base points. That means it takes points on one curve
and returns points on the other curve. Every point on the input curve will map
to a point on the output curve; but multiple points may map to the same
point. Formally speaking, the isogeny is surjective. An isogeny is also a
homomorphism. That is, it preserves the structure of the curve. For any two
points P and Q, <code>phi(P + Q) = phi(P) + phi(Q)</code>.</p>
<p>We have a bunch of formulas for generating isogenies from a curve and a
point. You might remember that the set of values a function takes is its
"domain", and the set of values it returns is called its "codomain". The
domain of such an isogeny is the curve you give it; its codomain might be the
same curve, or it might be a different one. In general, for SIDH, we care
about the case where it produces a new curve.</p>
<p><strong>OK, so explain how SIDH works again.</strong></p>
<p>Roughly speaking, a secret key is an isogeny, and a public key is an elliptic
curve. By "mixing" their isogeny with the peer's public curve, each peer
generates a secret curve. The two peers will generally generate different
curves, but those curves will have the same j-invariant.</p>
<p><strong>Wait, what's a j-invariant?</strong></p>
<p>The j-invariant is a number you can compute for a particular curve. Perhaps
the best analogy would be the discriminant for quadratic equation you might
remember from high school math; it's a single number that tells you something
interesting about the underlying curve. There are different formulas for
curves in different forms. For example, for a curve in short Weierstrass form
<code>y^2 = x^3 + ax + b</code>, the j-invariant is:</p>
<pre class="code literal-block"><span></span>j(E) = (1728 * 4a^3)/(4a^3 + 27b^2)
</pre>


<p>The j-invariant has a few cool properties. For example, while this is the
formula for the short Weierstrass form, the value of j doesn't change if you
put the same curve in a different form. Also, all curves with the same
j-invariant are isomorphic. However, for SIDH you don't really care about
these properties; you just care that the j-invariant is a number you can
compute, and it'll be the same for the two secret curves that are generated by
the DH exchange.</p>
<p><strong>OK, try explaining SIDH again.</strong></p>
<p>The protocol fixes a supersingular curve E and four points on that
curve: P_A, Q_A, P_B, Q_B.</p>
<p>Alice picks two random integers, m_A and n_A. She takes a linear combination
of those two integers with P_A and Q_A to produce a random point R_A, so:</p>
<pre class="code literal-block"><span></span>R_A = n_A * P_A + m_A * Q_A
</pre>


<p>That random point defines Alice's secret isogeny through the isogeny formulas
I talked about above. The codomain of that isogeny forms Alice's public
curve. Alice transforms points P_B and Q_B with the isogeny. She sends Bob her
public curve and the two transformed points.</p>
<p>Bob does the same thing, except with A and B swapped.</p>
<p>Once Alice gets Bob's public key, she applies m_A and n_A again to the
corresponding transformed points she got from Bob. She generates a new isogeny
phiBA from the resulting point just like she did before to generate her
private key. That isogeny's codomain will be an elliptic curve E_BA.</p>
<p>When Bob performs his side of the exchange, he'll produce a different isogeny
and a different elliptic curve E_AB; but it will have the same j-invariant as
the curve Alice computed.  That j-invariant is the shared key.</p>
<p>I've compiled a <a href="https://dl.dropboxusercontent.com/u/38476311/Supersingular%20Isogeny%20Elliptic%20Curve%20Cryptography%20--%20Sage.pdf">transcript</a> of a Diffie-Hellman exchange using
Sage so you can see a (toy!) demo in action.</p>
<p><strong>I know a little about elliptic curves. I thought they were always
non-singular. What's a supersingular elliptic curve but a contradiction in
terms?</strong></p>
<p>You're right! Supersingular elliptic curves are somewhat confusingly
named. Supersingular elliptic curves are still elliptic curves, and they are
non-singular just like all other elliptic curves. The "supersingular" refers
to the singular values of the j-invariant. Equivalently, the Hasse invariant
will be 0.</p>
<p><strong>So, why does it matter that the curve is supersingular?</strong></p>
<p>Firstly, computing the isogeny is much easier on supersingular curves than on
ordinary (not supersingular) elliptic curves. Secondly, if the curve is
ordinary, the scheme can be broken in subexponential time by a quantum
attacker.</p>
<p>Isogeny-based cryptography using ordinary curves was considered as a
post-quantum secure cryptosystem before SIDH. However, Childs et al. showed a
subexponential quantum algorithm in 2010. This paper appeared to have ended
isogeny-based cryptography: it was already slower than other post-quantum
systems, and now it was shown that it wasn't even post-quantum secure.</p>
<p>Because supersingular curves are rare, they had not previously been considered
for isogeny-based cryptography. However, the paper itself suggested that
supersingular curves might be worth examining, so it ended up pushing research
in a new direction rather than ending it.</p>
<p>Explaining why the supersingular curve makes the problem quantum-hard is
tricky without being thoroughly familiar with isogenies and quantum
computing. If you're really interested, <a href="https://arxiv.org/pdf/1012.4019v2.pdf">the Childs paper</a> explains
how the quantum attack in the ordinary case works. Informally, in the ordinary
case, there is a group action (the <em>isogeny star operator</em>) of the ideal class
group onto the set of isomorphism classes of isogenous curves with the same
endomorphism ring. That can be shown to be a special case of the abelian group
hidden shift problem, which can be solved quickly on a quantum computer. In
the supersingular case, there is no such group action to exploit. (If you're
trying to solve for this at home; this is why SIDH needs to define the 4
points P_A, P_B, Q_A, Q_B.)</p>
<p><em>I would like to thank Thomas Ptacek for reviewing this blog post and bearing
with me as I struggle through trying to come up with human-readable
explanations for all of this stuff; Sean Devlin for reminding me that Sage is
an excellent educational tool; and Watson Ladd for pointing out a correction
w.r.t the Hasse invariant (the Hasse-Witt matrix is undefined, not
singular.). Finally, I'd like to thank all the people who reviewed drafts of
this post, including (in no particular order) Bryan Geraghty, Shane Wilton,
Sean Devlin, Thomas Ptacek, Tanner Prynn, Glyph Lefkowitz and Chris Wolfe.</em></p>
</div>
    </div>
    </article><article class="h-entry post-text"><header><h1 class="p-name entry-title"><a href="posts/introducing-teleport.html" class="u-url">Introducing Teleport</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn">
                lvh
            </span></p>
            <p class="dateline"><a href="posts/introducing-teleport.html" rel="bookmark"><time class="published dt-published" datetime="2016-03-12T09:35:56-08:00" title="2016-03-12 09:35">2016-03-12 09:35</time></a></p>
        </div>
    </header><div class="e-content entry-content">
    <div>
<p>I'm happy to introduce <a href="https://github.com/gravitational/teleport">Teleport</a>, a new open source platform for
managing SSH infrastructure. Teleport is built by <a href="http://www.gravitational.com/">Gravitational</a>, a Y
Combinator company that ships SaaS on any platform. While I'm not a part of
Gravitational, I have been advising them on the Teleport project.</p>
<p>Most teams don't have a great authentication story. Some rely on passing
passwords around haphazardly, while others rely on copying everyone's
<code>~/.ssh/id_rsa.pub</code> to every new box. More complex homegrown systems quickly
become unwieldy. These methods are problematic both operationally and from a
security perspective: when security and usability are at odds, security tends
to lose out. For a lot of teams, a single compromised key off of a developer
machine spells disaster, on-boarding new team members is painful, and key
rotation doesn't happen.</p>
<p>In the last few years, strong multi-factor authentication has become the
norm. Tokens are only valid for a brief period of time, use challenge-response
protocols, or both. Teleport helps bring the same level of sophistication to
infrastructure. It helps system administrators leverage the security benefits
of short-lived certificates, while keeping the operational benefits of
decoupling server authentication from user authentication. It lets you run
isolated clusters, so that a compromise of staging credentials doesn't lead to
a compromise in production. It automatically maintains clear audit logs: who
logged in, when and where they logged in, and what they did once they got
there.</p>
<p>Teleport comes with a beautiful, usable UI, making it easy to visualize
different clusters and the available machines within them. The UI is optional:
many system administrators will prefer to use their existing SSH client, and
Teleport supports that natively.  Because it implements the <code>SSH_AUTH_SOCK</code>
protocol, integrating your current CLI workflow is a simple matter of setting
a single environment variable.</p>
<p>As someone with an open-source background, I'm glad to see this software
released and developed out in the open. A decent SSH key management story
should be available to everyone, and that's what Teleport does. I believe
making this technology more accessible is good for everyone, including
commercial vendors. Democratizing a decent DIY story helps turn their product
into the battle-hardened and commercially supported version of industry best
practice; and as such, I hope this helps grow that market. As a principal
engineer at <a href="https://www.rackspace.com/security/">Rackspace Managed Security</a>, I'm excited to start working
towards better authentication stories, both internally and for our customers,
with Teleport as the new baseline.</p>
<p>Releasing early and often is also an important part of open source
culture. That can be at odds with doing due diligence when releasing
security-critical systems like Teleport, especially when those systems have
non-trivial cryptographic components. We feel Teleport is ready to show to the
public now. To make sure we act as responsibly as possible, I've helped the
Teleport team to join forces with a competent independent third-party
auditor. We're not recommending that you bet the farm on Teleport by running
it in production as your only authentication method just yet, but we do think
it's ready for motivated individuals to start experimenting with it.</p>
<p>Some people might feel that a better SSH story means you're solving the wrong
problem. It seems at odds with the ideas behind immutable infrastructure and
treating servers as <a href="https://blog.engineyard.com/2014/pets-vs-cattle">cattle, not pets</a>. I don't think that's
true. Firstly, even with immutable infrastructure, being able to SSH into a
box to debug and monitor is still incredibly important. Being able to rapidly
deploy a bunch of fixed images quickly may be good, but you still have to know
what to fix first. Secondly, existing systems don't always work that way. It
may not be possible, let alone economically rational, to "port" them
effectively. It's easy to think of existing systems as legacy eyesores that
only exist until you can eradicate them, but they do exist, they're typically
here to stay, and they need a real security story, too.</p>
<p>Teleport is still in its early stages. It's usable today, and I'm convinced it
has a bright future ahead of it. It's written in a beautiful, hackable Go
codebase, and <a href="https://github.com/gravitational/teleport">available on Github</a> starting today.</p>
</div>
    </div>
    </article><article class="h-entry post-text"><header><h1 class="p-name entry-title"><a href="posts/dont-expose-the-docker-socket-not-even-to-a-container.html" class="u-url">Don't expose the Docker socket (not even to a container)</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn">
                lvh
            </span></p>
            <p class="dateline"><a href="posts/dont-expose-the-docker-socket-not-even-to-a-container.html" rel="bookmark"><time class="published dt-published" datetime="2015-09-23T14:54:24-07:00" title="2015-09-23 14:54">2015-09-23 14:54</time></a></p>
        </div>
    </header><div class="e-content entry-content">
    <div>
<p>Docker primarily works as a client that communicates with a daemon
process (<code>dockerd</code>). Typically that socket is a UNIX domain socket
called <code>/var/run/docker.sock</code>. That daemon is highly privileged;
effectively having root access. Any process that can write to the
<code>dockerd</code> socket <em>also</em> effectively has root access.</p>
<p>This is no big secret. Docker clearly documents this in a bunch of
places, including the introductory documentation. It's an excellent
reason to use Docker Machine for development purposes, even on
Linux. If your regular user can write to the <code>dockerd</code> socket, then
every code execution vulnerability comes with a free privilege
escalation.</p>
<p>The warnings around the Docker socket typically come with a (sometimes
implicit) context of being on the host to begin with. Write access to
the socket as an unprivileged user on the host may mean privileged
access to the host, but there seems to be some confusion about what
happens when you get write access to the socket <em>from a
container</em>.</p>
<p>The two most common misconceptions seem to be that it either doesn't
grant elevated privileges at all, or that it grants you privileged
access within the container (and without a way to break out). This is
false; write access to the Docker socket is root on the host,
regardless on where that write comes from. This is different from
<a href="https://github.com/jpetazzo/dind">Jerome Pettazoni's <code>dind</code></a>, which gives you Docker-in-Docker;
we're talking about access to the host's Docker socket.</p>
<p>The process works like this:</p>
<ol>
<li>The Docker container gets a <code>docker</code> client of its own, pointed at
   the <code>/var/run/docker.sock</code>.</li>
<li>The Docker container launches a new container mounting <code>/</code> on
   <code>/host</code>. This is the <em>host</em> root filesystem, not the first
   container.</li>
<li>The second container chroots to <code>/host</code>, and is now effectively
   root on the host. (There are a few differences between this and a
   clean login shell; for example, <code>/proc/self/cgroups</code> will still show
   Docker cgroups. However, the attacker has all of the permissions
   necessary to work around this.)</li>
</ol>
<p>This is identical to the process you'd use to escalate from outside of
a container. Write access to the Docker socket is root on the host,
full stop; who's writing, or where they're writing from, doesn't
matter.</p>
<p>Unfortunately, there are plenty of development teams unaware of this
property. I recently came across one, and ended up making a screencast
to unambiguously demonstrate the flaw in their setup (which involved a
container with write access to the Docker socket).</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/CB9Aa6QeRaI" frameborder="0" allowfullscreen></iframe>

<p>This isn't new; it's been a known property of the way Docker works
ever since the (unfortunately trivially cross-site scriptable) REST
API listening on a local TCP port was replaced with the
<code>/var/run/docker.sock</code> UNIX domain socket.</p>
</div>
    </div>
    </article><article class="h-entry post-text"><header><h1 class="p-name entry-title"><a href="posts/queryselectorall-from-an-element-probably-doesnt-do-what-you-think-it-does.html" class="u-url">querySelectorAll from an element probably doesn't do what you think it does</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn">
                lvh
            </span></p>
            <p class="dateline"><a href="posts/queryselectorall-from-an-element-probably-doesnt-do-what-you-think-it-does.html" rel="bookmark"><time class="published dt-published" datetime="2015-08-21T12:11:23-07:00" title="2015-08-21 12:11">2015-08-21 12:11</time></a></p>
        </div>
    </header><div class="e-content entry-content">
    <div>
<p>Modern browsers have APIs called <code>querySelector</code> and <code>querySelectorAll</code>. They
find one or more elements matching a CSS selector. I'm assuming basic
familiarity with CSS selectors: how you select elements, classes and ids. If
you haven't used them, the Mozilla Developer Network has an excellent
<a href="https://developer.mozilla.org/en-US/docs/Web/Guide/CSS/Getting_started/Selectors">introduction</a>.</p>
<p>Imagine the following HTML page:</p>
<pre class="code literal-block"><span></span><span class="cp">&lt;!DOCTYPE html&gt;</span>
<span class="p">&lt;</span><span class="nt">html</span><span class="p">&gt;</span>
<span class="p">&lt;</span><span class="nt">body</span><span class="p">&gt;</span>
    <span class="p">&lt;</span><span class="nt">img</span> <span class="na">id</span><span class="o">=</span><span class="s">"outside"</span><span class="p">&gt;</span>
    <span class="p">&lt;</span><span class="nt">div</span> <span class="na">id</span><span class="o">=</span><span class="s">"my-id"</span><span class="p">&gt;</span>
        <span class="p">&lt;</span><span class="nt">img</span> <span class="na">id</span><span class="o">=</span><span class="s">"inside"</span><span class="p">&gt;</span>
        <span class="p">&lt;</span><span class="nt">div</span> <span class="na">class</span><span class="o">=</span><span class="s">"lonely"</span><span class="p">&gt;&lt;/</span><span class="nt">div</span><span class="p">&gt;</span>
        <span class="p">&lt;</span><span class="nt">div</span> <span class="na">class</span><span class="o">=</span><span class="s">"outer"</span><span class="p">&gt;</span>
            <span class="p">&lt;</span><span class="nt">div</span> <span class="na">class</span><span class="o">=</span><span class="s">"inner"</span><span class="p">&gt;&lt;/</span><span class="nt">div</span><span class="p">&gt;</span>
        <span class="p">&lt;/</span><span class="nt">div</span><span class="p">&gt;</span>
    <span class="p">&lt;/</span><span class="nt">div</span><span class="p">&gt;</span>
<span class="p">&lt;/</span><span class="nt">body</span><span class="p">&gt;</span>
<span class="p">&lt;/</span><span class="nt">html</span><span class="p">&gt;</span>
</pre>


<p><code>document.querySelectorAll("div")</code> returns a <code>NodeList</code> of all of the <code>&lt;div&gt;</code>
elements on the page. <code>document.querySelector("div.lonely")</code> returns that
single lonely div.</p>
<p><code>document</code> supports both <a href="https://developer.mozilla.org/en-US/docs/Web/API/Document/querySelector"><code>querySelector</code></a> and
<a href="https://developer.mozilla.org/en-US/docs/Web/API/Document/querySelectorAll"><code>querySelectorAll</code></a>, letting you find elements in the entire
document. Elements themselves also support both <a href="https://developer.mozilla.org/en-US/docs/Web/API/Element/querySelector"><code>querySelector</code></a> and
<a href="https://developer.mozilla.org/en-US/docs/Web/API/Element/querySelectorAll"><code>querySelectorAll</code></a>, letting you query for elements that are
descendants of that element. For example, the following expression will find
images that are descendants of <code>#my-id</code>:</p>
<pre class="code literal-block"><span></span><span class="nb">document</span><span class="p">.</span><span class="nx">querySelector</span><span class="p">(</span><span class="s2">"#my-id"</span><span class="p">).</span><span class="nx">querySelectorAll</span><span class="p">(</span><span class="s2">"img"</span><span class="p">)</span>
</pre>


<p>In the sample HTML page above, it will find <code>&lt;img id="inside"&gt;</code> but not <code>&lt;img
id="outside"&gt;</code>.</p>
<p>With that in mind, what do these two expressions do?</p>
<pre class="code literal-block"><span></span><span class="nb">document</span><span class="p">.</span><span class="nx">querySelectorAll</span><span class="p">(</span><span class="s2">"#my-id div div"</span><span class="p">);</span>
<span class="nb">document</span><span class="p">.</span><span class="nx">querySelector</span><span class="p">(</span><span class="s2">"#my-id"</span><span class="p">).</span><span class="nx">querySelectorAll</span><span class="p">(</span><span class="s2">"div div"</span><span class="p">);</span>
</pre>


<p>You might reasonably expect them to be equivalent. After all, one asks for
<code>div</code> elements inside <code>div</code> elements inside <code>#my-id</code>, and the other asks for
<code>div</code> elements inside <code>div</code> elements that are <em>descendants</em> of
<code>#my-id</code>. However, when you look at <a href="http://jsbin.com/hineco/edit?html,js,output">this JSbin</a>, you'll see that they
produce very different results:</p>
<pre class="code literal-block"><span></span><span class="nb">document</span><span class="p">.</span><span class="nx">querySelectorAll</span><span class="p">(</span><span class="s2">"#my-id div div"</span><span class="p">).</span><span class="nx">length</span> <span class="o">===</span> <span class="mi">1</span><span class="p">;</span>
<span class="nb">document</span><span class="p">.</span><span class="nx">querySelector</span><span class="p">(</span><span class="s2">"#my-id"</span><span class="p">).</span><span class="nx">querySelectorAll</span><span class="p">(</span><span class="s2">"div div"</span><span class="p">).</span><span class="nx">length</span> <span class="o">===</span> <span class="mi">3</span><span class="p">;</span>
</pre>


<p>What is going on here?</p>
<p>It turns out that <a href="https://developer.mozilla.org/en-US/docs/Web/API/Element/querySelectorAll"><code>element.querySelectorAll</code></a> doesn't match elements
starting from <code>element</code>. Instead, it matches elements matching the query that
are also descendants of <code>element</code>. Therefore, we're seeing three <code>div</code>
elements: <code>div.lonely</code>, <code>div.outer</code>, <code>div.inner</code>. We're seeing them because
they both match the <code>div div</code> selector and are all descendants of <code>#my-id</code>.</p>
<p>The trick to remembering this is that CSS selectors are absolute. They are not
relative to any particular element, not even the element you're calling
<code>querySelectorAll</code> on.</p>
<p>This even works with elements <em>outside</em> the element you're calling
<code>querySelectorAll</code> on. For example, this selector:</p>
<pre class="code literal-block"><span></span><span class="nb">document</span><span class="p">.</span><span class="nx">querySelector</span><span class="p">(</span><span class="s2">"#my-id"</span><span class="p">).</span><span class="nx">querySelector</span><span class="p">(</span><span class="s2">"div div div"</span><span class="p">)</span>
</pre>


<p>... matches <code>div.inner</code> in this snippet (<a href="http://jsbin.com/woropuc/edit?html,js,output">JSbin</a>):</p>
<pre class="code literal-block"><span></span><span class="cp">&lt;!DOCTYPE html&gt;</span>
<span class="p">&lt;</span><span class="nt">html</span><span class="p">&gt;</span>
  <span class="p">&lt;</span><span class="nt">body</span><span class="p">&gt;</span>
    <span class="p">&lt;</span><span class="nt">div</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">div</span> <span class="na">id</span><span class="o">=</span><span class="s">"my-id"</span><span class="p">&gt;</span>
        <span class="p">&lt;</span><span class="nt">div</span> <span class="na">class</span><span class="o">=</span><span class="s">"inner"</span><span class="p">&gt;&lt;/</span><span class="nt">div</span><span class="p">&gt;</span>
      <span class="p">&lt;/</span><span class="nt">div</span><span class="p">&gt;</span>
    <span class="p">&lt;/</span><span class="nt">div</span><span class="p">&gt;</span>
  <span class="p">&lt;/</span><span class="nt">body</span><span class="p">&gt;</span>
<span class="p">&lt;/</span><span class="nt">html</span><span class="p">&gt;</span>
</pre>


<p>I think this API is surprising, and the front-end engineers I've asked seem to
agree with me. This is, however, not a bug. It's how the spec defines it to
work, and browsers consistently implement it that way.
Safari. <a href="http://ejohn.org/blog/thoughts-on-queryselectorall/">John Resig commented</a> how he and others felt this behavior
was quite confusing back when the spec came out.</p>
<p>If you can't easily rewrite the selector to be absolute like we did above,
there are two alternatives: the <code>:scope</code> CSS pseudo-selector, and
<code>query</code>/<code>queryAll</code>.</p>
<p>The <code>:scope</code> pseudo-selector matches against the current scope. The
name comes from the <a href="https://html.spec.whatwg.org/multipage/semantics.html#attr-style-scoped">CSS scoping</a>, which limits the scope
of styles to part of the document. The element we're calling
<code>querySelectorAll</code> on also counts as a scope, so this expression only
matches <code>div.inner</code>:</p>
<pre class="code literal-block"><span></span><span class="nb">document</span><span class="p">.</span><span class="nx">querySelector</span><span class="p">(</span><span class="s2">"#my-id"</span><span class="p">).</span><span class="nx">querySelectorAll</span><span class="p">(</span><span class="s2">":scope div div"</span><span class="p">);</span>
</pre>


<p>Unfortunately, <a href="https://developer.mozilla.org/en-US/docs/Web/CSS/%3Ascope#Browser_compatibility">browser support</a> for scoped CSS and the <code>:scope</code>
pseudo-selector is extremely limited. Only recent versions of Firefox support
it by default. Blink-based browsers like Chrome and Opera require the
well-hidden experimental features flag to be turned on. Safari has a buggy
implementation. Internet Explorer doesn't support it at all.</p>
<p>The other alternative is <code>element.query</code>/<code>queryAll</code>. These are alternative
methods to <code>querySelector</code> and <code>querySelectorAll</code> that exist on DOM parent
nodes. They also take selectors, except these selectors are interpreted
relative to the element being queried from.  Unfortunately, these methods are
even more obscure: they are not referenced on MDN or <code>caniuse.com</code>, and are
missing from the <a href="http://www.w3.org/TR/dom/#interface-parentnode">current DOM4 working draft</a>, dated 18
June 2015. They were still present in <a href="http://www.w3.org/TR/2014/WD-dom-20140204/#interface-parentnode">an older version</a>, dated 4
February 2014, as well as in the <a href="https://dom.spec.whatwg.org/#interface-parentnode">WHATWG Living Document</a> version
of the spec. They have also been implemented by at least two polyfills:</p>
<ul>
<li><a href="https://webreflection.github.io/dom4/">Dom4</a></li>
<li><a href="https://github.com/barberboy/dom-elements">dom-elements</a></li>
</ul>
<p>In conclusion, the DOM spec doesn't always necessarily do the most obvious
thing. It's important to know pitfalls like these, because they're difficult
to discover from just the behavior. Fortunately, you can often rewrite your
selector so that it isn't a problem. If you can't, there's always a polyfill
to give you the modern API you want. Alternatively, libraries like jQuery can
also help you get a consistent, friendly interface for querying the DOM.</p>
</div>
    </div>
    </article><article class="h-entry post-text"><header><h1 class="p-name entry-title"><a href="posts/todays-openssl-bug-for-techies-without-infosec-chops.html" class="u-url">Today's OpenSSL bug (for techies without infosec chops)</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn">
                lvh
            </span></p>
            <p class="dateline"><a href="posts/todays-openssl-bug-for-techies-without-infosec-chops.html" rel="bookmark"><time class="published dt-published" datetime="2015-07-09T08:26:58-07:00" title="2015-07-09 08:26">2015-07-09 08:26</time></a></p>
        </div>
    </header><div class="e-content entry-content">
    <div>
<h2>What happened?</h2>
<p>OpenSSL 1.0.1n+ and 1.0.2b+ had a new feature that allows finding an
alternative certificate chain when the first one fails. The logic in
that feature had a bug in it, such that it didn't properly verify if
the certificates in the alternative chain had the appropriate
permissions; specifically, it didn't check if those certificates are
certificate authorities.</p>
<p>Specifically, this means that an attacker who has a valid certificate
for any domain, can use that certificate to produce new
certificates. Those normally wouldn't work, but the algorithm for
finding the alternative trust chain doesn't check if the valid
certificate can act as a certificate authority.</p>
<h2>What's a certificate (chain)?</h2>
<p>A certificate is a bit like an ID card: it has some information about
you (like your name), and is authenticated by a certificate authority
(in the case of an ID, usually your government).</p>
<h2>What's a certificate authority?</h2>
<p>A certificate authority is an entity that's allowed to authenticate
certificates. Your computer typically ships with the identity of those
certificate authorities, so it knows how to recognize certificates
authorized by them.</p>
<p>In the ID analogy, your computer knows how to recognize photo IDs
issued by e.g. California.</p>
<p>The issue here is that in some cases, OpenSSL was willing to accept
signatures authenticated by certificates that don't have certificate
authority powers. In the analogy, it would mean that it accepted
CostCo cards as valid ID, too.</p>
<h2>Why did they say it wouldn't affect most users?</h2>
<p>This basically means "we're assuming most users are using OpenSSL for
vanilla servers", which is probably true. Most servers do use OpenSSL,
and most clients (browsers) don't.</p>
<p>The bug affects anyone trying to authenticate their peer. That
includes regular clients, and servers doing client
authentication. Regular servers aren't affected, because they don't
authenticate their peer.</p>
<p>Servers doing client authentication are fairly rare. The biggest
concern is with clients. While browsers typically don't use OpenSSL, a
lot of API clients do. For those few people affected by the bug and
with clients that use OpenSSL, the bug is catastrophic.</p>
<h2>What's client authentication?</h2>
<p>The vast majority of TLS connections only authenticate the
server. When the client opens the connection, the server sends its
certificate. The client checks the certificate chain against the list
of certificate authorities that it knows about. The client is
typically authenticated, but over the protocol spoken inside of TLS
(usually HTTP), not at a TLS level.</p>
<p>That isn't the only way TLS can work. TLS also supports authenticating
clients with certificates, just like it authenticates servers. This is
called mutually authenticated TLS, because both peers authenticate
each other. At Rackspace Managed Security, we use this for all
communication between internal nodes. We also operate our own
certificate authority to sign all of those certificates.</p>
<h2>What's TLS?</h2>
<p>TLS is what SSL has been called for way over a decade. The old name
stuck (particularly in the name "OpenSSL"), but you should probably
stop using it when you're talking about the secure protocol, since all
of the versions of the protocol that were called "SSL" have crippling
security bugs.</p>
<h2>Why wasn't this found by automated testing?</h2>
<p>I'm not sure. I wish automated testing this stuff was easier. Since
I'm both a user and a big fan of client authentication, which is a
pretty rare feature, I hope to spend more time in the future creating
easy-to-use automated testing tools for this kind of scenario.</p>
<h2>How big is the window?</h2>
<p>1.0.1n and 1.0.2b were both released on 11 Jun 2015. The fixes, 1.0.1p
and 1.0.2d, were released today, on 9 Jul 2015.</p>
<p>The "good news" is that the bad releases are recent. Most people who
have an affected version will be updating regularly, so the number of
people affected is small.</p>
<p>The bug affected following platforms (non-exhaustive):</p>
<ul>
<li>It did not affect stock OS X, because they still ship
  0.9.8. However, the bug does affect a stable version shipped through
  Homebrew (1.0.2c).</li>
<li>
<a href="http://people.canonical.com/~ubuntu-security/cve/2015/CVE-2015-1793.html">Ubuntu is mostly not affected</a>. The only affected version
  is the unreleased 15.10 (Wily). Ubuntu has already released an
  update for it.</li>
<li>The bug affects stable releases of Fedora. I previously mistakenly
  reported that the contrary, but that information was based on their
  package version numbers, which did not match upstream. Fedora
  backported the faulty logic to their version of 1.0.1k, which was
  available in Fedora 21 and 22. They have since released patches; see
  <a href="https://bugzilla.redhat.com/show_bug.cgi?id=1241544">this ticket</a> for details. Thanks to Major Hayden for the
  correction!</li>
<li>The bug does not affect Debian stable, but it does affect
  <a href="https://security-tracker.debian.org/tracker/CVE-2015-1793s=openssl">testing and unstable</a>.</li>
<li>The bug affects <a href="https://www.archlinux.org/packages/?sort=-last_update">ArchLinux testing</a>.</li>
</ul>
<h2>In conclusion</h2>
<p>The bug is disastrous, but affects few people. If you're running
stable versions of your operating system, you're almost certainly
safe.</p>
<p>The biggest concern is with software developers using OS X. That
audience uses HTTPS APIs frequently, and the clients to connect to
those APIs typically use OpenSSL. OS X comes with 0.9.8zf by default
now, which is a recent revision of an ancient branch. Therefore,
people have a strong motivation to get their OpenSSL from a
third-party source. The most popular source is Homebrew, which up
until earlier this morning shipped 1.0.2c. The bug affects that
version. If you installed OpenSSL through Homebrew, you should go
update right now.</p>
</div>
    </div>
    </article><article class="h-entry post-text"><header><h1 class="p-name entry-title"><a href="posts/they-do-take-security-seriously.html" class="u-url">They do take security seriously</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn">
                lvh
            </span></p>
            <p class="dateline"><a href="posts/they-do-take-security-seriously.html" rel="bookmark"><time class="published dt-published" datetime="2015-07-05T13:17:18-07:00" title="2015-07-05 13:17">2015-07-05 13:17</time></a></p>
        </div>
    </header><div class="e-content entry-content">
    <div>
<p>Earlier today, I read an <a href="http://www.troyhunt.com/2015/07/we-take-security-seriously-otherwise.html">article</a> about the plethora of
information security breaches in recent history. Its title reads:</p>
<blockquote>
<p>“We take security seriously”, otherwise known as “We didn’t take it
seriously enough”</p>
</blockquote>
<p>The article then lists a number of companies informing the public that
they've been breached.</p>
<p>I think this article doesn't just blame the victims of those attacks,
but subjects them to public ridicule. Neither helps anyone, least of
all end users.</p>
<p>I'm surprised to hear such comments from Troy Hunt. He's certainly an
accomplished professional with extensive security experience. This is
not the first time people have expressed similar thoughts; the
<a href="https://news.ycombinator.com/item?id=9834099">HN thread</a> for that article is rife with them.</p>
<p>The explicit assumption is that these companies wouldn't have gotten
in trouble if only they had taken security more seriously. In a world
where the information services store is increasingly valuable and
software increasingly complex, breaches are going to happen. The idea
that getting breached is their own darn fault is unrealistic.</p>
<p>This idea is also counterproductive. Firstly, there's one thing all of
the victims being ostracized have in common: they disclosed the
details of the breach. That is exactly what they should have done;
punishing them creates a perverse incentive for victims to hide
breaches in the future, a decidedly worse end-user outcome.</p>
<p>Secondly, if any breach is as bad as any other breach, there is no
incentive to proactively mitigate damage from future breaches by
hardening internal systems. Why encrypt records, invest in access
control or keep sensitive information in a separate database with
extensive audit logging? It might materially impact end-user security,
but who cares -- all anyone is going to remember is that you got
popped.</p>
<p>Finally, there's a subtle PR issue: how can the security industry
build deep relationships with clients when we publicly ridicule them
when the inevitable happens?</p>
<p>These commentators have presumably not been the victims of a breach
themselves. I have trouble swallowing that anyone who's been through
the terrifying experience of being breached, seeing a breach up close
or even just witnessing a hairy situation being defused could air
those thoughts.</p>
<p>If you haven't been the victim of an attack, and feel that your
security posture is keeping you from becoming one, consider this:</p>
<ol>
<li>What's your threat model?</li>
<li>How confident are you in your estimation of the capabilities of
   attackers?</li>
<li>Would you still be okay if your database became three orders of
   magnitude more valuable? Most personal data's value will scale
   linearly with the number of people affected, so if you're a small
   start-up with growth prospects, you'll either fail to execute, or
   be subject to that scenario.</li>
<li>Would you still be okay if the attacker has a few 0-days?</li>
<li>What if the adversary is a nation-state?</li>
<li>How do you <em>know</em> you haven't been breached?</li>
</ol>
<p>That brings me to my final thesis: I contest the claim that all of the
companies in the article didn't take security seriously. It is far
more probable that all of the companies cited in the article have
expended massive efforts to protect themselves, and, in doing so,
foiled many attacks. It's also possible that they haven't; but the
onus there is certainly on the accuser.</p>
<p>Clearly, that's a weak form of disagreement, since "taking something
seriously" is entirely subjective. However, keep in mind that many
targets <em>actually</em> haven't taken security seriously, and would not
even have the technical sophistication to detect an attack.</p>
<p>(By the way, if you too would like to help materially improve people's
security, we're hiring. Contact me at <code>_@lvh.io</code>.)</p>
</div>
    </div>
    </article>
</div>

        <nav class="postindexpager"><ul class="pager">
<li class="next">
                <a href="index-5.html" rel="next">Older posts</a>
            </li>
        </ul></nav>
</div>
        <!--End of body content-->

        <footer id="footer">
            Contents © 2017         <a href="mailto:_@lvh.io">lvh</a> - Powered by         <a href="http://getnikola.com" rel="nofollow">Nikola</a>         
            
        </footer>
</div>
</div>


            <script src="assets/js/all-nocdn.js"></script><script>$('a.image-reference:not(.islink) img:not(.islink)').parent().colorbox({rel:"gal",maxWidth:"100%",maxHeight:"100%",scalePhotos:true});</script><!-- fancy dates --><script>
    moment.locale("en");
    fancydates(0, "YYYY-MM-DD HH:mm");
    </script><!-- end fancy dates --><script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-36779422-1', 'auto');
  ga('send', 'pageview');

</script>
</body>
</html>
