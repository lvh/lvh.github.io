<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>lvh (crypto)</title><link>http://www.lvh.io/</link><description></description><atom:link href="http://www.lvh.io/categories/crypto.xml" type="application/rss+xml" rel="self"></atom:link><language>en</language><lastBuildDate>Tue, 30 Dec 2014 12:34:36 GMT</lastBuildDate><generator>http://getnikola.com/</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>On discussing software security improvements</title><link>http://www.lvh.io/posts/on-discussing-software-security-improvements.html</link><dc:creator>lvh</dc:creator><description>&lt;div&gt;&lt;p&gt;A common criticism of information security folks is that they tend to
advise people to not do any crypto. Through projects like &lt;a class="reference external" href="https://www.crypto101.io/"&gt;Crypto
101&lt;/a&gt;, I've attempted to make a small contribution towards fixing
that.&lt;/p&gt;
&lt;p&gt;In the open source world, various people often try to improve the
security of a project. Because designing secure systems is pretty
hard, they often produce flawed proposals. The aforemetioned tendency
for infosec-conscious people to tell them to stop doing crypto is
experienced as unwelcoming, even dismissive. Typically, the only thing
that's accomplished is that a lot of feelings get hurt; it seems to
only rarely result in improved software.&lt;/p&gt;
&lt;p&gt;I think that's quite unfortunate. I think open source is great, and we
should be not just welcoming and inclusive, but aiming to produce
secure software. Furthermore, even if a flawed proposal is
unsalvageable, a clear description of &lt;em&gt;why&lt;/em&gt; it is flawed will
presumably result in fewer negative interactions. Best case scenario,
the issues with a proposal can be discussed and potentially rectified.&lt;/p&gt;
&lt;p&gt;In an effort to improve this situation, I'm documenting what I believe
to be a useful way to discuss security changes and their tradeoffs. As
Zooko has taught me:&lt;/p&gt;
&lt;blockquote&gt;
Security isn't about perfect versus imperfect or about better versus
worse, it's about &lt;em&gt;this&lt;/em&gt; attack surface versus &lt;em&gt;that&lt;/em&gt; attack
surface.&lt;/blockquote&gt;
&lt;p&gt;This document aims to be the equivalent of an &lt;a class="reference external" href="http://www.sscce.org/"&gt;SSCCE&lt;/a&gt; for generic bug
reports: a blueprint for making suggestions likely to lead to
productive discourse, as long as we can agree that we're trying to
produce more secure software, as well as provide a welcoming
development environment.&lt;/p&gt;
&lt;div class="section" id="important-points"&gt;
&lt;h2&gt;Important points&lt;/h2&gt;
&lt;p&gt;A good proposal should contain:&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;A brief description of what you're suggesting.&lt;/li&gt;
&lt;li&gt;A description of the attack model you're considering, why the
current system does not address this issue, and why the suggested
system &lt;em&gt;does&lt;/em&gt; address this issue.&lt;/li&gt;
&lt;li&gt;A motivation of the attack model. Why is it important that this
issue is actually addressed?&lt;/li&gt;
&lt;li&gt;How does this change affect the attack surface (i.e. all of the
ways an attacker can attempt to attack a system)?&lt;/li&gt;
&lt;li&gt;What does the user experience for all users of the system look
like? Many cryptosystems fall over because they're simply unusable
for most users of the system.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div class="section" id="an-example"&gt;
&lt;h2&gt;An example&lt;/h2&gt;
&lt;p&gt;Wul (the widely underestimated language, pronounced &lt;em&gt;/wool/&lt;/em&gt;) is a
general purpose programming language. It has a package repository,
WuPI (the Wul package index, pronounced &lt;em&gt;/woopie/&lt;/em&gt;), the de facto
standard for distributing and installing Wul software.&lt;/p&gt;
&lt;p&gt;WuPI uses TLS as a secure transport. The WuF (Wul foundation,
pronounced &lt;em&gt;/woof/&lt;/em&gt;), maintains a root certificate, distributed with
Wul. Thanks to a well-managed system of intermediary CAs run by a
tireless army of volunteers, this means that both package authors and
consumers know they're talking to the real WuPI.&lt;/p&gt;
&lt;p&gt;Alice is the WuPI BDFL. Bob is a Wul programmer, and would like to
improve the security of WuPI.&lt;/p&gt;
&lt;p&gt;While consumers and authors know that they're talking to the real
WuPI, there is no protection against a malicious WuPI endpoint. (This
problem was recently made worse because WuPI introduced a CDN, greatly
increasing the number of people who could own a node.). You know that
you're talking to something with a WuF-signed certificate (presumably
WuPI, provided the WuF has done a good job managing that certificate),
but you have no idea if that thing is being honest about the packages
it serves you.&lt;/p&gt;
&lt;p&gt;Bob believes WuPI could solve this by using GPG signatures.&lt;/p&gt;
&lt;p&gt;He starts with a brief description of the suggestion:&lt;/p&gt;
&lt;blockquote&gt;
I would like to suggest that WuPI grows support for GPG signatures
of packages. These signatures would be created when a package author
uploads a package. They would optionally be verified when the user
downloads a package.&lt;/blockquote&gt;
&lt;p&gt;He continues with the attack model being considered:&lt;/p&gt;
&lt;blockquote&gt;
I believe this would secure WuPI consumers against a malicious WuPI
endpoints. A malicious WuPI endpoint (assuming it acquires an
appropriate certificate) is currently free to deliver whatever
packages it wants.&lt;/blockquote&gt;
&lt;p&gt;He explains why the current model doesn't address this:&lt;/p&gt;
&lt;blockquote&gt;
The current system assures authenticity and secrecy of the stream
(through TLS), and it ensures that the server authenticates itself
with a WuPI/WuF certificate. It does not ensure that the package is
what the author uploaded.&lt;/blockquote&gt;
&lt;p&gt;He explains why he believes his model does address this:&lt;/p&gt;
&lt;blockquote&gt;
Because the signatures are produced by the author's GPG key, a
malicious WuPI endpoint would not be able to forge them. Therefore,
a consumer is sure that a package with a valid signature is indeed
from the author.&lt;/blockquote&gt;
&lt;p&gt;He explains why this attack model is important:&lt;/p&gt;
&lt;blockquote&gt;
With the new CDN support, the number of people with access to such a
certificate has greatly increased. While I certainly trust all of
the volunteers involved, it would be nice if we didn't &lt;em&gt;have&lt;/em&gt; to.
Furthermore, the software on the servers can always be vulnerable to
attack; as a high-value target, it certainly isn't inconceivable
that an attacker would use an unknown vulnerability to take over a
WuPI endpoint.&lt;/blockquote&gt;
&lt;p&gt;He (believes to) address the attack surface:&lt;/p&gt;
&lt;blockquote&gt;
Because the signatures are optional, the attack surface remains the
same.&lt;/blockquote&gt;
&lt;p&gt;Finally, he addresses the user experience:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The weak point of this scheme is most likely the user experience,
because users historically seem to dislike using GPG.&lt;/p&gt;
&lt;p&gt;I am hopeful that this increased value of participating in the GPG
web of trust will mean that more people participate.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Alice reviews this, and notes a flaw in the proposal:&lt;/p&gt;
&lt;blockquote&gt;
This proposal aims to address a security flaw when the WuPI endpoint
is malicious by adding signatures. However, a malicious WuPI
endpoint can lie by omission, and claim a package was never signed
by the author.&lt;/blockquote&gt;
&lt;p&gt;Bob now realizes this issue, and suggests an improvement:&lt;/p&gt;
&lt;blockquote&gt;
This could be rectified if the user insists on a signature for
packages they expect to be signed.&lt;/blockquote&gt;
&lt;p&gt;As a side note, Alice notes that the attack surface does increase:&lt;/p&gt;
&lt;blockquote&gt;
This places trust in author's ability to manage private keys, which
has historically been shown to be problematic. That introduces a new
attack vector: an attacker can attempt to go after the author's
private key.&lt;/blockquote&gt;
&lt;p&gt;Regardless of the outcome of this conversation, there actually was a
conversation. I believe this to be an improvement over the overall
status quo.&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;</description><category>crypto</category><category>python</category><category>security</category><guid>http://www.lvh.io/posts/on-discussing-software-security-improvements.html</guid><pubDate>Tue, 29 Jul 2014 06:58:27 GMT</pubDate></item><item><title>On TrueCrypt and full-disk encryption</title><link>http://www.lvh.io/posts/2014/05/on-truecrypt-and-full-disk-encryption.html</link><dc:creator>lvh</dc:creator><description>&lt;div&gt;&lt;p&gt;Since the early hours of May 29th (CEST), &lt;a href="https://www.truecrypt.org"&gt;the TrueCrypt website
(&lt;code&gt;https://www.truecrypt.org&lt;/code&gt;)&lt;/a&gt; has pointed
to &lt;code&gt;http://truecrypt.sourceforge.net/&lt;/code&gt;, with an ominous-looking error
message:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;WARNING: Using TrueCrypt is not secure as it may contain unfixed
security issues&lt;/p&gt;
&lt;p&gt;This page exists only to help migrate existing data encrypted by
TrueCrypt.&lt;/p&gt;
&lt;p&gt;The development of TrueCrypt was ended in 5/2014 after Microsoft
terminated support of Windows XP. Windows 8/7/Vista and later offer
integrated support for encrypted disks and virtual disk images. Such
integrated support is also available on other platforms (click here
for more information). You should migrate any data encrypted by
TrueCrypt to encrypted disks or virtual disk images supported on
your platform.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The website then explains how you can install BitLocker, a proprietary
disk encryption system available in many versions of Windows, as well
as how you could "rescue" existing TrueCrypt volumes.&lt;/p&gt;
&lt;p&gt;People were pretty unhappy, for a variety of reasons:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Lack of trust in proprietary full-disk encryption software.&lt;/li&gt;
&lt;li&gt;Many versions of Windows didn't even ship with BitLocker, only a few
  "premium" versions did.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As a proprietary system, BitLocker is not susceptible to the same
amount of public scrutiny as a publicly available system. This is in
stark contrast with TrueCrypt, where Matt Green recently raised around
70k USD to perform an &lt;a href="http://istruecryptauditedyet.com/"&gt;audit&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;There are variety of scenarios that could've caused the TrueCypt
website to suddenly sport that message. The live Internet audience has
speculated wildly. I'll share my thoughts on that near the end of this
post, but there are two points I'd like to make that I think are far
more important:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Consider if full-disk encryption is really what you want.&lt;/li&gt;
&lt;li&gt;If it is, consider if it should be TrueCrypt.&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;Full-disk encryption&lt;/h4&gt;
&lt;p&gt;Something I learned from the inimitable &lt;a href="https://zooko.com"&gt;Zooko&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Security isn't about perfect versus imperfect or about better versus
worse, it's about &lt;em&gt;this&lt;/em&gt; attack surface versus &lt;em&gt;that&lt;/em&gt; attack
surface.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;You can't just add more crypto junk to something and expect to somehow
get better security. You have to consider what it is buying you, and
what it's costing you.&lt;/p&gt;
&lt;p&gt;Full-disk encryption buys you one simple thing: if someone steals your
device while the encrypted volume is locked, they probably can't read
it.&lt;/p&gt;
&lt;p&gt;If the encrypted volume is unlocked, it's over; and that's the state
it's probably usually in. If the key lives in RAM (it usually does),
there's a variety of ways that can be extracted. There's devices that
have complete direct memory access, including everything that speaks
FireWire or has FireWire-compatibility built-in. Even if you shut off
your machine, cold boot attacks mean that the key can be extracted for
a limited about of time. Various jurisdictions can try to force you to
hand over the keys.&lt;/p&gt;
&lt;p&gt;If an attacker can write to the (encrypted!) volume, it's probably
over. Virtually all sector-level full-disk encryption formats are
unauthenticated (with good reason), they're all malleable and
vulnerable to (adaptive) chosen-ciphertext attacks.&lt;/p&gt;
&lt;p&gt;If you're encrypting files or blobs of data within other files, an
authenticated encryption scheme like GPG is far more useful.&lt;/p&gt;
&lt;p&gt;Don't get me wrong. Full-disk encryption is a good idea, and you
should do it. I'm just saying that what it actually protects against
is pretty limited. If there's files you want to keep secret, full-disk
encryption is probably not enough.&lt;/p&gt;
&lt;p&gt;For more details, try Thomas &amp;amp; Erin Ptacek's
&lt;a href="http://sockpuppet.org/blog/2014/04/30/you-dont-want-xts/"&gt;blog post&lt;/a&gt;
on why XTS isn't what you want. XTS is a way to build tweakable
ciphers that can be used to create full-disk encryption; but the post
applies to full-disk encryption generically as well.&lt;/p&gt;
&lt;h4&gt;TrueCrypt as a full-disk encryption mechanism&lt;/h4&gt;
&lt;p&gt;So, you probably want full-disk encryption, and you probably want to
make sure that sensitive data is encrypted on top of that. Great. What
full-disk encryption scheme do you use?&lt;/p&gt;
&lt;p&gt;I don't want to bash TrueCrypt. It is (was, perhaps?) a great go-to
project for full-disk encryption. It got a lot of things right. It
also got a bunch of things wrong.&lt;/p&gt;
&lt;p&gt;TrueCrypt was made by a bunch of people we don't know occasionally
throwing a bunch of binaries over the wall. The source is available
(under a non-OSI license), so you could audit that and compile your
own binaries, but the truth is that the vast majority of TrueCrypt
users never actually did that.&lt;/p&gt;
&lt;p&gt;The TrueCrypt disk format and encryption standards are a bit iffy. It
has terrible file system support. It has major performance issues,
partially due to questionable cryptographic practices such as cipher
cascades. It was great for when it was originally conceived and
full-disk encryption was still new and exciting, it's still pretty
decent now, but we can certainly do better.&lt;/p&gt;
&lt;h4&gt;What actually happened to the website?&lt;/h4&gt;
&lt;p&gt;We don't really know. There's a couple of guesses.&lt;/p&gt;
&lt;p&gt;First of all, it looks like it are the original authors that folded:
the key is the same one that was being used to sign releases months
ago. Of course, that could mean that it was compromised or that they
were forced to hand it over.&lt;/p&gt;
&lt;p&gt;Since the DNS records changed, the e-mail server behavior changed, the
same key was used, the Sourceforge client was involved, and fairly
major changes to the source code were involved, it's unlikely that
it's a simple defacement.&lt;/p&gt;
&lt;p&gt;It's unlikely that they folded because they felt discovery of some
backdoor was imminent. Folding wouldn't actually stop that discovery,
because the source was already open.&lt;/p&gt;
&lt;p&gt;It's strange that they would point to alternatives like Bitlocker for
Windows and even more dubious alternatives for other operating
systems. The authors certainly knew that this would not be an
acceptable alternative for the vast majority of their users.&lt;/p&gt;
&lt;p&gt;Additionally, the support window for XP ending isn't a particularly
convincing impetus for migrating away from TrueCrypt &lt;em&gt;right now&lt;/em&gt;. This
has lead some to believe that it's an automated release. Worse, it
could be a gagged response: a big and powerful three-letter agency
might be twisting their arm and forcing them to fold, in return for
something else (like, say, not being thrown in a Gitmo cell and
forgotten about). Again: pure speculation.&lt;/p&gt;
&lt;p&gt;Another option is that some of the developers just really, genuinely
folded. They felt that for atechnical users, BitLocker was good
enough, while the particularly discerning TrueCrypt user would be able
to find some other alternative.&lt;/p&gt;
&lt;p&gt;Bottom line is that none of this really matters. What matters is what
you should do next if you want to have full-disk encryption.&lt;/p&gt;
&lt;h4&gt;So what do I do now?&lt;/h4&gt;
&lt;p&gt;I think LUKS is probably the best system that we have right now, and
one of the few that actually improves on TrueCrypt.&lt;/p&gt;
&lt;p&gt;If you're on Linux, dm-crypt + cryptsetup + LUKS is probably what you
want. If you're on OS X, FileVault 2 is probably what you want. If
you're on Windows, your options are looking a bit thin right now:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Keep using old versions of TrueCrypt.&lt;/li&gt;
&lt;li&gt;Give up and use BitLocker.&lt;/li&gt;
&lt;li&gt;Use a Linux virtual machine to use dm-crypt/LUKS, eventually
  migrating to native support.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Personally, I think the first option is the most reasonable right now,
and then wait until someone actually writes the native LUKS support.&lt;/p&gt;
&lt;p&gt;Oh, and you should probably install GPG to encrypt some of those files
stored on that encrypted volume.&lt;/p&gt;&lt;/div&gt;</description><category>crypto</category><guid>http://www.lvh.io/posts/2014/05/on-truecrypt-and-full-disk-encryption.html</guid><pubDate>Thu, 29 May 2014 17:53:00 GMT</pubDate></item><item><title>Thoughts on RDRAND in Linux</title><link>http://www.lvh.io/posts/2013/10/thoughts-on-rdrand-in-linux.html</link><dc:creator>lvh</dc:creator><description>&lt;div&gt;&lt;p&gt;This has been brewing since I read &lt;a href="https://www.change.org/en-GB/petitions/linus-torvalds-remove-rdrand-from-dev-random-4/responses/9066"&gt;Linus' response to the petition to
remove &lt;code&gt;RDRAND&lt;/code&gt; from /dev/random&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;For those of you who don't know, &lt;code&gt;RDRAND&lt;/code&gt; is a CPU instruction
introduced by Intel on recent CPUs. It (supposedly) uses a hardware
entropy source, and runs it through AES in CBC-MAC mode, to produce
random numbers.&lt;/p&gt;
&lt;p&gt;Out of fear that &lt;code&gt;RDRAND&lt;/code&gt; may somehow be backdoored, someone
petitioned to remove &lt;code&gt;RDRAND&lt;/code&gt; support to "improve the overall security
of the kernel". If &lt;code&gt;RDRAND&lt;/code&gt; contains a back door, and an unknown
attacker can control the output, that could break pretty much all
userland crypto.&lt;/p&gt;
&lt;p&gt;Linus fulminated, as he is wont to do. He suggested we go read
&lt;code&gt;drivers/char/random.c&lt;/code&gt;. I quote (expletives and insults omitted):&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;we use rdrand as &lt;em&gt;one&lt;/em&gt; of many inputs into the random pool, and we
use it as a way to &lt;em&gt;improve&lt;/em&gt; that random pool. So even if rdrand
were to be back-doored by the NSA, our use of rdrand actually
improves the quality of the random numbers you get from
/dev/random.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I went ahead and read &lt;code&gt;random.c&lt;/code&gt;. You can read it for yourself &lt;a href="https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/tree/drivers/char/random.c"&gt;in
Linus' tree&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Disclaimer: I am not an expert in this piece of code. I have no doubt
Linus is far more familiar with it than I am. I'd love to be proven
wrong. I'm just taking his advice and reading some code.&lt;/p&gt;
&lt;p&gt;The function I'm interested in is &lt;code&gt;extract_buf&lt;/code&gt;:&lt;/p&gt;
&lt;pre class="code literal-block"&gt;    &lt;span class="cm"&gt;/*&lt;/span&gt;
&lt;span class="cm"&gt;     * If we have a architectural hardware random number&lt;/span&gt;
&lt;span class="cm"&gt;     * generator, mix that in, too.&lt;/span&gt;
&lt;span class="cm"&gt;     */&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;LONGS&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;EXTRACT_SIZE&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;++&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="kt"&gt;unsigned&lt;/span&gt; &lt;span class="kt"&gt;long&lt;/span&gt; &lt;span class="n"&gt;v&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;!&lt;/span&gt;&lt;span class="n"&gt;arch_get_random_long&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;v&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
            &lt;span class="k"&gt;break&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
        &lt;span class="n"&gt;hash&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;l&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;^=&lt;/span&gt; &lt;span class="n"&gt;v&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;


&lt;p&gt;This is in the extraction phase. This is after the hash is being mixed
back in to the pool (and that's for backtracking attacks: not intended
as an input to the pool). It seems to me like the output of
&lt;code&gt;arch_get_random_long&lt;/code&gt; is being XORed in with the extracted output,
not with the pool.&lt;/p&gt;
&lt;p&gt;If I were to put on my tin-foil hat, I would suggest that the
difficulty has now been moved from being able to subvert the pool as
one of its entropy sources (which we think is impossible), versus
being able to see what you're about to be XORed with. The latter seems
a lot closer to the realm of stuff a microcode instruction can do.&lt;/p&gt;
&lt;p&gt;To put it into Python:&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;inspect&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;currentframe&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;random&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;getrandbits&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;extract_buf&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="sd"&gt;"""Gets 16 bytes from the pool, and mixes them with RDRAND output.&lt;/span&gt;

&lt;span class="sd"&gt;    """&lt;/span&gt;
    &lt;span class="n"&gt;pool_bits&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;extract_from_pool&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;rdrand_bits&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;rdrand&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt;  &lt;span class="n"&gt;pool_bits&lt;/span&gt; &lt;span class="o"&gt;^&lt;/span&gt; &lt;span class="n"&gt;rdrand_bits&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;extract_from_pool&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="sd"&gt;"""Pretend to get some good, unpredictable bytes from the pool.&lt;/span&gt;

&lt;span class="sd"&gt;    Actually gets a long with some non-cryptographically secure random&lt;/span&gt;
&lt;span class="sd"&gt;    bits from random.getrandbits, which is usually a Mersenne Twister.&lt;/span&gt;

&lt;span class="sd"&gt;    """&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;getrandbits&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;32&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;rdrand&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="sd"&gt;"""&lt;/span&gt;
&lt;span class="sd"&gt;    A malicious hardware instruction.&lt;/span&gt;
&lt;span class="sd"&gt;    """&lt;/span&gt;
    &lt;span class="n"&gt;pool_bits&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;currentframe&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;f_back&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;f_locals&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;"pool_bits"&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;pool_bits&lt;/span&gt; &lt;span class="o"&gt;^&lt;/span&gt; &lt;span class="mh"&gt;0xabad1dea&lt;/span&gt;

&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;__name__&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s"&gt;"__main__"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;assert&lt;/span&gt; &lt;span class="n"&gt;extract_buf&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mh"&gt;0xabad1dea&lt;/span&gt;
&lt;/pre&gt;


&lt;p&gt;Why can't RDRAND work like this?&lt;/p&gt;
&lt;p&gt;Some comments based on feedback I've gotten so far:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;This attack does not need to know where the PRNG state lives in
memory. First of all, this isn't an attack on the PRNG state, it's on
the PRNG output. Secondly, the instruction only needs to peek ahead at
what is about to happen (specifically, what's about to be XORed with)
the RDRAND output. That doesn't require knowing where the PRNG state
(or its output) is being stored in memory; we're already talking
register level at that point.&lt;/li&gt;
&lt;li&gt;While it's certainly true that if you can't trust the CPU, you
can't trust anything, that doesn't really make this problem go away.
&lt;code&gt;RDRAND&lt;/code&gt; being broken wouldn't make software crash, which is a lot
harder for almost all other instructions. &lt;code&gt;RDRAND&lt;/code&gt; being broken
wouldn't result in measurable side-effects, unlike what would happen
if &lt;code&gt;PCLMULDQ&lt;/code&gt; contained a back door. Furthermore, it's a lot easier to
backdoor one single microcode instruction and a lot more plausible and
feasible for a CSPRNG to be backdoored than it is to think of a CPU as
some kind of intelligent being that's actively malicious or being
remotely controlled.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For what it's worth, it seems &lt;a href="https://twitter.com/zooko/status/392334674690723840"&gt;Zooko agrees with me&lt;/a&gt;.&lt;/p&gt;&lt;/div&gt;</description><category>crypto</category><guid>http://www.lvh.io/posts/2013/10/thoughts-on-rdrand-in-linux.html</guid><pubDate>Sun, 20 Oct 2013 04:47:00 GMT</pubDate></item></channel></rss>