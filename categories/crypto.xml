<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>lvh (crypto)</title><link>https://www.lvh.io/</link><description></description><atom:link href="https://www.lvh.io/categories/crypto.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><lastBuildDate>Tue, 22 Nov 2016 16:18:15 GMT</lastBuildDate><generator>https://getnikola.com/</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Nonce misuse resistance 101</title><link>https://www.lvh.io/posts/nonce-misuse-resistance-101.html</link><dc:creator>lvh</dc:creator><description>&lt;div&gt;&lt;p&gt;&lt;em&gt;This post is an introduction to nonce-misused resistant cryptosystems and why
I think they matter. The first part of this post is about nonce-based
authenticated encryption schemes: how they work, and how they fail. If you're
already familiar with them, you can skip to the section on
&lt;a href="https://www.lvh.io/posts/nonce-misuse-resistance-101.html#proto"&gt;protocol design&lt;/a&gt;. If you're completely new to cryptography, you might
like my free introductory course to cryptography, &lt;a href="https://www.crypto101.io"&gt;Crypto 101&lt;/a&gt;. In a
future blog post, I'll talk about some nonce-misuse resistant schemes I've
implemented using libsodium.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Many stream ciphers and stream cipher-like constructions such as CTR,
GCM, (X)Salsa20... take a nonce. You can think of it as a pointer that lets
you jump to a particular point in the keystream. This makes these ciphers
"seekable", meaning that you can decrypt a small part of a big ciphertext,
instead of having to decrypt everything up to that point first. (That ends up
being trickier than it seems, because you still want to authenticate that
small chunk of ciphertext, but that's a topic for another time.)&lt;/p&gt;
&lt;p&gt;The critical security property of a nonce is that it's never repeated under
the same key. You can remember this by the mnemonic that a &lt;em&gt;nonce&lt;/em&gt; is a
"number used once". If you were to repeat the nonce, the keystream would also
repeat. That means that an attacker can take the two ciphertexts and XOR them
to compute the XOR of the plaintexts. If &lt;code&gt;C_n&lt;/code&gt; are ciphertexts, &lt;code&gt;P_n&lt;/code&gt;
plaintexts, &lt;code&gt;K_n&lt;/code&gt; keystreams, and &lt;code&gt;^&lt;/code&gt; is bitwise exclusive or:&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span&gt;&lt;/span&gt;C_1 = K_1 ^ P_1
C_2 = K_2 ^ P_2
&lt;/pre&gt;


&lt;p&gt;The attacker just XORs &lt;code&gt;C_1&lt;/code&gt; and &lt;code&gt;C_2&lt;/code&gt; together:&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span&gt;&lt;/span&gt;C_1 ^ C_2 = K_1 ^ P_1 ^ K_2 ^ P_2
&lt;/pre&gt;


&lt;p&gt;Since XOR is commutative (you can rearrange the order), &lt;code&gt;K_1 = K_2&lt;/code&gt;, and
XOR'ing two equal values cancels them out:&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span&gt;&lt;/span&gt;C_1 ^ C_2 = P_1 ^ P_2
&lt;/pre&gt;


&lt;p&gt;That tells an attacker a lot about the plaintext, especially if some of one of
the plaintexts is predictable. If the attacker has access to an encryption
oracle, meaning that they can get encryptions for plaintexts of their
choosing, they can even get perfect decryptions. That is not an unrealistic
scenario. For example, if you're encrypting session cookies that contain the
user name and e-mail, I can register using a name and e-mail address that has
a lot of &lt;code&gt;Z&lt;/code&gt; characters, and then I know that just XORing with &lt;code&gt;Z&lt;/code&gt; will reveal
most of the plaintext. For an idea of the state of the art in attacking
two-time pads (the usual term for two ciphertexts with a reused keystream),
see &lt;a href="https://www.cs.jhu.edu/~jason/papers/mason+al.ccs06.pdf"&gt;Mason06&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a id="proto"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;Protocol design&lt;/h3&gt;
&lt;p&gt;For many on-line protocols like TLS, the explicit nonce provides a convenient
way to securely send many messages under a per-session key. Because the
critical security property for a nonce is that it is never repeated with the
same key, it's safe to use a counter. In protocols where both peers send
messages to each other, you can just have one peer use odd nonces and have the
other use even ones. There are some caveats here: for example, if the nonce
size is sufficiently small, an attacker might try to make that counter
overflow, resulting in a repeated nonce.&lt;/p&gt;
&lt;p&gt;For off-line (or at-rest) protocols, it's a little trickier. You don't have a
live communication channel to negotiate a new ephemeral key over, so you're
stuck with longer-term keys or keys derived from them. If multiple systems are
participating, you need to decide ahead of time which systems own which
nonces. Even then, systems need to keep track of which nonces they've
used. That doesn't work well, especially not in a distributed system where
nodes and connections can fail at any time. This is why some cryptosystems
like &lt;a href="https://cryptography.io/en/latest/fernet/"&gt;Fernet&lt;/a&gt; provide an API that doesn't require you to specify
anything besides a key and a message.&lt;/p&gt;
&lt;p&gt;One solution is to use randomized nonces. Since nonces can't repeat, random
nonces should be large: if they're too small, you might randomly select the
same nonce twice, per the birthday bound. That is the only difference between
Salsa20 and XSalsa20: Salsa20 has a 64 bit nonce, whereas XSalsa20 has a 192
bit nonce. That change exists explicitly to make random nonces secure.&lt;/p&gt;
&lt;p&gt;Picking a random nonce and just prepending it to the secretbox ciphertext is
secure, but there are a few problems with this approach. It's not clear to
practitioners that that's a secure construct. Doing this may seem obvious to a
cryptographer, but not to someone who just wants to encrypt a
message. Prepending a nonce doesn't feel much different from e.g. appending a
MAC. A somewhat knowledgeable practitioner knows that there's plenty of ways
to use MACs that are insecure, and they don't immediately see that the
prefix-nonce construction is secure. Not wanting to design your own
cryptosystems is a good reflex which we should be encouraging.&lt;/p&gt;
&lt;p&gt;Random nonces also mean that any system sending messages needs access to
high-quality random number generators while they're sending a message. That's
often, but not always true. Bugs around random number generation, especially
userspace CSPRNGs, &lt;a href="http://sockpuppet.org/blog/2014/02/25/safely-generate-random-numbers/"&gt;keep popping up&lt;/a&gt;. This is often a consequence of
poor programming practice, but it can also be a consequence of
poorly-configured VMs or limitations of embedded hardware.&lt;/p&gt;
&lt;h3&gt;Nonce-misuse resistant systems&lt;/h3&gt;
&lt;p&gt;To recap, not all protocols have the luxury of an obvious nonce choice, and
through circumstances or poor practices, nonces might repeat
anyway. Regardless of how cryptographers feel about how important nonce misuse
is, we can anecdotally and empirically verify that such issues are real and
common. This is true even for systems like TLS where there is an "obvious"
nonce available (&lt;a href="https://eprint.iacr.org/2016/475.pdf"&gt;BÃ¶ck et al, 2016&lt;/a&gt;). It's easy to point fingers, but
it's better to produce cryptosystems that fail gracefully.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://web.cs.ucdavis.edu/~rogaway/papers/keywrap.pdf"&gt;Rogaway and Shrimpton (2006)&lt;/a&gt; defined a new model called nonce-misuse
resistance. Informally, nonce-misuse resistance schemes ensure that a repeated
random nonce doesn't result in plaintext compromise. In the case of a broken
system where the attacker can cause repeated nonces, an attacker will only be
able to discern if a particular message repeated, but they will not be able
to decrypt the message.&lt;/p&gt;
&lt;p&gt;Rogaway and Shrimpton also later developed a mode of operation called SIV
(synthetic IV), which Gueron and Lindell are refined to GCM-SIV, a SIV-like
that takes advantage of fast GCM hardware implementations. Those two authors
are currently working with Adam Langley to standardize the AES-GCM-SIV
construction through CFRG. AEZ and HS1-SIV, two entries in the CAESAR
competition, also feature nonce-misuse resistance. CAESAR is an ongoing
competition, and GCM-SIV is not officially finished yet, so this is clearly
a field that is still evolving.&lt;/p&gt;
&lt;p&gt;There are parallels between nonce-misuse resistance and length extension
attacks. Both address issues that arguably only affected systems that were
doing it wrong to begin with. (Note, however, in the embedded case above, it
might not be a software design flaw but a hardware limitation.) Fortunately,
the SHA-3 competition showed that you can have increased performance and
still be immune to a class of problems. I'm hopeful that CAESAR will consider
nonce-misuse resistance an important property of an authenticated encryption
standard.&lt;/p&gt;
&lt;h3&gt;Repeated messages&lt;/h3&gt;
&lt;p&gt;Repeated messages are suboptimal, and in some protocols they might be
unacceptable. However, they're a fail-safe failure mode for nonce
misuse. You're not choosing to have a repeated ciphertext, you're just getting
a repeated ciphertext instead of a plaintext disclosure (where the attacker
would also know that you repeated a message). In the case of a secure random
nonce, a nonce-misuse resistant scheme is just as secure, at the cost of a
performance hit.&lt;/p&gt;
&lt;p&gt;In a context where attackers can see individual messages to detect repeated
ciphertexts, it makes sense to also consider a model where attackers can
replay messages. If replaying messages (which presumably have side effects) is
a problem, a common approach is to add a validity timestamp. This is a feature
of &lt;a href="https://cryptography.io/en/latest/fernet/"&gt;Fernet&lt;/a&gt;, for example. A device that doesn't have access to
sufficient entropy will still typically have access to a reasonably
high-resolution clock, which is still more than good enough to make sure the
synthetic IVs don't repeat either.&lt;/p&gt;
&lt;h3&gt;OK, but how does it work?&lt;/h3&gt;
&lt;p&gt;Being able to trade plaintext disclosure for attackers being able to detect
repeated messages sounds like magic, but it makes sense once you realize how
they work. As demonstrated in the start of this post, nonce re-use normally
allows an attacker to have two keystreams cancel out. That only makes sense if
two &lt;em&gt;distinct&lt;/em&gt; messages are encrypted using the same (key, nonce) pair. NMR
solves this by making the nonce also depend on the message itself. Informally,
it means that a nonce should never repeat for two distinct
messages. Therefore, an attacker can't cancel out the keystreams without
cancelling out the messages themselves as well.&lt;/p&gt;
&lt;p&gt;This model does imply off-line operation, in that the entire message has to be
scanned before the nonce can be computed. For some protocols, that may not be
acceptable, although plenty of protocols work around this assumption by simply
making individual messages sufficiently small.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Thanks to Aaron Zauner and Kurt Griffiths for proofreading this post.&lt;/em&gt;&lt;/p&gt;&lt;/div&gt;</description><category>crypto</category><category>security</category><guid>https://www.lvh.io/posts/nonce-misuse-resistance-101.html</guid><pubDate>Thu, 19 May 2016 19:25:44 GMT</pubDate></item><item><title>Supersingular isogeny Diffie-Hellman 101</title><link>https://www.lvh.io/posts/supersingular-isogeny-diffie-hellman-101.html</link><dc:creator>lvh</dc:creator><description>&lt;div&gt;&lt;p&gt;Craig Costello, Patrick Longa and Michael Naehrig, three cryptographers at
Microsoft Research, recently published a &lt;a href="https://eprint.iacr.org/2016/413"&gt;paper&lt;/a&gt; on supersingular
isogeny Diffie-Hellman. This paper garnered a lot of interest in the security
community and even made it to the front page of Hacker News. Most of the
discussion around it seemed to be how no one understands isogenies, even
within cryptography-literate communities. This article aims to give you a
high-level understanding of what this cryptosystem is and why it works.&lt;/p&gt;
&lt;p&gt;This post assumes that you already know how Diffie-Hellman works in the
abstract, and that you know elliptic curves are a mathematical construct that
you can use to perform Diffie-Hellman operations, just like you can with the
integers &lt;em&gt;mod p&lt;/em&gt; (that would be "regular" Diffie-Hellman). If that was
gibberish to you and you'd like to know more, check out &lt;a href="https://www.crypto101.io"&gt;Crypto 101&lt;/a&gt;, my
free introductory book on cryptography. You don't need a math background to
understand those concepts at a high level. The main difference is that Crypto
101 sticks to production cryptography, while this is still experimental.&lt;/p&gt;
&lt;p&gt;It's not surprising that isogeny-based cryptography is so confusing. Up until
recently, it was unambiguously in the realm of research, not even close to
being practically applicable. Its mathematical underpinnings are much more
complex than regular elliptic curves, let alone integers &lt;em&gt;mod p&lt;/em&gt;. It also
looks superficially similar to elliptic curve Diffie-Hellman, which only adds
to the confusion.&lt;/p&gt;
&lt;p&gt;With that, let's begin!&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What is this paper about?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Supersingular isogeny Diffie-Hellman (SIDH) is one of a handful of
"post-quantum" cryptosystems. Those are cryptosystems that will remain secure
even if the attacker has access to a large quantum computer. This has nothing
to do with quantum cryptography (for example, quantum key distribution)
beyond their shared quantum mechanical underpinning.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Why should I care about quantum computers?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;General quantum computers are not useful as general-purpose computing devices,
but they can solve some problems much faster than classical
computers. Classical computers can emulate quantum computers, but only with
exponential slowdown. A sufficiently large quantum computer could break most
production cryptography, including cryptosystems based on the difficulty of
factoring large numbers (like RSA), taking discrete logs over the integers
&lt;em&gt;mod p&lt;/em&gt; (like regular DH), or taking discrete logs over elliptic curves (like
ECDH and ECDSA). To quantify that, consider the following table:&lt;/p&gt;
&lt;p&gt;&lt;img alt="quantum computer attack cost versus classical" src="https://www.lvh.io/img/post-quantum/quantum-computer-relative-cost.png"&gt;&lt;/p&gt;
&lt;p&gt;In this table, n refers to the modulus size for RSA, and the field size for
ECC. Look at the rightmost column, which represents time taken by the
classical algorithm, and compare it to the "time" columns, which represent how
much a quantum computer would take. As &lt;em&gt;n&lt;/em&gt; increases, the amount of time the
quantum computer would take stays in the same ballpark, whereas, for a
classical computer, it increases (almost) exponentially. Therefore, increasing
n is an effective strategy for keeping up with ever-faster classical
computers, but it is ineffective at increasing the run time for a quantum
computer.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Aah! Why isn't everyone panicking about this?!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The good news is that these large quantum computers don't exist yet.&lt;/p&gt;
&lt;p&gt;If you look at the qubits column, you'll see that these attacks require large
universal quantum computers. The state of the art in those only has a handful
of qubits. In 2011, IBM successfully factored 143 using a 4-qubit quantum
computer. Scaling the number of qubits up is troublesome. In that light,
larger key sizes may prove effective after all; we simply don't know yet how
hard it is to build quantum computers that big.&lt;/p&gt;
&lt;p&gt;D-wave, a quantum computing company, has produced computers with 128 and 512
qubits and even &amp;gt;1000 qubits. While there is some discussion if D-waves
provide quantum speedup or are even real quantum computers at all; there is no
discussion that they are not &lt;em&gt;universal&lt;/em&gt; quantum computers. Specifically, they
only claim to solve one particular problem called quantum annealing. The 1000
qubit D-Wave 2X cannot factor RSA moduli of ~512 bits or solve discrete logs
on curves of ~120 bits.&lt;/p&gt;
&lt;p&gt;The systems at risk implement asymmetric encryption, signatures, and
Diffie-Hellman key exchanges. That's no accident: all post-quantum
alternatives are asymmetric algorithms. Post-quantum secure symmetric
cryptography is easier: we can just use bigger key sizes, which are still
small enough to be practical and result in fast primitives. Quantum computers
simply halve the security level, so all we need to do to maintain a 128 bit
security level is to use ciphers with 256 bit keys, like Salsa20.&lt;/p&gt;
&lt;p&gt;Quantum computers also have an advantage against SIDH, but both are still
exponential in the field size. The SIDH scheme in the new paper has 192 bits
of security against a classical attacker, but still has 128 bits of security
against a quantum attacker. That's in the same ballpark as most symmetric
cryptography, and better than the 2048-bit RSA certificates that underpin the
security of the Internet.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What makes this paper special?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Post-quantum cryptography has been firmly in the realm of academic research
and experiments. This paper makes significant advancements in how practically
applicable SIDH is.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Being future-proof sounds good. If this makes it practical, why don't we
start using it right now?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;SIDH is a young cryptosystem in a young field, and hasn't had the same level
of scrutiny as some of the other post-quantum cryptosystems, let alone the
"regular" cryptosystems we use daily. Attacks only get better, they never get
worse. It's possible that SIDH is insecure, and we just don't know how to
break it yet. It does have a good argument for why quantum algorithms wouldn't
be able to crack it (more on that later), but that's a hypothesis, not a
proof.&lt;/p&gt;
&lt;p&gt;The new performance figures from this paper are impressive, but this system is
still much slower than the ones we use today. Key generation and key exchange
take a good 50 million cycles or so each. That's about a thousand times slower
than Curve25519, a curve designed about 10 years ago. Key sizes are also much
larger: SIDH public keys are 751 bytes, whereas Curve25519 keys are only 32
bytes. For on-line protocols like HTTPS operating over TCP, that's a
significant cost.&lt;/p&gt;
&lt;p&gt;Finally, there are issues with implementing SIDH safely. Systems like
Diffie-Hellman over integers &lt;em&gt;mod p&lt;/em&gt; are much less complex than elliptic curve
Diffie-Hellman (ECDH), let alone SIDH. With ECDH and ECC in general, we've
seen new implementation difficulties, especially with early curves. Point
addition formulas would work, unless you were adding a point to itself. You
have to check that input points are on the curve, or leak the secret key
modulo some small order. These are real implementation problems, even though
we know how to solve them.&lt;/p&gt;
&lt;p&gt;This is nothing compared to the difficulties implementing SIDH. Currently,
SIDH security arguments rely on honest peers. A peer that gives you a
pathological input can utterly break the security of the scheme. To make
matters worse, while we understand how to verify inputs for elliptic curve
Diffie-Hellman, we don't have a way to verify inputs for isogeny-based
cryptography at all. We don't have much research to fall back on here
either. This isn't a SIDH-specific problem; post-quantum cryptography isn't
mature enough yet to have implementation issues like these nailed down
yet. (For an example from lattice-based cryptography, see the recent paper by
&lt;a href="https://eprint.iacr.org/2016/415"&gt;Bindel et al&lt;/a&gt;.)&lt;/p&gt;
&lt;p&gt;I don't want to diminish the importance of this paper in any way!  Just
because it's not something that your browser is going to be doing tomorrow
doesn't mean it's not an impressive accomplishment. It's just a step on the
path that might lead to production crypto one day.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;OK, fine. Why is this so different from elliptic curve Diffie-Hellman?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;While SIDH and ECDH both use elliptic curves, they're different beasts. SIDH
generates new curves to perform a DH exchange, whereas ECDH uses points on one
fixed curve. These supersingular curves also have different properties from
regular curves. Using a supersingular curve for regular elliptic curve
operations would be horribly insecure. If you have some background in elliptic
curves: supersingular curves have a tiny embedding degree, meaning that
solving the ECDLP over &lt;code&gt;F(p)&lt;/code&gt; can easily be transformed into solving the DLP
over &lt;code&gt;F(p^n)&lt;/code&gt; where &lt;code&gt;n&lt;/code&gt; is that small embedding degree. Most curves have large
embedding degrees, meaning that solving the ECDLP directly is easier than
translating it into a DLP and then solving that.  You generally have to go out
of your way to find a curve with a small embedding degree. That is only done
in specialized systems, like for pairing-based cryptography, or, as in this
case, supersingular isogeny-based Diffie-Hellman.&lt;/p&gt;
&lt;p&gt;Let's recap ECDH. Public keys are points on a curve, and secret keys are
numbers. Alice and Bob agree on the parameters of the exchange ahead of time,
such as the curve &lt;em&gt;E&lt;/em&gt; and a generator point &lt;em&gt;P&lt;/em&gt; on that curve. Alice picks a
secret integer &lt;em&gt;a&lt;/em&gt; and computes her public key &lt;em&gt;aP&lt;/em&gt;. Bob picks a secret
integer &lt;em&gt;b&lt;/em&gt; and computes his public key &lt;em&gt;bP&lt;/em&gt;. Alice and Bob send each other
their public keys, and multiply their secret key by the other peer's public
key. Since &lt;em&gt;abP = baP&lt;/em&gt;, they compute the same secret. Since an attacker has
neither secret key, they can't compute the shared secret.&lt;/p&gt;
&lt;p&gt;SIDH is different. Secret keys are isogenies...&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Whoa whoa whoa. What the heck are isogenies?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;An isogeny between elliptic curves is a function from one elliptic curve to
another that preserves base points. That means it takes points on one curve
and returns points on the other curve. Every point on the input curve will map
to a point on the output curve; but multiple points may map to the same
point. Formally speaking, the isogeny is surjective. An isogeny is also a
homomorphism. That is, it preserves the structure of the curve. For any two
points P and Q, &lt;code&gt;phi(P + Q) = phi(P) + phi(Q)&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;We have a bunch of formulas for generating isogenies from a curve and a
point. You might remember that the set of values a function takes is its
"domain", and the set of values it returns is called its "codomain". The
domain of such an isogeny is the curve you give it; its codomain might be the
same curve, or it might be a different one. In general, for SIDH, we care
about the case where it produces a new curve.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;OK, so explain how SIDH works again.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Roughly speaking, a secret key is an isogeny, and a public key is an elliptic
curve. By "mixing" their isogeny with the peer's public curve, each peer
generates a secret curve. The two peers will generally generate different
curves, but those curves will have the same j-invariant.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Wait, what's a j-invariant?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The j-invariant is a number you can compute for a particular curve. Perhaps
the best analogy would be the discriminant for quadratic equation you might
remember from high school math; it's a single number that tells you something
interesting about the underlying curve. There are different formulas for
curves in different forms. For example, for a curve in short Weierstrass form
&lt;code&gt;y^2 = x^3 + ax + b&lt;/code&gt;, the j-invariant is:&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span&gt;&lt;/span&gt;j(E) = (1728 * 4a^3)/(4a^3 + 27b^2)
&lt;/pre&gt;


&lt;p&gt;The j-invariant has a few cool properties. For example, while this is the
formula for the short Weierstrass form, the value of j doesn't change if you
put the same curve in a different form. Also, all curves with the same
j-invariant are isomorphic. However, for SIDH you don't really care about
these properties; you just care that the j-invariant is a number you can
compute, and it'll be the same for the two secret curves that are generated by
the DH exchange.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;OK, try explaining SIDH again.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The protocol fixes a supersingular curve E and four points on that
curve: P_A, Q_A, P_B, Q_B.&lt;/p&gt;
&lt;p&gt;Alice picks two random integers, m_A and n_A. She takes a linear combination
of those two integers with P_A and Q_A to produce a random point R_A, so:&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span&gt;&lt;/span&gt;R_A = n_A * P_A + m_A * Q_A
&lt;/pre&gt;


&lt;p&gt;That random point defines Alice's secret isogeny through the isogeny formulas
I talked about above. The codomain of that isogeny forms Alice's public
curve. Alice transforms points P_B and Q_B with the isogeny. She sends Bob her
public curve and the two transformed points.&lt;/p&gt;
&lt;p&gt;Bob does the same thing, except with A and B swapped.&lt;/p&gt;
&lt;p&gt;Once Alice gets Bob's public key, she applies m_A and n_A again to the
corresponding transformed points she got from Bob. She generates a new isogeny
phiBA from the resulting point just like she did before to generate her
private key. That isogeny's codomain will be an elliptic curve E_BA.&lt;/p&gt;
&lt;p&gt;When Bob performs his side of the exchange, he'll produce a different isogeny
and a different elliptic curve E_AB; but it will have the same j-invariant as
the curve Alice computed.  That j-invariant is the shared key.&lt;/p&gt;
&lt;p&gt;I've compiled a &lt;a href="https://dl.dropboxusercontent.com/u/38476311/Supersingular%20Isogeny%20Elliptic%20Curve%20Cryptography%20--%20Sage.pdf"&gt;transcript&lt;/a&gt; of a Diffie-Hellman exchange using
Sage so you can see a (toy!) demo in action.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;I know a little about elliptic curves. I thought they were always
non-singular. What's a supersingular elliptic curve but a contradiction in
terms?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;You're right! Supersingular elliptic curves are somewhat confusingly
named. Supersingular elliptic curves are still elliptic curves, and they are
non-singular just like all other elliptic curves. The "supersingular" refers
to the singular values of the j-invariant. Equivalently, the Hasse invariant
will be 0.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;So, why does it matter that the curve is supersingular?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Firstly, computing the isogeny is much easier on supersingular curves than on
ordinary (not supersingular) elliptic curves. Secondly, if the curve is
ordinary, the scheme can be broken in subexponential time by a quantum
attacker.&lt;/p&gt;
&lt;p&gt;Isogeny-based cryptography using ordinary curves was considered as a
post-quantum secure cryptosystem before SIDH. However, Childs et al. showed a
subexponential quantum algorithm in 2010. This paper appeared to have ended
isogeny-based cryptography: it was already slower than other post-quantum
systems, and now it was shown that it wasn't even post-quantum secure.&lt;/p&gt;
&lt;p&gt;Because supersingular curves are rare, they had not previously been considered
for isogeny-based cryptography. However, the paper itself suggested that
supersingular curves might be worth examining, so it ended up pushing research
in a new direction rather than ending it.&lt;/p&gt;
&lt;p&gt;Explaining why the supersingular curve makes the problem quantum-hard is
tricky without being thoroughly familiar with isogenies and quantum
computing. If you're really interested, &lt;a href="https://arxiv.org/pdf/1012.4019v2.pdf"&gt;the Childs paper&lt;/a&gt; explains
how the quantum attack in the ordinary case works. Informally, in the ordinary
case, there is a group action (the &lt;em&gt;isogeny star operator&lt;/em&gt;) of the ideal class
group onto the set of isomorphism classes of isogenous curves with the same
endomorphism ring. That can be shown to be a special case of the abelian group
hidden shift problem, which can be solved quickly on a quantum computer. In
the supersingular case, there is no such group action to exploit. (If you're
trying to solve for this at home; this is why SIDH needs to define the 4
points P_A, P_B, Q_A, Q_B.)&lt;/p&gt;
&lt;p&gt;&lt;em&gt;I would like to thank Thomas Ptacek for reviewing this blog post and bearing
with me as I struggle through trying to come up with human-readable
explanations for all of this stuff; Sean Devlin for reminding me that Sage is
an excellent educational tool; and Watson Ladd for pointing out a correction
w.r.t the Hasse invariant (the Hasse-Witt matrix is undefined, not
singular.). Finally, I'd like to thank all the people who reviewed drafts of
this post, including (in no particular order) Bryan Geraghty, Shane Wilton,
Sean Devlin, Thomas Ptacek, Tanner Prynn, Glyph Lefkowitz and Chris Wolfe.&lt;/em&gt;&lt;/p&gt;&lt;/div&gt;</description><category>crypto</category><category>security</category><guid>https://www.lvh.io/posts/supersingular-isogeny-diffie-hellman-101.html</guid><pubDate>Sat, 30 Apr 2016 16:00:28 GMT</pubDate></item><item><title>We're just getting started</title><link>https://www.lvh.io/posts/were-just-getting-started.html</link><dc:creator>lvh</dc:creator><description>&lt;div&gt;&lt;p&gt;Most conference talks are transactional. The speaker has a point to
make. After the presentation, it's "over"; only spoken about in
perfect tenses. You've communicated your thoughts, perhaps had a
conversation or two, but, mostly, moved on.&lt;/p&gt;
&lt;p&gt;I've given talks like these. However, about two years ago, I gave a
talk that had a deep impact on my life. That talk was Crypto 101.&lt;/p&gt;
&lt;p&gt;Right before the presentation, cryptanalytic research was released
that popped RC4. I couldn't have asked for a better setup. Turns out
it wasn't &lt;em&gt;just&lt;/em&gt; luck; eventually our systemic failure as an industry
in taking security seriously was bound to catch up with us. Since
then, the proverbial piper has been well-paid. We've seen a plethora
of serious security bugs. Huge corporations have been the victims of
attacks in the billions of dollars a pop. As I'm writing this blog
post, there's an article on a new TLS attack in my reading list.&lt;/p&gt;
&lt;p&gt;It quickly became clear that this wasn't just a one-off thing. I
started writing &lt;a href="http://crypto101.github.io"&gt;Crypto 101, the book,&lt;/a&gt; not too long after
giving the talk. We were, unwittingly, at the crest of a wave that's
still growing. Projects like PyCA and LibreSSL started fighting
tirelessly to make the software we use better. Security talks became a
mandatory part of the programming conference food pyramid. My friends
Hynek and Ying gave fantastic talks. They, too, got "lucky" with a
security bombshell: Heartbleed happened mere days before the
conference.&lt;/p&gt;
&lt;p&gt;Last week, I presented Crypto 101 again at rax.io, Rackspace's
internal conference. It was well-received, and I think I provided
value for people's time. One thing, more than anything, it
crystallized where we are. We're not done yet. There's still a huge
audience left to reach. Interest in information security has done
nothing but grow.  With a total of just over 100,000 downloads for the
book and about half as many for the recording of the presentation,
people are definitely listening. We've made real impact, and we have
people's attention, but we need to keep going.&lt;/p&gt;
&lt;p&gt;One of the two talks I'll be giving at PyCon is a more high-level
overview of how we can build secure systems. More friends of mine will
talk in about TLS there too. Within Rackspace, I'm focusing on
information security. There are awesome things brewing here, and I
hope that we can continue the great work we've been doing so far.&lt;/p&gt;
&lt;p&gt;We've accomplished a lot, but we're just getting started.&lt;/p&gt;&lt;/div&gt;</description><category>crypto</category><category>security</category><guid>https://www.lvh.io/posts/were-just-getting-started.html</guid><pubDate>Tue, 03 Mar 2015 01:06:41 GMT</pubDate></item><item><title>Securing APIs with shims</title><link>https://www.lvh.io/posts/securing-apis-with-shims.html</link><dc:creator>lvh</dc:creator><description>&lt;div&gt;&lt;p&gt;Imagine that you had a capability URL, except instead of giving you
the ability to perform a specific &lt;em&gt;action&lt;/em&gt;, it gave you the ability to
perform a (limited) set of operations on a third party API,
e.g. OpenStack. The capability URL wouldn't just be something you
exercise or revoke; it'd be an API endpoint, mostly indistinguishable
from the real API. Incoming requests would be inspected, and based on
a set of rules, either be rejected or forwarded to the API being
shimmed.&lt;/p&gt;
&lt;h2&gt;Proof of concept&lt;/h2&gt;
&lt;p&gt;At my day job, we had a programming task that I thought logic
programming would be well-suited for. Unfortunately, logic programming
is kind of weird and esoteric. Even programmers with otherwise broad
experiences professed to not being quite sure how it worked, or what
to do with it.&lt;/p&gt;
&lt;p&gt;Therefore, I used up my hack day (a day where we get to hack on random
projects) to cook up some cool stuff using logic programming. I demoed
the usual suspects (&lt;a href="https://github.com/lvh/shimmer/blob/master/src/shimmer/monkey.clj"&gt;the monkey with the banana&lt;/a&gt;, and a
&lt;a href="https://github.com/lvh/shimmer/blob/master/src/shimmer/sudoku.clj"&gt;sudoku solver&lt;/a&gt;), illustrating the difference between the
relational nature of the logic programs and the imperative nature of
the algorithms you might otherwise write to solve the same problems.
Finally, I demoed the aforementioned proxying API shim. The proof of
concept, codenamed shimmer, is &lt;a href="https://github.com/lvh/shimmer/"&gt;up on Github&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Let's take a look at the handler function, which takes incoming
requests and modifies them slightly so they can be passed on:&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kd"&gt;defn &lt;/span&gt;&lt;span class="nv"&gt;build-handler&lt;/span&gt;
  &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;target-host&lt;/span&gt; &lt;span class="nv"&gt;target-port&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
  &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;fn &lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;incoming-request&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;if &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;match&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;spy&lt;/span&gt; &lt;span class="nv"&gt;incoming-request&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
      &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;let &lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;modified-request&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;-&amp;gt; &lt;/span&gt;&lt;span class="nv"&gt;incoming-request&lt;/span&gt;
                                 &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;dissoc &lt;/span&gt;&lt;span class="ss"&gt;:scheme&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;;; hack&lt;/span&gt;
                                 &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;assoc &lt;/span&gt;&lt;span class="ss"&gt;:host&lt;/span&gt; &lt;span class="nv"&gt;target-host&lt;/span&gt;
                                        &lt;span class="ss"&gt;:port&lt;/span&gt; &lt;span class="nv"&gt;target-port&lt;/span&gt;
                                        &lt;span class="ss"&gt;:throw-exceptions&lt;/span&gt; &lt;span class="nv"&gt;false&lt;/span&gt;&lt;span class="p"&gt;))]&lt;/span&gt;
        &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;spy&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;request&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;spy&lt;/span&gt; &lt;span class="nv"&gt;modified-request&lt;/span&gt;&lt;span class="p"&gt;))))&lt;/span&gt;
      &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="ss"&gt;:status&lt;/span&gt; &lt;span class="mi"&gt;403&lt;/span&gt; &lt;span class="c1"&gt;;; Forbidden&lt;/span&gt;
       &lt;span class="ss"&gt;:headers&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s"&gt;"content-type"&lt;/span&gt; &lt;span class="s"&gt;"text/plain"&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
       &lt;span class="ss"&gt;:body&lt;/span&gt; &lt;span class="s"&gt;"Doesn't match!"&lt;/span&gt;&lt;span class="p"&gt;})))&lt;/span&gt;
&lt;/pre&gt;


&lt;p&gt;(Those &lt;code&gt;spy&lt;/code&gt; calls are from the excellent &lt;a href="https://github.com/ptaoussanis/timbre"&gt;&lt;code&gt;timbre&lt;/code&gt;&lt;/a&gt;
library. They make it easy to log values without cluttering up your
code; a godsend while developing with some libraries you're not
terribly familiar with.)&lt;/p&gt;
&lt;p&gt;The matching function looks like this:&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kd"&gt;defn &lt;/span&gt;&lt;span class="nv"&gt;match&lt;/span&gt;
  &lt;span class="s"&gt;"Checks if the request is allowed."&lt;/span&gt;
  &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;req&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
  &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;not= &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;l/run&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;q&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
          &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;l/conde&lt;/span&gt;
           &lt;span class="p"&gt;[(&lt;/span&gt;&lt;span class="nf"&gt;l/featurec&lt;/span&gt; &lt;span class="nv"&gt;req&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="ss"&gt;:request-method&lt;/span&gt; &lt;span class="ss"&gt;:get&lt;/span&gt;&lt;span class="p"&gt;})]&lt;/span&gt;
           &lt;span class="p"&gt;[(&lt;/span&gt;&lt;span class="nf"&gt;l/featurec&lt;/span&gt; &lt;span class="nv"&gt;req&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="ss"&gt;:request-method&lt;/span&gt; &lt;span class="ss"&gt;:post&lt;/span&gt;
                             &lt;span class="ss"&gt;:headers&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s"&gt;"x-some-header"&lt;/span&gt;
                                       &lt;span class="s"&gt;"the right header value"&lt;/span&gt;&lt;span class="p"&gt;}})]&lt;/span&gt;
           &lt;span class="p"&gt;[(&lt;/span&gt;&lt;span class="nf"&gt;l/featurec&lt;/span&gt; &lt;span class="nv"&gt;req&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="ss"&gt;:request-method&lt;/span&gt; &lt;span class="ss"&gt;:post&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;
            &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;l/featurec&lt;/span&gt; &lt;span class="nv"&gt;req&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="ss"&gt;:headers&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s"&gt;"x-some-header"&lt;/span&gt;
                                       &lt;span class="s"&gt;"another right header value"&lt;/span&gt;&lt;span class="p"&gt;}})]))&lt;/span&gt;
        &lt;span class="o"&gt;'&lt;/span&gt;&lt;span class="p"&gt;()))&lt;/span&gt;
&lt;/pre&gt;


&lt;h2&gt;Future work&lt;/h2&gt;
&lt;p&gt;Make this thing actually vaguely correct. That means e.g. also
inspecting the body for URL references, and changing those to go
through the proxy as well.&lt;/p&gt;
&lt;p&gt;Start collecting a library of short hand notations for specific API
functionality, e.g. if you're proxying an OpenStack API, you should be
able to just say you want to allow server creation requests, without
having to figure out exactly what those requests look like.&lt;/p&gt;
&lt;p&gt;The spec is hard-coded, it should be specified at runtime. That was
trickier than I had originally anticipated: the vast majority of
&lt;code&gt;core.logic&lt;/code&gt; behavior uses macros. While some functionality is fairly
easy to port, that's probably a red herring: I don't want to port a
gazillion macros. As an example, here's &lt;code&gt;conds&lt;/code&gt;, which is just&lt;code&gt;conde&lt;/code&gt;
as a function (except without support for logical conjunction per
disjunctive set of goals):&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kd"&gt;defn &lt;/span&gt;&lt;span class="o"&gt;^&lt;/span&gt;&lt;span class="ss"&gt;:private&lt;/span&gt; &lt;span class="nv"&gt;conds&lt;/span&gt;
  &lt;span class="s"&gt;"Like conde, but a function."&lt;/span&gt;
  &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;goals&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
  &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;if &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;empty?&lt;/span&gt; &lt;span class="nv"&gt;goals&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="nv"&gt;l/fail&lt;/span&gt;
    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;l/conde&lt;/span&gt; &lt;span class="p"&gt;[(&lt;/span&gt;&lt;span class="nb"&gt;first &lt;/span&gt;&lt;span class="nv"&gt;goals&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
             &lt;span class="p"&gt;[(&lt;/span&gt;&lt;span class="nf"&gt;conds&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;rest &lt;/span&gt;&lt;span class="nv"&gt;goals&lt;/span&gt;&lt;span class="p"&gt;))])))&lt;/span&gt;
&lt;/pre&gt;


&lt;p&gt;That's not the worst function, but let's just say I see a lot of
&lt;code&gt;macroexpand&lt;/code&gt; in my future if I'm going to take this seriously.&lt;/p&gt;
&lt;p&gt;URLs and bodies should be parsed, so that you can write assertions
against structured data, or against URL patterns, instead of specific
URLs.&lt;/p&gt;
&lt;p&gt;If I ever end up letting any of this be a serious part of my day job,
I'm going to invest a ton of time improving the documentation for both
&lt;code&gt;core.logic&lt;/code&gt; and &lt;code&gt;core.typed&lt;/code&gt;. They're &lt;em&gt;fantastic&lt;/em&gt; projects, but
they're harder to get started with than they could be, and that's a
shame.&lt;/p&gt;&lt;/div&gt;</description><category>crypto</category><category>security</category><guid>https://www.lvh.io/posts/securing-apis-with-shims.html</guid><pubDate>Sat, 21 Feb 2015 06:24:03 GMT</pubDate></item><item><title>On discussing software security improvements</title><link>https://www.lvh.io/posts/on-discussing-software-security-improvements.html</link><dc:creator>lvh</dc:creator><description>&lt;div&gt;&lt;p&gt;A common criticism of information security folks is that they tend to
advise people to not do any crypto. Through projects like &lt;a class="reference external" href="https://www.crypto101.io/"&gt;Crypto
101&lt;/a&gt;, I've attempted to make a small contribution towards fixing
that.&lt;/p&gt;
&lt;p&gt;In the open source world, various people often try to improve the
security of a project. Because designing secure systems is pretty
hard, they often produce flawed proposals. The aforemetioned tendency
for infosec-conscious people to tell them to stop doing crypto is
experienced as unwelcoming, even dismissive. Typically, the only thing
that's accomplished is that a lot of feelings get hurt; it seems to
only rarely result in improved software.&lt;/p&gt;
&lt;p&gt;I think that's quite unfortunate. I think open source is great, and we
should be not just welcoming and inclusive, but aiming to produce
secure software. Furthermore, even if a flawed proposal is
unsalvageable, a clear description of &lt;em&gt;why&lt;/em&gt; it is flawed will
presumably result in fewer negative interactions. Best case scenario,
the issues with a proposal can be discussed and potentially rectified.&lt;/p&gt;
&lt;p&gt;In an effort to improve this situation, I'm documenting what I believe
to be a useful way to discuss security changes and their tradeoffs. As
Zooko has taught me:&lt;/p&gt;
&lt;blockquote&gt;
Security isn't about perfect versus imperfect or about better versus
worse, it's about &lt;em&gt;this&lt;/em&gt; attack surface versus &lt;em&gt;that&lt;/em&gt; attack
surface.&lt;/blockquote&gt;
&lt;p&gt;This document aims to be the equivalent of an &lt;a class="reference external" href="http://www.sscce.org/"&gt;SSCCE&lt;/a&gt; for generic bug
reports: a blueprint for making suggestions likely to lead to
productive discourse, as long as we can agree that we're trying to
produce more secure software, as well as provide a welcoming
development environment.&lt;/p&gt;
&lt;div class="section" id="important-points"&gt;
&lt;h2&gt;Important points&lt;/h2&gt;
&lt;p&gt;A good proposal should contain:&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;A brief description of what you're suggesting.&lt;/li&gt;
&lt;li&gt;A description of the attack model you're considering, why the
current system does not address this issue, and why the suggested
system &lt;em&gt;does&lt;/em&gt; address this issue.&lt;/li&gt;
&lt;li&gt;A motivation of the attack model. Why is it important that this
issue is actually addressed?&lt;/li&gt;
&lt;li&gt;How does this change affect the attack surface (i.e. all of the
ways an attacker can attempt to attack a system)?&lt;/li&gt;
&lt;li&gt;What does the user experience for all users of the system look
like? Many cryptosystems fall over because they're simply unusable
for most users of the system.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div class="section" id="an-example"&gt;
&lt;h2&gt;An example&lt;/h2&gt;
&lt;p&gt;Wul (the widely underestimated language, pronounced &lt;em&gt;/wool/&lt;/em&gt;) is a
general purpose programming language. It has a package repository,
WuPI (the Wul package index, pronounced &lt;em&gt;/woopie/&lt;/em&gt;), the de facto
standard for distributing and installing Wul software.&lt;/p&gt;
&lt;p&gt;WuPI uses TLS as a secure transport. The WuF (Wul foundation,
pronounced &lt;em&gt;/woof/&lt;/em&gt;), maintains a root certificate, distributed with
Wul. Thanks to a well-managed system of intermediary CAs run by a
tireless army of volunteers, this means that both package authors and
consumers know they're talking to the real WuPI.&lt;/p&gt;
&lt;p&gt;Alice is the WuPI BDFL. Bob is a Wul programmer, and would like to
improve the security of WuPI.&lt;/p&gt;
&lt;p&gt;While consumers and authors know that they're talking to the real
WuPI, there is no protection against a malicious WuPI endpoint. (This
problem was recently made worse because WuPI introduced a CDN, greatly
increasing the number of people who could own a node.). You know that
you're talking to something with a WuF-signed certificate (presumably
WuPI, provided the WuF has done a good job managing that certificate),
but you have no idea if that thing is being honest about the packages
it serves you.&lt;/p&gt;
&lt;p&gt;Bob believes WuPI could solve this by using GPG signatures.&lt;/p&gt;
&lt;p&gt;He starts with a brief description of the suggestion:&lt;/p&gt;
&lt;blockquote&gt;
I would like to suggest that WuPI grows support for GPG signatures
of packages. These signatures would be created when a package author
uploads a package. They would optionally be verified when the user
downloads a package.&lt;/blockquote&gt;
&lt;p&gt;He continues with the attack model being considered:&lt;/p&gt;
&lt;blockquote&gt;
I believe this would secure WuPI consumers against a malicious WuPI
endpoints. A malicious WuPI endpoint (assuming it acquires an
appropriate certificate) is currently free to deliver whatever
packages it wants.&lt;/blockquote&gt;
&lt;p&gt;He explains why the current model doesn't address this:&lt;/p&gt;
&lt;blockquote&gt;
The current system assures authenticity and secrecy of the stream
(through TLS), and it ensures that the server authenticates itself
with a WuPI/WuF certificate. It does not ensure that the package is
what the author uploaded.&lt;/blockquote&gt;
&lt;p&gt;He explains why he believes his model does address this:&lt;/p&gt;
&lt;blockquote&gt;
Because the signatures are produced by the author's GPG key, a
malicious WuPI endpoint would not be able to forge them. Therefore,
a consumer is sure that a package with a valid signature is indeed
from the author.&lt;/blockquote&gt;
&lt;p&gt;He explains why this attack model is important:&lt;/p&gt;
&lt;blockquote&gt;
With the new CDN support, the number of people with access to such a
certificate has greatly increased. While I certainly trust all of
the volunteers involved, it would be nice if we didn't &lt;em&gt;have&lt;/em&gt; to.
Furthermore, the software on the servers can always be vulnerable to
attack; as a high-value target, it certainly isn't inconceivable
that an attacker would use an unknown vulnerability to take over a
WuPI endpoint.&lt;/blockquote&gt;
&lt;p&gt;He (believes to) address the attack surface:&lt;/p&gt;
&lt;blockquote&gt;
Because the signatures are optional, the attack surface remains the
same.&lt;/blockquote&gt;
&lt;p&gt;Finally, he addresses the user experience:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The weak point of this scheme is most likely the user experience,
because users historically seem to dislike using GPG.&lt;/p&gt;
&lt;p&gt;I am hopeful that this increased value of participating in the GPG
web of trust will mean that more people participate.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Alice reviews this, and notes a flaw in the proposal:&lt;/p&gt;
&lt;blockquote&gt;
This proposal aims to address a security flaw when the WuPI endpoint
is malicious by adding signatures. However, a malicious WuPI
endpoint can lie by omission, and claim a package was never signed
by the author.&lt;/blockquote&gt;
&lt;p&gt;Bob now realizes this issue, and suggests an improvement:&lt;/p&gt;
&lt;blockquote&gt;
This could be rectified if the user insists on a signature for
packages they expect to be signed.&lt;/blockquote&gt;
&lt;p&gt;As a side note, Alice notes that the attack surface does increase:&lt;/p&gt;
&lt;blockquote&gt;
This places trust in author's ability to manage private keys, which
has historically been shown to be problematic. That introduces a new
attack vector: an attacker can attempt to go after the author's
private key.&lt;/blockquote&gt;
&lt;p&gt;Regardless of the outcome of this conversation, there actually was a
conversation. I believe this to be an improvement over the overall
status quo.&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;</description><category>crypto</category><category>python</category><category>security</category><guid>https://www.lvh.io/posts/on-discussing-software-security-improvements.html</guid><pubDate>Tue, 29 Jul 2014 06:58:27 GMT</pubDate></item><item><title>On TrueCrypt and full-disk encryption</title><link>https://www.lvh.io/posts/2014/05/on-truecrypt-and-full-disk-encryption.html</link><dc:creator>lvh</dc:creator><description>&lt;div&gt;&lt;p&gt;Since the early hours of May 29th (CEST), &lt;a href="https://www.truecrypt.org"&gt;the TrueCrypt website
(&lt;code&gt;https://www.truecrypt.org&lt;/code&gt;)&lt;/a&gt; has pointed
to &lt;code&gt;http://truecrypt.sourceforge.net/&lt;/code&gt;, with an ominous-looking error
message:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;WARNING: Using TrueCrypt is not secure as it may contain unfixed
security issues&lt;/p&gt;
&lt;p&gt;This page exists only to help migrate existing data encrypted by
TrueCrypt.&lt;/p&gt;
&lt;p&gt;The development of TrueCrypt was ended in 5/2014 after Microsoft
terminated support of Windows XP. Windows 8/7/Vista and later offer
integrated support for encrypted disks and virtual disk images. Such
integrated support is also available on other platforms (click here
for more information). You should migrate any data encrypted by
TrueCrypt to encrypted disks or virtual disk images supported on
your platform.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The website then explains how you can install BitLocker, a proprietary
disk encryption system available in many versions of Windows, as well
as how you could "rescue" existing TrueCrypt volumes.&lt;/p&gt;
&lt;p&gt;People were pretty unhappy, for a variety of reasons:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Lack of trust in proprietary full-disk encryption software.&lt;/li&gt;
&lt;li&gt;Many versions of Windows didn't even ship with BitLocker, only a few
  "premium" versions did.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As a proprietary system, BitLocker is not susceptible to the same
amount of public scrutiny as a publicly available system. This is in
stark contrast with TrueCrypt, where Matt Green recently raised around
70k USD to perform an &lt;a href="http://istruecryptauditedyet.com/"&gt;audit&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;There are variety of scenarios that could've caused the TrueCypt
website to suddenly sport that message. The live Internet audience has
speculated wildly. I'll share my thoughts on that near the end of this
post, but there are two points I'd like to make that I think are far
more important:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Consider if full-disk encryption is really what you want.&lt;/li&gt;
&lt;li&gt;If it is, consider if it should be TrueCrypt.&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;Full-disk encryption&lt;/h4&gt;
&lt;p&gt;Something I learned from the inimitable &lt;a href="https://zooko.com"&gt;Zooko&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Security isn't about perfect versus imperfect or about better versus
worse, it's about &lt;em&gt;this&lt;/em&gt; attack surface versus &lt;em&gt;that&lt;/em&gt; attack
surface.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;You can't just add more crypto junk to something and expect to somehow
get better security. You have to consider what it is buying you, and
what it's costing you.&lt;/p&gt;
&lt;p&gt;Full-disk encryption buys you one simple thing: if someone steals your
device while the encrypted volume is locked, they probably can't read
it.&lt;/p&gt;
&lt;p&gt;If the encrypted volume is unlocked, it's over; and that's the state
it's probably usually in. If the key lives in RAM (it usually does),
there's a variety of ways that can be extracted. There's devices that
have complete direct memory access, including everything that speaks
FireWire or has FireWire-compatibility built-in. Even if you shut off
your machine, cold boot attacks mean that the key can be extracted for
a limited about of time. Various jurisdictions can try to force you to
hand over the keys.&lt;/p&gt;
&lt;p&gt;If an attacker can write to the (encrypted!) volume, it's probably
over. Virtually all sector-level full-disk encryption formats are
unauthenticated (with good reason), they're all malleable and
vulnerable to (adaptive) chosen-ciphertext attacks.&lt;/p&gt;
&lt;p&gt;If you're encrypting files or blobs of data within other files, an
authenticated encryption scheme like GPG is far more useful.&lt;/p&gt;
&lt;p&gt;Don't get me wrong. Full-disk encryption is a good idea, and you
should do it. I'm just saying that what it actually protects against
is pretty limited. If there's files you want to keep secret, full-disk
encryption is probably not enough.&lt;/p&gt;
&lt;p&gt;For more details, try Thomas &amp;amp; Erin Ptacek's
&lt;a href="http://sockpuppet.org/blog/2014/04/30/you-dont-want-xts/"&gt;blog post&lt;/a&gt;
on why XTS isn't what you want. XTS is a way to build tweakable
ciphers that can be used to create full-disk encryption; but the post
applies to full-disk encryption generically as well.&lt;/p&gt;
&lt;h4&gt;TrueCrypt as a full-disk encryption mechanism&lt;/h4&gt;
&lt;p&gt;So, you probably want full-disk encryption, and you probably want to
make sure that sensitive data is encrypted on top of that. Great. What
full-disk encryption scheme do you use?&lt;/p&gt;
&lt;p&gt;I don't want to bash TrueCrypt. It is (was, perhaps?) a great go-to
project for full-disk encryption. It got a lot of things right. It
also got a bunch of things wrong.&lt;/p&gt;
&lt;p&gt;TrueCrypt was made by a bunch of people we don't know occasionally
throwing a bunch of binaries over the wall. The source is available
(under a non-OSI license), so you could audit that and compile your
own binaries, but the truth is that the vast majority of TrueCrypt
users never actually did that.&lt;/p&gt;
&lt;p&gt;The TrueCrypt disk format and encryption standards are a bit iffy. It
has terrible file system support. It has major performance issues,
partially due to questionable cryptographic practices such as cipher
cascades. It was great for when it was originally conceived and
full-disk encryption was still new and exciting, it's still pretty
decent now, but we can certainly do better.&lt;/p&gt;
&lt;h4&gt;What actually happened to the website?&lt;/h4&gt;
&lt;p&gt;We don't really know. There's a couple of guesses.&lt;/p&gt;
&lt;p&gt;First of all, it looks like it are the original authors that folded:
the key is the same one that was being used to sign releases months
ago. Of course, that could mean that it was compromised or that they
were forced to hand it over.&lt;/p&gt;
&lt;p&gt;Since the DNS records changed, the e-mail server behavior changed, the
same key was used, the Sourceforge client was involved, and fairly
major changes to the source code were involved, it's unlikely that
it's a simple defacement.&lt;/p&gt;
&lt;p&gt;It's unlikely that they folded because they felt discovery of some
backdoor was imminent. Folding wouldn't actually stop that discovery,
because the source was already open.&lt;/p&gt;
&lt;p&gt;It's strange that they would point to alternatives like Bitlocker for
Windows and even more dubious alternatives for other operating
systems. The authors certainly knew that this would not be an
acceptable alternative for the vast majority of their users.&lt;/p&gt;
&lt;p&gt;Additionally, the support window for XP ending isn't a particularly
convincing impetus for migrating away from TrueCrypt &lt;em&gt;right now&lt;/em&gt;. This
has lead some to believe that it's an automated release. Worse, it
could be a gagged response: a big and powerful three-letter agency
might be twisting their arm and forcing them to fold, in return for
something else (like, say, not being thrown in a Gitmo cell and
forgotten about). Again: pure speculation.&lt;/p&gt;
&lt;p&gt;Another option is that some of the developers just really, genuinely
folded. They felt that for atechnical users, BitLocker was good
enough, while the particularly discerning TrueCrypt user would be able
to find some other alternative.&lt;/p&gt;
&lt;p&gt;Bottom line is that none of this really matters. What matters is what
you should do next if you want to have full-disk encryption.&lt;/p&gt;
&lt;h4&gt;So what do I do now?&lt;/h4&gt;
&lt;p&gt;I think LUKS is probably the best system that we have right now, and
one of the few that actually improves on TrueCrypt.&lt;/p&gt;
&lt;p&gt;If you're on Linux, dm-crypt + cryptsetup + LUKS is probably what you
want. If you're on OS X, FileVault 2 is probably what you want. If
you're on Windows, your options are looking a bit thin right now:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Keep using old versions of TrueCrypt.&lt;/li&gt;
&lt;li&gt;Give up and use BitLocker.&lt;/li&gt;
&lt;li&gt;Use a Linux virtual machine to use dm-crypt/LUKS, eventually
  migrating to native support.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Personally, I think the first option is the most reasonable right now,
and then wait until someone actually writes the native LUKS support.&lt;/p&gt;
&lt;p&gt;Oh, and you should probably install GPG to encrypt some of those files
stored on that encrypted volume.&lt;/p&gt;&lt;/div&gt;</description><category>crypto</category><guid>https://www.lvh.io/posts/2014/05/on-truecrypt-and-full-disk-encryption.html</guid><pubDate>Thu, 29 May 2014 17:53:00 GMT</pubDate></item><item><title>Thoughts on RDRAND in Linux</title><link>https://www.lvh.io/posts/2013/10/thoughts-on-rdrand-in-linux.html</link><dc:creator>lvh</dc:creator><description>&lt;div&gt;&lt;p&gt;This has been brewing since I read &lt;a href="https://www.change.org/en-GB/petitions/linus-torvalds-remove-rdrand-from-dev-random-4/responses/9066"&gt;Linus' response to the petition to
remove &lt;code&gt;RDRAND&lt;/code&gt; from /dev/random&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;For those of you who don't know, &lt;code&gt;RDRAND&lt;/code&gt; is a CPU instruction
introduced by Intel on recent CPUs. It (supposedly) uses a hardware
entropy source, and runs it through AES in CBC-MAC mode, to produce
random numbers.&lt;/p&gt;
&lt;p&gt;Out of fear that &lt;code&gt;RDRAND&lt;/code&gt; may somehow be backdoored, someone
petitioned to remove &lt;code&gt;RDRAND&lt;/code&gt; support to "improve the overall security
of the kernel". If &lt;code&gt;RDRAND&lt;/code&gt; contains a back door, and an unknown
attacker can control the output, that could break pretty much all
userland crypto.&lt;/p&gt;
&lt;p&gt;Linus fulminated, as he is wont to do. He suggested we go read
&lt;code&gt;drivers/char/random.c&lt;/code&gt;. I quote (expletives and insults omitted):&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;we use rdrand as &lt;em&gt;one&lt;/em&gt; of many inputs into the random pool, and we
use it as a way to &lt;em&gt;improve&lt;/em&gt; that random pool. So even if rdrand
were to be back-doored by the NSA, our use of rdrand actually
improves the quality of the random numbers you get from
/dev/random.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I went ahead and read &lt;code&gt;random.c&lt;/code&gt;. You can read it for yourself &lt;a href="https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/tree/drivers/char/random.c"&gt;in
Linus' tree&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Disclaimer: I am not an expert in this piece of code. I have no doubt
Linus is far more familiar with it than I am. I'd love to be proven
wrong. I'm just taking his advice and reading some code.&lt;/p&gt;
&lt;p&gt;The function I'm interested in is &lt;code&gt;extract_buf&lt;/code&gt;:&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span&gt;&lt;/span&gt;    &lt;span class="cm"&gt;/*&lt;/span&gt;
&lt;span class="cm"&gt;     * If we have a architectural hardware random number&lt;/span&gt;
&lt;span class="cm"&gt;     * generator, mix that in, too.&lt;/span&gt;
&lt;span class="cm"&gt;     */&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;LONGS&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;EXTRACT_SIZE&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;++&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="kt"&gt;unsigned&lt;/span&gt; &lt;span class="kt"&gt;long&lt;/span&gt; &lt;span class="n"&gt;v&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;!&lt;/span&gt;&lt;span class="n"&gt;arch_get_random_long&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;v&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
            &lt;span class="k"&gt;break&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
        &lt;span class="n"&gt;hash&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;l&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;^=&lt;/span&gt; &lt;span class="n"&gt;v&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;


&lt;p&gt;This is in the extraction phase. This is after the hash is being mixed
back in to the pool (and that's for backtracking attacks: not intended
as an input to the pool). It seems to me like the output of
&lt;code&gt;arch_get_random_long&lt;/code&gt; is being XORed in with the extracted output,
not with the pool.&lt;/p&gt;
&lt;p&gt;If I were to put on my tin-foil hat, I would suggest that the
difficulty has now been moved from being able to subvert the pool as
one of its entropy sources (which we think is impossible), versus
being able to see what you're about to be XORed with. The latter seems
a lot closer to the realm of stuff a microcode instruction can do.&lt;/p&gt;
&lt;p&gt;To put it into Python:&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;inspect&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;currentframe&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;random&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;getrandbits&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;extract_buf&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="sd"&gt;"""Gets 16 bytes from the pool, and mixes them with RDRAND output.&lt;/span&gt;

&lt;span class="sd"&gt;    """&lt;/span&gt;
    &lt;span class="n"&gt;pool_bits&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;extract_from_pool&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;rdrand_bits&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;rdrand&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt;  &lt;span class="n"&gt;pool_bits&lt;/span&gt; &lt;span class="o"&gt;^&lt;/span&gt; &lt;span class="n"&gt;rdrand_bits&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;extract_from_pool&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="sd"&gt;"""Pretend to get some good, unpredictable bytes from the pool.&lt;/span&gt;

&lt;span class="sd"&gt;    Actually gets a long with some non-cryptographically secure random&lt;/span&gt;
&lt;span class="sd"&gt;    bits from random.getrandbits, which is usually a Mersenne Twister.&lt;/span&gt;

&lt;span class="sd"&gt;    """&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;getrandbits&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;32&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;rdrand&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="sd"&gt;"""&lt;/span&gt;
&lt;span class="sd"&gt;    A malicious hardware instruction.&lt;/span&gt;
&lt;span class="sd"&gt;    """&lt;/span&gt;
    &lt;span class="n"&gt;pool_bits&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;currentframe&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;f_back&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;f_locals&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;"pool_bits"&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;pool_bits&lt;/span&gt; &lt;span class="o"&gt;^&lt;/span&gt; &lt;span class="mh"&gt;0xabad1dea&lt;/span&gt;

&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;__name__&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s2"&gt;"__main__"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;assert&lt;/span&gt; &lt;span class="n"&gt;extract_buf&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mh"&gt;0xabad1dea&lt;/span&gt;
&lt;/pre&gt;


&lt;p&gt;Why can't RDRAND work like this?&lt;/p&gt;
&lt;p&gt;Some comments based on feedback I've gotten so far:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;This attack does not need to know where the PRNG state lives in
memory. First of all, this isn't an attack on the PRNG state, it's on
the PRNG output. Secondly, the instruction only needs to peek ahead at
what is about to happen (specifically, what's about to be XORed with)
the RDRAND output. That doesn't require knowing where the PRNG state
(or its output) is being stored in memory; we're already talking
register level at that point.&lt;/li&gt;
&lt;li&gt;While it's certainly true that if you can't trust the CPU, you
can't trust anything, that doesn't really make this problem go away.
&lt;code&gt;RDRAND&lt;/code&gt; being broken wouldn't make software crash, which is a lot
harder for almost all other instructions. &lt;code&gt;RDRAND&lt;/code&gt; being broken
wouldn't result in measurable side-effects, unlike what would happen
if &lt;code&gt;PCLMULDQ&lt;/code&gt; contained a back door. Furthermore, it's a lot easier to
backdoor one single microcode instruction and a lot more plausible and
feasible for a CSPRNG to be backdoored than it is to think of a CPU as
some kind of intelligent being that's actively malicious or being
remotely controlled.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For what it's worth, it seems &lt;a href="https://twitter.com/zooko/status/392334674690723840"&gt;Zooko agrees with me&lt;/a&gt;.&lt;/p&gt;&lt;/div&gt;</description><category>crypto</category><guid>https://www.lvh.io/posts/2013/10/thoughts-on-rdrand-in-linux.html</guid><pubDate>Sun, 20 Oct 2013 04:47:00 GMT</pubDate></item></channel></rss>