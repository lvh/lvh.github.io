<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>lvh (security)</title><link>http://www.lvh.io/</link><description></description><atom:link href="http://www.lvh.io/categories/security.xml" type="application/rss+xml" rel="self"></atom:link><language>en</language><lastBuildDate>Tue, 30 Dec 2014 13:50:14 GMT</lastBuildDate><generator>http://getnikola.com/</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>On discussing software security improvements</title><link>http://www.lvh.io/posts/on-discussing-software-security-improvements.html</link><dc:creator>lvh</dc:creator><description>&lt;div&gt;&lt;p&gt;A common criticism of information security folks is that they tend to
advise people to not do any crypto. Through projects like &lt;a class="reference external" href="https://www.crypto101.io/"&gt;Crypto
101&lt;/a&gt;, I've attempted to make a small contribution towards fixing
that.&lt;/p&gt;
&lt;p&gt;In the open source world, various people often try to improve the
security of a project. Because designing secure systems is pretty
hard, they often produce flawed proposals. The aforemetioned tendency
for infosec-conscious people to tell them to stop doing crypto is
experienced as unwelcoming, even dismissive. Typically, the only thing
that's accomplished is that a lot of feelings get hurt; it seems to
only rarely result in improved software.&lt;/p&gt;
&lt;p&gt;I think that's quite unfortunate. I think open source is great, and we
should be not just welcoming and inclusive, but aiming to produce
secure software. Furthermore, even if a flawed proposal is
unsalvageable, a clear description of &lt;em&gt;why&lt;/em&gt; it is flawed will
presumably result in fewer negative interactions. Best case scenario,
the issues with a proposal can be discussed and potentially rectified.&lt;/p&gt;
&lt;p&gt;In an effort to improve this situation, I'm documenting what I believe
to be a useful way to discuss security changes and their tradeoffs. As
Zooko has taught me:&lt;/p&gt;
&lt;blockquote&gt;
Security isn't about perfect versus imperfect or about better versus
worse, it's about &lt;em&gt;this&lt;/em&gt; attack surface versus &lt;em&gt;that&lt;/em&gt; attack
surface.&lt;/blockquote&gt;
&lt;p&gt;This document aims to be the equivalent of an &lt;a class="reference external" href="http://www.sscce.org/"&gt;SSCCE&lt;/a&gt; for generic bug
reports: a blueprint for making suggestions likely to lead to
productive discourse, as long as we can agree that we're trying to
produce more secure software, as well as provide a welcoming
development environment.&lt;/p&gt;
&lt;div class="section" id="important-points"&gt;
&lt;h2&gt;Important points&lt;/h2&gt;
&lt;p&gt;A good proposal should contain:&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;A brief description of what you're suggesting.&lt;/li&gt;
&lt;li&gt;A description of the attack model you're considering, why the
current system does not address this issue, and why the suggested
system &lt;em&gt;does&lt;/em&gt; address this issue.&lt;/li&gt;
&lt;li&gt;A motivation of the attack model. Why is it important that this
issue is actually addressed?&lt;/li&gt;
&lt;li&gt;How does this change affect the attack surface (i.e. all of the
ways an attacker can attempt to attack a system)?&lt;/li&gt;
&lt;li&gt;What does the user experience for all users of the system look
like? Many cryptosystems fall over because they're simply unusable
for most users of the system.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div class="section" id="an-example"&gt;
&lt;h2&gt;An example&lt;/h2&gt;
&lt;p&gt;Wul (the widely underestimated language, pronounced &lt;em&gt;/wool/&lt;/em&gt;) is a
general purpose programming language. It has a package repository,
WuPI (the Wul package index, pronounced &lt;em&gt;/woopie/&lt;/em&gt;), the de facto
standard for distributing and installing Wul software.&lt;/p&gt;
&lt;p&gt;WuPI uses TLS as a secure transport. The WuF (Wul foundation,
pronounced &lt;em&gt;/woof/&lt;/em&gt;), maintains a root certificate, distributed with
Wul. Thanks to a well-managed system of intermediary CAs run by a
tireless army of volunteers, this means that both package authors and
consumers know they're talking to the real WuPI.&lt;/p&gt;
&lt;p&gt;Alice is the WuPI BDFL. Bob is a Wul programmer, and would like to
improve the security of WuPI.&lt;/p&gt;
&lt;p&gt;While consumers and authors know that they're talking to the real
WuPI, there is no protection against a malicious WuPI endpoint. (This
problem was recently made worse because WuPI introduced a CDN, greatly
increasing the number of people who could own a node.). You know that
you're talking to something with a WuF-signed certificate (presumably
WuPI, provided the WuF has done a good job managing that certificate),
but you have no idea if that thing is being honest about the packages
it serves you.&lt;/p&gt;
&lt;p&gt;Bob believes WuPI could solve this by using GPG signatures.&lt;/p&gt;
&lt;p&gt;He starts with a brief description of the suggestion:&lt;/p&gt;
&lt;blockquote&gt;
I would like to suggest that WuPI grows support for GPG signatures
of packages. These signatures would be created when a package author
uploads a package. They would optionally be verified when the user
downloads a package.&lt;/blockquote&gt;
&lt;p&gt;He continues with the attack model being considered:&lt;/p&gt;
&lt;blockquote&gt;
I believe this would secure WuPI consumers against a malicious WuPI
endpoints. A malicious WuPI endpoint (assuming it acquires an
appropriate certificate) is currently free to deliver whatever
packages it wants.&lt;/blockquote&gt;
&lt;p&gt;He explains why the current model doesn't address this:&lt;/p&gt;
&lt;blockquote&gt;
The current system assures authenticity and secrecy of the stream
(through TLS), and it ensures that the server authenticates itself
with a WuPI/WuF certificate. It does not ensure that the package is
what the author uploaded.&lt;/blockquote&gt;
&lt;p&gt;He explains why he believes his model does address this:&lt;/p&gt;
&lt;blockquote&gt;
Because the signatures are produced by the author's GPG key, a
malicious WuPI endpoint would not be able to forge them. Therefore,
a consumer is sure that a package with a valid signature is indeed
from the author.&lt;/blockquote&gt;
&lt;p&gt;He explains why this attack model is important:&lt;/p&gt;
&lt;blockquote&gt;
With the new CDN support, the number of people with access to such a
certificate has greatly increased. While I certainly trust all of
the volunteers involved, it would be nice if we didn't &lt;em&gt;have&lt;/em&gt; to.
Furthermore, the software on the servers can always be vulnerable to
attack; as a high-value target, it certainly isn't inconceivable
that an attacker would use an unknown vulnerability to take over a
WuPI endpoint.&lt;/blockquote&gt;
&lt;p&gt;He (believes to) address the attack surface:&lt;/p&gt;
&lt;blockquote&gt;
Because the signatures are optional, the attack surface remains the
same.&lt;/blockquote&gt;
&lt;p&gt;Finally, he addresses the user experience:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The weak point of this scheme is most likely the user experience,
because users historically seem to dislike using GPG.&lt;/p&gt;
&lt;p&gt;I am hopeful that this increased value of participating in the GPG
web of trust will mean that more people participate.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Alice reviews this, and notes a flaw in the proposal:&lt;/p&gt;
&lt;blockquote&gt;
This proposal aims to address a security flaw when the WuPI endpoint
is malicious by adding signatures. However, a malicious WuPI
endpoint can lie by omission, and claim a package was never signed
by the author.&lt;/blockquote&gt;
&lt;p&gt;Bob now realizes this issue, and suggests an improvement:&lt;/p&gt;
&lt;blockquote&gt;
This could be rectified if the user insists on a signature for
packages they expect to be signed.&lt;/blockquote&gt;
&lt;p&gt;As a side note, Alice notes that the attack surface does increase:&lt;/p&gt;
&lt;blockquote&gt;
This places trust in author's ability to manage private keys, which
has historically been shown to be problematic. That introduces a new
attack vector: an attacker can attempt to go after the author's
private key.&lt;/blockquote&gt;
&lt;p&gt;Regardless of the outcome of this conversation, there actually was a
conversation. I believe this to be an improvement over the overall
status quo.&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;</description><category>crypto</category><category>python</category><category>security</category><guid>http://www.lvh.io/posts/on-discussing-software-security-improvements.html</guid><pubDate>Tue, 29 Jul 2014 06:58:27 GMT</pubDate></item><item><title>Securing against timing attacks with Twisted</title><link>http://www.lvh.io/posts/2013/01/securing-against-timing-attacks-with-twisted.html</link><dc:creator>lvh</dc:creator><description>&lt;div&gt;&lt;h3&gt;What are timing attacks?&lt;/h3&gt;
&lt;p&gt;Timing attacks are side-channel attacks that rely on inferring secret
information from operations by measuring how long they take to execute.&lt;/p&gt;
&lt;p&gt;A complete explanation is outside of the scope of this article, but
the &lt;a href="https://en.wikipedia.org/wiki/Timing_attack"&gt;Wikipedia article&lt;/a&gt;
might be a good starting point for the interested reader.&lt;/p&gt;
&lt;h3&gt;Why should I care about them?&lt;/h3&gt;
&lt;p&gt;Because they can creep in before you know it, and break your otherwise
fine system.&lt;/p&gt;
&lt;p&gt;A common way they're introduced are string comparisons. String
comparisons in many langauge implementations, including all
implementations of Python I know of, short-circuit. They work roughly
like this:&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;strcmp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;s1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;s2&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;s1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;!=&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;s2&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;False&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;c1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;c2&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;zip&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;s1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;s2&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;c1&lt;/span&gt; &lt;span class="o"&gt;!=&lt;/span&gt; &lt;span class="n"&gt;c2&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;False&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;True&lt;/span&gt;
&lt;/pre&gt;


&lt;p&gt;This means that as soon as they can prove the two strings are not
equal, they return &lt;code&gt;False&lt;/code&gt; and ignore the rest of the string. This is
a very simple and effective performance optimization. And that's
precisely why, from a security point of view, it's a liability.&lt;/p&gt;
&lt;p&gt;Since it takes longer to compare "The quick brown fox jumps over the
lazy dog" to "The quick brown fox jumps over the lazy god" than it
does to compare it to "Lorem ipsum", or even a Lipsum of the same
length, an attacker can use that timing information to figure out what
the original string is that is being compared to, even when he
shouldn't.&lt;/p&gt;
&lt;h3&gt;Why did you care?&lt;/h3&gt;
&lt;p&gt;Password resets.&lt;/p&gt;
&lt;p&gt;The typically recommended way of doing them is to generate a random
number, one that's sufficiently long that an attacker can't guess it.
Then, you relay that number to the user using some alternative channel
(usually a URL in an e-mail).&lt;/p&gt;
&lt;p&gt;Bar the random number, those URLs are predictable. That means an
attacker can trivially try any number he wants. If an attacker is
allowed to do that enough, he could measure timing differences between
different numbers (introduced by string comparison functions that
short-circuit or even by the database's index) to efficiently deduce
the value of a "good" number.&lt;/p&gt;
&lt;p&gt;Also, if membership is private, an attacker may exploit timing
differences in the password reset &lt;em&gt;request&lt;/em&gt; function to farm e-mail
addresses.&lt;/p&gt;
&lt;h3&gt;How do I protect against timing attacks?&lt;/h3&gt;
&lt;p&gt;Just to be clear: timing attacks are a complicated problem, and this
article describes just one strategy I've applied to help secure
against them.&lt;/p&gt;
&lt;p&gt;The easiest way to prevent a timing attack is to make sure that the
timing your hypothetical attacker can measure is unrelated to the work
you have to do.&lt;/p&gt;
&lt;p&gt;Since I was using &lt;a href="http://twistedmatrix.com"&gt;Twisted&lt;/a&gt; and
&lt;a href="http://amp-protocol.net/"&gt;AMP&lt;/a&gt;, this was actually quite easy. I wrote
a decorator for AMP responder functions that does exactly that:&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;_immediateResponder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;"""&lt;/span&gt;
&lt;span class="sd"&gt;    A decorator for responder functions that should return immediately and&lt;/span&gt;
&lt;span class="sd"&gt;    execute asynchronously, as a defense against timing attacks.&lt;/span&gt;

&lt;span class="sd"&gt;    The responder decorator should be applied after (above) this decorator::&lt;/span&gt;

&lt;span class="sd"&gt;        @SomeCommand.responder&lt;/span&gt;
&lt;span class="sd"&gt;        @_immediateResponder&lt;/span&gt;
&lt;span class="sd"&gt;        def responder(...):&lt;/span&gt;
&lt;span class="sd"&gt;            ....&lt;/span&gt;

&lt;span class="sd"&gt;    This should be timing attack resistant since it is unconditional: the&lt;/span&gt;
&lt;span class="sd"&gt;    the AMP response is returned immediately, and the real responder is&lt;/span&gt;
&lt;span class="sd"&gt;    scheduled to run at the next chance the reactor has to do so.&lt;/span&gt;

&lt;span class="sd"&gt;    This only works with AMP commands with empty responses. That's probably a&lt;/span&gt;
&lt;span class="sd"&gt;    good idea anyway: almost all information you could add to the response&lt;/span&gt;
&lt;span class="sd"&gt;    is liable to introduce a timing attack vulnerability.&lt;/span&gt;

&lt;span class="sd"&gt;    Since this precludes your ability to communicate success or failure to&lt;/span&gt;
&lt;span class="sd"&gt;    the caller, the decorated function should return quite quickly (or, if it&lt;/span&gt;
&lt;span class="sd"&gt;    can't, that should be clearly documented). Otherwise, you may end up in a&lt;/span&gt;
&lt;span class="sd"&gt;    a race condition, where the caller assumes the operation has completed,&lt;/span&gt;
&lt;span class="sd"&gt;    but it is in progress or hasn't started yet.&lt;/span&gt;

&lt;span class="sd"&gt;    The original responder function is available on the decorated function as&lt;/span&gt;
&lt;span class="sd"&gt;    the ``responderFunction`` attribute.&lt;/span&gt;
&lt;span class="sd"&gt;    """&lt;/span&gt;
    &lt;span class="nd"&gt;@functools.wraps&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;wrapped&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;kwargs&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;reactor&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;callLater&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;kwargs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;{}&lt;/span&gt;

    &lt;span class="n"&gt;wrapped&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;responderFunction&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;wrapped&lt;/span&gt;
&lt;/pre&gt;


&lt;p&gt;When I get an incoming RPC call, the function doing the actual work is
scheduled to run at the next reactor iteration. Then, an empty
response is returned. All of this happens in amortized constant time,
and independent of any secrets. As a result, it can't really leak
much about them.&lt;/p&gt;
&lt;h3&gt;An abstraction too high&lt;/h3&gt;
&lt;p&gt;The above was an effective response to a proof-of-concept timing
attack exploit. Unfortunately, that doesn't mean you've fixed every
timing attack.&lt;/p&gt;
&lt;p&gt;In particular, this example is a few layers of abstraction removed
from the grit of real-world I/O. Just because I returned &lt;code&gt;{}&lt;/code&gt; (an
empty response) immediately, doesn't mean the underlying IO happens
immediately.&lt;/p&gt;
&lt;p&gt;In particular, the write output latency could be coerced to depend on
the computation time, because the function passed to it could be
executed before the &lt;code&gt;write&lt;/code&gt;. If that happens, and the time it takes is
dependant on some secret, delaying the write, the latency on the
attacker's side could be used to measure the work done.&lt;/p&gt;
&lt;p&gt;I have not yet been able to turn the above into a working exploit.&lt;/p&gt;
&lt;p&gt;There are a number of ways this could be mitigated. Since there's no
working exploit, it's unclear if this mitigation would render a timing
attack infeasible.&lt;/p&gt;
&lt;p&gt;One way I've considered to mitigate this is to limit the time that the
reactor is blocked. In my concrete example, this was fortunately
already the case, since one of the first things it did was defer to a
thread that released the GIL (to compute the key from the password
using &lt;code&gt;scrypt&lt;/code&gt;). Alternatively, if you're doing this for Python
code, you could write your function cooperatively.&lt;/p&gt;&lt;/div&gt;</description><category>amp</category><category>security</category><category>twisted</category><guid>http://www.lvh.io/posts/2013/01/securing-against-timing-attacks-with-twisted.html</guid><pubDate>Thu, 31 Jan 2013 02:55:00 GMT</pubDate></item></channel></rss>