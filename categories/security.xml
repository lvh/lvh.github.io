<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>lvh (security)</title><link>https://www.lvh.io/</link><description></description><atom:link rel="self" type="application/rss+xml" href="https://www.lvh.io/categories/security.xml"></atom:link><language>en</language><lastBuildDate>Tue, 03 May 2016 16:38:21 GMT</lastBuildDate><generator>https://getnikola.com/</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Supersingular isogeny Diffie-Hellman 101</title><link>https://www.lvh.io/posts/supersingular-isogeny-diffie-hellman-101.html</link><dc:creator>lvh</dc:creator><description>&lt;div&gt;&lt;p&gt;Craig Costello, Patrick Longa and Michael Naehrig, three cryptographers at
Microsoft Research, recently published a &lt;a href="https://eprint.iacr.org/2016/413"&gt;paper&lt;/a&gt; on supersingular
isogeny Diffie-Hellman. This paper garnered a lot of interest in the security
community and even made it to the front page of Hacker News. Most of the
discussion around it seemed to be how no-one understands isogenies, even
within cryptography-literate communities. This article aims to give you a
high-level understanding of what this cryptosystem is and why it works.&lt;/p&gt;
&lt;p&gt;This post assumes that you already know how Diffie-Hellman works in the
abstract, and that you know elliptic curves are a mathematical construct that
you can use to perform Diffie-Hellman operations, just like you can with the
integers &lt;em&gt;mod p&lt;/em&gt; (that would be "regular" Diffie-Hellman). If that was
gibberish to you and you'd like to know more, check out &lt;a href="https://www.crypto101.io"&gt;Crypto 101&lt;/a&gt;, my
free introductory book on cryptography. You don't need a math background to
understand those concepts at a high level. The main difference is that Crypto
101 sticks to production cryptography, while this is still experimental.&lt;/p&gt;
&lt;p&gt;It's not surprising that isogeny-based cryptography is so confusing. Up until
recently, it was unambiguously in the realm of research, not even close to
being practically applicable. Its mathematical underpinnings are much more
complex than regular elliptic curves, let alone integers &lt;em&gt;mod p&lt;/em&gt;. It also
looks superficially similar to elliptic curve Diffie-Hellman, which only adds
to the confusion.&lt;/p&gt;
&lt;p&gt;With that, let's begin!&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What is this paper about?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Supersingular isogeny Diffie-Hellman (SIDH) is one of a handful of
"post-quantum" cryptosystems. Those are cryptosystems that will remain secure
even if the attacker has access to a large quantum computer. This has nothing
to do with quantum cryptography (for example, quantum key distribution)
beyond their shared quantum mechanical underpinning.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Why should I care about quantum computers?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;General quantum computers are not useful as general-purpose computing devices,
but they can solve some problems much faster than classical
computers. Classical computers can emulate quantum computers, but only with
exponential slowdown. A sufficiently large quantum computer could break most
production cryptography, including cryptosystems based on the difficulty of
factoring large numbers (like RSA), taking discrete logs over the integers
&lt;em&gt;mod p&lt;/em&gt; (like regular DH), or taking discrete logs over elliptic curves (like
ECDH and ECDSA). To quantify that, consider the following table:&lt;/p&gt;
&lt;p&gt;&lt;img alt="quantum computer attack cost versus classical" src="https://www.lvh.io/img/post-quantum/quantum-computer-relative-cost.png"&gt;&lt;/p&gt;
&lt;p&gt;In this table, n refers to the modulus size for RSA, and the field size for
ECC. Look at the rightmost column, which represents time taken by the
classical algorithm, and compare it to the "time" columns, which represent how
much a quantum computer would take. As &lt;em&gt;n&lt;/em&gt; increases, the amount of time the
quantum computer would take stays in the same ballpark, whereas for a
classical computer it increases (almost) exponentially. Therefore, increasing
n is an effective strategy for keeping up with ever-faster classical
computers, but it is ineffective at increasing the run time for a quantum
computer.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Aah! Why isn't everyone panicking about this?!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The good news is that these large quantum computers don't exist yet.&lt;/p&gt;
&lt;p&gt;If you look at the qubits column, you'll see that these attacks require large
universal quantum computers. The state of the art in those only has a handful
of qubits. In 2011, IBM successfully factored 143 using a 4-qubit quantum
computer. Scaling the number of qubits up is troublesome. In that light,
larger key sizes may prove effective after all; we simply don't know yet how
hard it is to build quantum computers that big.&lt;/p&gt;
&lt;p&gt;D-wave, a quantum computing company, has produced computers with 128 and 512
qubits and even &amp;gt;1000 qubits. While there is some discussion if D-waves
provide quantum speedup or are even real quantum computers at all; there is no
discussion that they are not &lt;em&gt;universal&lt;/em&gt; quantum computers. Specifically, they
only claim to solve one particular problem called quantum annealing. The 1000
qubit D-Wave 2X cannot factor RSA moduli of ~512 bits or solve discrete logs
on curves of ~120 bits.&lt;/p&gt;
&lt;p&gt;The systems at risk implement asymmetric encryption, signatures, and
Diffie-Hellman key exchanges. That's no accident: all post-quantum
alternatives are asymmetric algorithms. Post-quantum secure symmetric
cryptography is easier: we can just use bigger key sizes, which are still
small enough to be practical and result in fast primitives. Quantum computers
simply halve the security level, so all we need to do to maintain a 128 bit
security level is to use ciphers with 256 bit keys, like Salsa20.&lt;/p&gt;
&lt;p&gt;Quantum computers also have an advantage against SIDH, but both are still
exponential in the field size. The SIDH scheme in the new paper has 192 bits
of security against a classical attacker, but still has 128 bits of security
against a quantum attacker. That's in the same ballpark as most symmetric
cryptography, and better than the 2048-bit RSA certificates that underpin the
security of the Internet.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What makes this paper special?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Post-quantum cryptography has been firmly in the realm of academic research
and experiments. This paper makes significant advancements in how practically
applicable SIDH is.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Being future-proof sounds good. If this makes it practical, why don't we
start using it right now?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;SIDH is a young cryptosystem in a young field, and hasn't had the same level
of scrutiny as some of the other post-quantum cryptosystems, let alone the
"regular" cryptosystems we use daily. Attacks only get better, they never get
worse. It's possible that SIDH is insecure and we just don't know how to break
it yet. It does have a good argument for why quantum algorithms wouldn't be
able to crack it (more on that later), but that's a hypothesis, not a proof.&lt;/p&gt;
&lt;p&gt;The new performance figures from this paper are impressive, but this system is
still much slower than the ones we use today. Key generation and key exchange
take a good 50 million cycles or so each. That's about a thousand times slower
than Curve25519, a curve designed about 10 years ago. Key sizes are also much
larger: SIDH public keys are 751 bytes, whereas Curve25519 keys are only 32
bytes. For on-line protocols like HTTPS operating over TCP, that's a
significant cost.&lt;/p&gt;
&lt;p&gt;Finally, there are issues with implementing SIDH safely. Systems like
Diffie-Hellman over integers &lt;em&gt;mod p&lt;/em&gt; are much less complex than elliptic curve
Diffie-Hellman (ECDH), let alone SIDH. With ECDH and ECC in general, we've
seen new implementation difficulties, especially with early curves. Point
addition formulas would work, unless you were adding a point to itself. You
have to check that input points are on the curve, or leak the secret key
modulo some small order. These are real implementation problems, even though
we know how to solve them.&lt;/p&gt;
&lt;p&gt;This is nothing compared to the difficulties implementing SIDH. Currently,
SIDH security arguments rely on honest peers. A peer that gives you a
pathological input can utterly break the security of the scheme. To make
matters worse, while we understand how to verify inputs for elliptic curve
Diffie-Hellman, we don't have a way to verify inputs for isogeny-based
cryptography at all. We don't have much research to fall back on here
either. This isn't a SIDH-specific problem; post-quantum cryptography isn't
mature enough yet to have implementation issues like these nailed down
yet. (For an example from lattice-based cryptography, see the recent paper by
&lt;a href="https://eprint.iacr.org/2016/415"&gt;Bindel et al&lt;/a&gt;.)&lt;/p&gt;
&lt;p&gt;I don't want to diminish the importance of this paper in any way!  Just
because it's not something that your browser is going to be doing tomorrow
doesn't mean it's not an impressive accomplishment. It's just a step on the
path that might lead to production crypto one day.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;OK, fine. Why is this so different from elliptic curve Diffie-Hellman?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;While SIDH and ECDH both use elliptic curves, they're different beasts. SIDH
generates new curves to perform a DH exchange, whereas ECDH uses points on one
fixed curve. These supersingular curves also have different properties from
regular curves. Using a supersingular curve for regular elliptic curve
operations would be horribly insecure. If you have some background in elliptic
curves: supersingular curves have a tiny embedding degree, meaning that
solving the ECDLP over &lt;code&gt;F(p)&lt;/code&gt; can easily be transformed into solving the DLP
over &lt;code&gt;F(p^n)&lt;/code&gt; where &lt;code&gt;n&lt;/code&gt; is that small embedding degree. Most curves have large
embedding degrees, meaning that solving the ECDLP directly is easier than
translating it into a DLP and then solving that.  You generally have to go out
of your way to find a curve with a small embedding degree. That is only done
in specialized systems, like for pairing-based cryptography, or, as in this
case, supersingular isogeny-based Diffie-Hellman.&lt;/p&gt;
&lt;p&gt;Let's recap ECDH. Public keys are points on a curve, and secret keys are
numbers. Alice and Bob agree on the parameters of the exchange ahead of time,
such as the curve &lt;em&gt;E&lt;/em&gt; and a generator point &lt;em&gt;P&lt;/em&gt; on that curve. Alice picks a
secret integer &lt;em&gt;a&lt;/em&gt; and computes her public key &lt;em&gt;aP&lt;/em&gt;. Bob picks a secret
integer &lt;em&gt;b&lt;/em&gt; and computes his public key &lt;em&gt;bP&lt;/em&gt;. Alice and Bob send each other
their public keys, and multiply their secret key by the other peer's public
key. Since &lt;em&gt;abP = baP&lt;/em&gt;, they compute the same secret. Since an attacker has
neither secret key, they can't compute the shared secret.&lt;/p&gt;
&lt;p&gt;SIDH is different. Secret keys are isogenies...&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Whoa whoa whoa. What the heck are isogenies?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;An isogeny between elliptic curves is a function from one elliptic curve to
another that preserves base points. That means it takes points on one curve
and returns points on the other curve. Every point on the input curve will map
to a point on the output curve; but multiple points may map to the same
point. Formally speaking, the isogeny is surjective. An isogeny is also a
homomorphism, that is, it preserves the structure of the curve. For any two
points P and Q, &lt;code&gt;phi(P + Q) = phi(P) + phi(Q)&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;We have a bunch of formulas for generating isogenies from a curve and a
point. You might remember that the set of values a function takes is its
"domain", and the set of values it returns is called its "codomain". The
domain of such an isogeny is the curve you give it; its codomain might be the
same curve, or it might be a different one. In general, for SIDH, we care
about the case where it produces a new curve.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;OK, so explain how SIDH works again.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Roughly speaking, a secret key is an isogeny, and a public key is an elliptic
curve. By "mixing" their isogeny with the peer's public curve, each peer
generates a secret curve. The two peers will generally generate different
curves, but those curves will have the same j-invariant.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Wait, what's a j-invariant?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The j-invariant is a number you can compute for a particular curve. Perhaps
the best analogy would be the discriminant for quadratic equation you might
remember from high school math; it's a single number that tells you something
interesting about the underlying curve. There are different formulas for
curves in different forms. For example, for a curve in short Weierstrass form
&lt;code&gt;y^2 = x^3 + ax + b&lt;/code&gt;, the j-invariant is:&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span&gt;&lt;/span&gt;j(E) = (1728 * 4a^3)/(4a^3 + 27b^2)
&lt;/pre&gt;


&lt;p&gt;The j-invariant has a few cool properties: for example, while this is the
formula for the short Weierstrass form, the value of j doesn't change if you
put the same curve in a different form. Also, all curves with the same
j-invariant are isomorphic. However, for SIDH you don't really care about
these properties; you just care that the j-invariant is a number you can
compute and it'll be the same for the two secret curves that are generated by
the DH exchange.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;OK, try explaining SIDH again.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The protocol fixes a supersingular curve E and four points on that
curve: PA, QA, PB, QB.&lt;/p&gt;
&lt;p&gt;Alice picks two random integers, mA and nA. She takes a linear combination of
those two integers with PA and QA to produce a random point RA, so:&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span&gt;&lt;/span&gt;RA = nA * PA + mA * QA
&lt;/pre&gt;


&lt;p&gt;That random point defines Alice's secret isogeny through the isogeny formulas
I talked about above. The codomain of that isogeny forms Alice's public
curve. Alice transforms points PB and QB with the isogeny. She sends Bob her
public curve, and the two transformed points.&lt;/p&gt;
&lt;p&gt;Bob does the same thing, except with A and B swapped.&lt;/p&gt;
&lt;p&gt;Once Alice gets Bob's public key, she applies mA and nA again to the
corresponding transformed points she got from Bob. She generates a new isogeny
phiBA from the resulting point just like she did before to generate her
private key. That isogeny's codomain will be an elliptic curve EBA.&lt;/p&gt;
&lt;p&gt;When Bob performs his side of the exchange, he'll produce a different isogeny
and a different elliptic curve EAB; but it will have the same j-invariant as
the curve Alice computed.  That j-invariant is the shared key.&lt;/p&gt;
&lt;p&gt;I've compiled a &lt;a href="https://dl.dropboxusercontent.com/u/38476311/Supersingular%20Isogeny%20Elliptic%20Curve%20Cryptography%20--%20Sage.pdf"&gt;transcript&lt;/a&gt; of a Diffie-Hellman exchange using
Sage so you can see a (toy!) demo in action.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;I know a little about elliptic curves. I thought they were always
non-singular. What's a supersingular elliptic curve but a contradiction in
terms?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;You're right! Supersingular elliptic curves are somewhat confusingly
named. Supersingular elliptic curves are still elliptic curves, and they are
non-singular just like all other elliptic curves. The "supersingular" refers
to the singular values of the j-invariant. Equivalently, the Hasse-Witt matrix
will be singular as well (and therefore the Hasse invariant will be 0).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;So, why does it matter that the curve is supersingular?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Firstly, computing the isogeny is much easier on supersingular curves than on
ordinary (not supersingular) elliptic curves. Secondly, if the curve is
ordinary, the scheme can be broken in subexponential time by a quantum
attacker.&lt;/p&gt;
&lt;p&gt;Isogeny-based cryptography using ordinary curves was considered as a
post-quantum secure cryptosystem before SIDH. However, Childs et al. showed a
subexponential quantum algorithm in 2010. This paper appeared to have ended
isogeny-based cryptography: it was already slower than other post-quantum
systems, and now it was shown that it wasn't even post-quantum secure.&lt;/p&gt;
&lt;p&gt;Because supersingular curves are rare, they had not previously been considered
for isogeny-based cryptography. However, the paper itself suggested that
supersingular curves might be worth examining, so it ended up pushing research
in a new direction rather than ending it.&lt;/p&gt;
&lt;p&gt;Explaining why the supersingular curve makes the problem quantum-hard is
tricky without being thoroughly familiar with isogenies and quantum
computing. If you're really interested, &lt;a href="https://arxiv.org/pdf/1012.4019v2.pdf"&gt;the Childs paper&lt;/a&gt; explains
how the quantum attack in the ordinary case works. Informally, in the ordinary
case, there is a group action (the &lt;em&gt;isogeny star operator&lt;/em&gt;) of the ideal class
group onto the set of isomorphism classes of isogenous curves with the same
endomorphism ring. That can be shown to be a special case of the abelian group
hidden shift problem, which can be solved quickly on a quantum computer. In
the supersingular case, there is no such group action to exploit. (If you're
trying to solve for this at home; this is why SIDH needs to define the 4
points PA, PB, QA, QB.)&lt;/p&gt;
&lt;p&gt;&lt;em&gt;I would like to thank Thomas Ptacek for reviewing this blog post and bearing
with me as I struggle through trying to come up with human-readable
explanations for all of this stuff; and Sean Devlin for reminding me that Sage
is an excellent educational tool.&lt;/em&gt;&lt;/p&gt;&lt;/div&gt;</description><category>crypto</category><category>security</category><guid>https://www.lvh.io/posts/supersingular-isogeny-diffie-hellman-101.html</guid><pubDate>Sat, 30 Apr 2016 16:00:28 GMT</pubDate></item><item><title>Introducing Teleport</title><link>https://www.lvh.io/posts/introducing-teleport.html</link><dc:creator>lvh</dc:creator><description>&lt;div&gt;&lt;p&gt;I'm happy to introduce &lt;a href="https://github.com/gravitational/teleport"&gt;Teleport&lt;/a&gt;, a new open source platform for
managing SSH infrastructure. Teleport is built by &lt;a href="http://www.gravitational.com/"&gt;Gravitational&lt;/a&gt;, a Y
Combinator company that ships SaaS on any platform. While I'm not a part of
Gravitational, I have been advising them on the Teleport project.&lt;/p&gt;
&lt;p&gt;Most teams don't have a great authentication story. Some rely on passing
passwords around haphazardly, while others rely on copying everyone's
&lt;code&gt;~/.ssh/id_rsa.pub&lt;/code&gt; to every new box. More complex homegrown systems quickly
become unwieldy. These methods are problematic both operationally and from a
security perspective: when security and usability are at odds, security tends
to lose out. For a lot of teams, a single compromised key off of a developer
machine spells disaster, on-boarding new team members is painful, and key
rotation doesn't happen.&lt;/p&gt;
&lt;p&gt;In the last few years, strong multi-factor authentication has become the
norm. Tokens are only valid for a brief period of time, use challenge-response
protocols, or both. Teleport helps bring the same level of sophistication to
infrastructure. It helps system administrators leverage the security benefits
of short-lived certificates, while keeping the operational benefits of
decoupling server authentication from user authentication. It lets you run
isolated clusters, so that a compromise of staging credentials doesn't lead to
a compromise in production. It automatically maintains clear audit logs: who
logged in, when and where they logged in, and what they did once they got
there.&lt;/p&gt;
&lt;p&gt;Teleport comes with a beautiful, usable UI, making it easy to visualize
different clusters and the available machines within them. The UI is optional:
many system administrators will prefer to use their existing SSH client, and
Teleport supports that natively.  Because it implements the &lt;code&gt;SSH_AUTH_SOCK&lt;/code&gt;
protocol, integrating your current CLI workflow is a simple matter of setting
a single environment variable.&lt;/p&gt;
&lt;p&gt;As someone with an open-source background, I'm glad to see this software
released and developed out in the open. A decent SSH key management story
should be available to everyone, and that's what Teleport does. I believe
making this technology more accessible is good for everyone, including
commercial vendors. Democratizing a decent DIY story helps turn their product
into the battle-hardened and commercially supported version of industry best
practice; and as such, I hope this helps grow that market. As a principal
engineer at &lt;a href="https://www.rackspace.com/security/"&gt;Rackspace Managed Security&lt;/a&gt;, I'm excited to start working
towards better authentication stories, both internally and for our customers,
with Teleport as the new baseline.&lt;/p&gt;
&lt;p&gt;Releasing early and often is also an important part of open source
culture. That can be at odds with doing due diligence when releasing
security-critical systems like Teleport, especially when those systems have
non-trivial cryptographic components. We feel Teleport is ready to show to the
public now. To make sure we act as responsibly as possible, I've helped the
Teleport team to join forces with a competent independent third-party
auditor. We're not recommending that you bet the farm on Teleport by running
it in production as your only authentication method just yet, but we do think
it's ready for motivated individuals to start experimenting with it.&lt;/p&gt;
&lt;p&gt;Some people might feel that a better SSH story means you're solving the wrong
problem. It seems at odds with the ideas behind immutable infrastructure and
treating servers as &lt;a href="https://blog.engineyard.com/2014/pets-vs-cattle"&gt;cattle, not pets&lt;/a&gt;. I don't think that's
true. Firstly, even with immutable infrastructure, being able to SSH into a
box to debug and monitor is still incredibly important. Being able to rapidly
deploy a bunch of fixed images quickly may be good, but you still have to know
what to fix first. Secondly, existing systems don't always work that way. It
may not be possible, let alone economically rational, to "port" them
effectively. It's easy to think of existing systems as legacy eyesores that
only exist until you can eradicate them, but they do exist, they're typically
here to stay, and they need a real security story, too.&lt;/p&gt;
&lt;p&gt;Teleport is still in its early stages. It's usable today, and I'm convinced it
has a bright future ahead of it. It's written in a beautiful, hackable Go
codebase, and &lt;a href="https://github.com/gravitational/teleport"&gt;available on Github&lt;/a&gt; starting today.&lt;/p&gt;&lt;/div&gt;</description><category>security</category><guid>https://www.lvh.io/posts/introducing-teleport.html</guid><pubDate>Sat, 12 Mar 2016 17:35:56 GMT</pubDate></item><item><title>Don't expose the Docker socket (not even to a container)</title><link>https://www.lvh.io/posts/dont-expose-the-docker-socket-not-even-to-a-container.html</link><dc:creator>lvh</dc:creator><description>&lt;div&gt;&lt;p&gt;Docker primarily works as a client that communicates with a daemon
process (&lt;code&gt;dockerd&lt;/code&gt;). Typically that socket is a UNIX domain socket
called &lt;code&gt;/var/run/docker.sock&lt;/code&gt;. That daemon is highly privileged;
effectively having root access. Any process that can write to the
&lt;code&gt;dockerd&lt;/code&gt; socket &lt;em&gt;also&lt;/em&gt; effectively has root access.&lt;/p&gt;
&lt;p&gt;This is no big secret. Docker clearly documents this in a bunch of
places, including the introductory documentation. It's an excellent
reason to use Docker Machine for development purposes, even on
Linux. If your regular user can write to the &lt;code&gt;dockerd&lt;/code&gt; socket, then
every code execution vulnerability comes with a free privilege
escalation.&lt;/p&gt;
&lt;p&gt;The warnings around the Docker socket typically come with a (sometimes
implicit) context of being on the host to begin with. Write access to
the socket as an unprivileged user on the host may mean privileged
access to the host, but there seems to be some confusion about what
happens when you get write access to the socket &lt;em&gt;from a
container&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;The two most common misconceptions seem to be that it either doesn't
grant elevated privileges at all, or that it grants you privileged
access within the container (and without a way to break out). This is
false; write access to the Docker socket is root on the host,
regardless on where that write comes from. This is different from
&lt;a href="https://github.com/jpetazzo/dind"&gt;Jerome Pettazoni's &lt;code&gt;dind&lt;/code&gt;&lt;/a&gt;, which gives you Docker-in-Docker;
we're talking about access to the host's Docker socket.&lt;/p&gt;
&lt;p&gt;The process works like this:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The Docker container gets a &lt;code&gt;docker&lt;/code&gt; client of its own, pointed at
   the &lt;code&gt;/var/run/docker.sock&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;The Docker container launches a new container mounting &lt;code&gt;/&lt;/code&gt; on
   &lt;code&gt;/host&lt;/code&gt;. This is the &lt;em&gt;host&lt;/em&gt; root filesystem, not the first
   container.&lt;/li&gt;
&lt;li&gt;The second container chroots to &lt;code&gt;/host&lt;/code&gt;, and is now effectively
   root on the host. (There are a few differences between this and a
   clean login shell; for example, &lt;code&gt;/proc/self/cgroups&lt;/code&gt; will still show
   Docker cgroups. However, the attacker has all of the permissions
   necessary to work around this.)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This is identical to the process you'd use to escalate from outside of
a container. Write access to the Docker socket is root on the host,
full stop; who's writing, or where they're writing from, doesn't
matter.&lt;/p&gt;
&lt;p&gt;Unfortunately, there are plenty of development teams unaware of this
property. I recently came across one, and ended up making a screencast
to unambiguously demonstrate the flaw in their setup (which involved a
container with write access to the Docker socket).&lt;/p&gt;
&lt;iframe width="560" height="315" src="https://www.youtube.com/embed/CB9Aa6QeRaI" frameborder="0" allowfullscreen&gt;&lt;/iframe&gt;

&lt;p&gt;This isn't new; it's been a known property of the way Docker works
ever since the (unfortunately trivially cross-site scriptable) REST
API listening on a local TCP port was replaced with the
&lt;code&gt;/var/run/docker.sock&lt;/code&gt; UNIX domain socket.&lt;/p&gt;&lt;/div&gt;</description><category>docker</category><category>security</category><guid>https://www.lvh.io/posts/dont-expose-the-docker-socket-not-even-to-a-container.html</guid><pubDate>Wed, 23 Sep 2015 21:54:24 GMT</pubDate></item><item><title>Today's OpenSSL bug (for techies without infosec chops)</title><link>https://www.lvh.io/posts/todays-openssl-bug-for-techies-without-infosec-chops.html</link><dc:creator>lvh</dc:creator><description>&lt;div&gt;&lt;h2&gt;What happened?&lt;/h2&gt;
&lt;p&gt;OpenSSL 1.0.1n+ and 1.0.2b+ had a new feature that allows finding an
alternative certificate chain when the first one fails. The logic in
that feature had a bug in it, such that it didn't properly verify if
the certificates in the alternative chain had the appropriate
permissions; specifically, it didn't check if those certificates are
certificate authorities.&lt;/p&gt;
&lt;p&gt;Specifically, this means that an attacker who has a valid certificate
for any domain, can use that certificate to produce new
certificates. Those normally wouldn't work, but the algorithm for
finding the alternative trust chain doesn't check if the valid
certificate can act as a certificate authority.&lt;/p&gt;
&lt;h2&gt;What's a certificate (chain)?&lt;/h2&gt;
&lt;p&gt;A certificate is a bit like an ID card: it has some information about
you (like your name), and is authenticated by a certificate authority
(in the case of an ID, usually your government).&lt;/p&gt;
&lt;h2&gt;What's a certificate authority?&lt;/h2&gt;
&lt;p&gt;A certificate authority is an entity that's allowed to authenticate
certificates. Your computer typically ships with the identity of those
certificate authorities, so it knows how to recognize certificates
authorized by them.&lt;/p&gt;
&lt;p&gt;In the ID analogy, your computer knows how to recognize photo IDs
issued by e.g. California.&lt;/p&gt;
&lt;p&gt;The issue here is that in some cases, OpenSSL was willing to accept
signatures authenticated by certificates that don't have certificate
authority powers. In the analogy, it would mean that it accepted
CostCo cards as valid ID, too.&lt;/p&gt;
&lt;h2&gt;Why did they say it wouldn't affect most users?&lt;/h2&gt;
&lt;p&gt;This basically means "we're assuming most users are using OpenSSL for
vanilla servers", which is probably true. Most servers do use OpenSSL,
and most clients (browsers) don't.&lt;/p&gt;
&lt;p&gt;The bug affects anyone trying to authenticate their peer. That
includes regular clients, and servers doing client
authentication. Regular servers aren't affected, because they don't
authenticate their peer.&lt;/p&gt;
&lt;p&gt;Servers doing client authentication are fairly rare. The biggest
concern is with clients. While browsers typically don't use OpenSSL, a
lot of API clients do. For those few people affected by the bug and
with clients that use OpenSSL, the bug is catastrophic.&lt;/p&gt;
&lt;h2&gt;What's client authentication?&lt;/h2&gt;
&lt;p&gt;The vast majority of TLS connections only authenticate the
server. When the client opens the connection, the server sends its
certificate. The client checks the certificate chain against the list
of certificate authorities that it knows about. The client is
typically authenticated, but over the protocol spoken inside of TLS
(usually HTTP), not at a TLS level.&lt;/p&gt;
&lt;p&gt;That isn't the only way TLS can work. TLS also supports authenticating
clients with certificates, just like it authenticates servers. This is
called mutually authenticated TLS, because both peers authenticate
each other. At Rackspace Managed Security, we use this for all
communication between internal nodes. We also operate our own
certificate authority to sign all of those certificates.&lt;/p&gt;
&lt;h2&gt;What's TLS?&lt;/h2&gt;
&lt;p&gt;TLS is what SSL has been called for way over a decade. The old name
stuck (particularly in the name "OpenSSL"), but you should probably
stop using it when you're talking about the secure protocol, since all
of the versions of the protocol that were called "SSL" have crippling
security bugs.&lt;/p&gt;
&lt;h2&gt;Why wasn't this found by automated testing?&lt;/h2&gt;
&lt;p&gt;I'm not sure. I wish automated testing this stuff was easier. Since
I'm both a user and a big fan of client authentication, which is a
pretty rare feature, I hope to spend more time in the future creating
easy-to-use automated testing tools for this kind of scenario.&lt;/p&gt;
&lt;h2&gt;How big is the window?&lt;/h2&gt;
&lt;p&gt;1.0.1n and 1.0.2b were both released on 11 Jun 2015. The fixes, 1.0.1p
and 1.0.2d, were released today, on 9 Jul 2015.&lt;/p&gt;
&lt;p&gt;The "good news" is that the bad releases are recent. Most people who
have an affected version will be updating regularly, so the number of
people affected is small.&lt;/p&gt;
&lt;p&gt;The bug affected following platforms (non-exhaustive):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It did not affect stock OS X, because they still ship
  0.9.8. However, the bug does affect a stable version shipped through
  Homebrew (1.0.2c).&lt;/li&gt;
&lt;li&gt;&lt;a href="http://people.canonical.com/~ubuntu-security/cve/2015/CVE-2015-1793.html"&gt;Ubuntu is mostly not affected&lt;/a&gt;. The only affected version
  is the unreleased 15.10 (Wily). Ubuntu has already released an
  update for it.&lt;/li&gt;
&lt;li&gt;The bug affects stable releases of Fedora. I previously mistakenly
  reported that the contrary, but that information was based on their
  package version numbers, which did not match upstream. Fedora
  backported the faulty logic to their version of 1.0.1k, which was
  available in Fedora 21 and 22. They have since released patches; see
  &lt;a href="https://bugzilla.redhat.com/show_bug.cgi?id=1241544"&gt;this ticket&lt;/a&gt; for details. Thanks to Major Hayden for the
  correction!&lt;/li&gt;
&lt;li&gt;The bug does not affect Debian stable, but it does affect
  &lt;a href="https://security-tracker.debian.org/tracker/CVE-2015-1793s=openssl"&gt;testing and unstable&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;The bug affects &lt;a href="https://www.archlinux.org/packages/?sort=-last_update"&gt;ArchLinux testing&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;In conclusion&lt;/h2&gt;
&lt;p&gt;The bug is disastrous, but affects few people. If you're running
stable versions of your operating system, you're almost certainly
safe.&lt;/p&gt;
&lt;p&gt;The biggest concern is with software developers using OS X. That
audience uses HTTPS APIs frequently, and the clients to connect to
those APIs typically use OpenSSL. OS X comes with 0.9.8zf by default
now, which is a recent revision of an ancient branch. Therefore,
people have a strong motivation to get their OpenSSL from a
third-party source. The most popular source is Homebrew, which up
until earlier this morning shipped 1.0.2c. The bug affects that
version. If you installed OpenSSL through Homebrew, you should go
update right now.&lt;/p&gt;&lt;/div&gt;</description><category>security</category><guid>https://www.lvh.io/posts/todays-openssl-bug-for-techies-without-infosec-chops.html</guid><pubDate>Thu, 09 Jul 2015 15:26:58 GMT</pubDate></item><item><title>They do take security seriously</title><link>https://www.lvh.io/posts/they-do-take-security-seriously.html</link><dc:creator>lvh</dc:creator><description>&lt;div&gt;&lt;p&gt;Earlier today, I read an &lt;a href="http://www.troyhunt.com/2015/07/we-take-security-seriously-otherwise.html"&gt;article&lt;/a&gt; about the plethora of
information security breaches in recent history. Its title reads:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“We take security seriously”, otherwise known as “We didn’t take it
seriously enough”&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The article then lists a number of companies informing the public that
they've been breached.&lt;/p&gt;
&lt;p&gt;I think this article doesn't just blame the victims of those attacks,
but subjects them to public ridicule. Neither helps anyone, least of
all end users.&lt;/p&gt;
&lt;p&gt;I'm surprised to hear such comments from Troy Hunt. He's certainly an
accomplished professional with extensive security experience. This is
not the first time people have expressed similar thoughts; the
&lt;a href="https://news.ycombinator.com/item?id=9834099"&gt;HN thread&lt;/a&gt; for that article is rife with them.&lt;/p&gt;
&lt;p&gt;The explicit assumption is that these companies wouldn't have gotten
in trouble if only they had taken security more seriously. In a world
where the information services store is increasingly valuable and
software increasingly complex, breaches are going to happen. The idea
that getting breached is their own darn fault is unrealistic.&lt;/p&gt;
&lt;p&gt;This idea is also counterproductive. Firstly, there's one thing all of
the victims being ostracized have in common: they disclosed the
details of the breach. That is exactly what they should have done;
punishing them creates a perverse incentive for victims to hide
breaches in the future, a decidedly worse end-user outcome.&lt;/p&gt;
&lt;p&gt;Secondly, if any breach is as bad as any other breach, there is no
incentive to proactively mitigate damage from future breaches by
hardening internal systems. Why encrypt records, invest in access
control or keep sensitive information in a separate database with
extensive audit logging? It might materially impact end-user security,
but who cares -- all anyone is going to remember is that you got
popped.&lt;/p&gt;
&lt;p&gt;Finally, there's a subtle PR issue: how can the security industry
build deep relationships with clients when we publicly ridicule them
when the inevitable happens?&lt;/p&gt;
&lt;p&gt;These commentators have presumably not been the victims of a breach
themselves. I have trouble swallowing that anyone who's been through
the terrifying experience of being breached, seeing a breach up close
or even just witnessing a hairy situation being defused could air
those thoughts.&lt;/p&gt;
&lt;p&gt;If you haven't been the victim of an attack, and feel that your
security posture is keeping you from becoming one, consider this:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;What's your threat model?&lt;/li&gt;
&lt;li&gt;How confident are you in your estimation of the capabilities of
   attackers?&lt;/li&gt;
&lt;li&gt;Would you still be okay if your database became three orders of
   magnitude more valuable? Most personal data's value will scale
   linearly with the number of people affected, so if you're a small
   start-up with growth prospects, you'll either fail to execute, or
   be subject to that scenario.&lt;/li&gt;
&lt;li&gt;Would you still be okay if the attacker has a few 0-days?&lt;/li&gt;
&lt;li&gt;What if the adversary is a nation-state?&lt;/li&gt;
&lt;li&gt;How do you &lt;em&gt;know&lt;/em&gt; you haven't been breached?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;That brings me to my final thesis: I contest the claim that all of the
companies in the article didn't take security seriously. It is far
more probable that all of the companies cited in the article have
expended massive efforts to protect themselves, and, in doing so,
foiled many attacks. It's also possible that they haven't; but the
onus there is certainly on the accuser.&lt;/p&gt;
&lt;p&gt;Clearly, that's a weak form of disagreement, since "taking something
seriously" is entirely subjective. However, keep in mind that many
targets &lt;em&gt;actually&lt;/em&gt; haven't taken security seriously, and would not
even have the technical sophistication to detect an attack.&lt;/p&gt;
&lt;p&gt;(By the way, if you too would like to help materially improve people's
security, we're hiring. Contact me at &lt;code&gt;_@lvh.io&lt;/code&gt;.)&lt;/p&gt;&lt;/div&gt;</description><category>security</category><guid>https://www.lvh.io/posts/they-do-take-security-seriously.html</guid><pubDate>Sun, 05 Jul 2015 20:17:18 GMT</pubDate></item><item><title>HTTPS requests with client certificates in Clojure</title><link>https://www.lvh.io/posts/https-requests-with-client-certificates-in-clojure.html</link><dc:creator>lvh</dc:creator><description>&lt;div&gt;&lt;p&gt;The vast majority of TLS connections only authenticate the
server. When the client opens the connection, the server sends its
certificate. The client checks the certificate against the list of
certificate authorities that it knows about. The client is typically
authenticated, but over the inner HTTP connection, not at a TLS level.&lt;/p&gt;
&lt;p&gt;That isn't the only way TLS can work. TLS also supports authenticating
clients with certificates, just like it authenticates servers. This is
called mutually authenticated TLS, because both peers authenticate
each other. At Rackspace Managed Security, we use this for all
communication between internal nodes. We also operate our own
certificate authority to sign all of those certificates.&lt;/p&gt;
&lt;p&gt;One major library, &lt;a href="https://github.com/http-kit/http-kit"&gt;&lt;code&gt;http-kit&lt;/code&gt;&lt;/a&gt;, makes use of Java's
&lt;code&gt;javax.net.ssl&lt;/code&gt;, notably &lt;code&gt;SSLContext&lt;/code&gt; and &lt;code&gt;SSLEngine&lt;/code&gt;. These Java APIs
are exhaustive, and very... Java. While it's easy to make fun of these
APIs, most other development environments leave you using OpenSSL,
whose APIs are patently misanthropic. While some of these APIs do
leave something to be desired, &lt;a href="https://aphyr.com/"&gt;aphyr&lt;/a&gt; has done a lot of the
hard work of making them more palatable with
&lt;a href="https://github.com/aphyr/less-awful-ssl"&gt;&lt;code&gt;less-awful-ssl&lt;/code&gt;&lt;/a&gt;. That gives you an
&lt;code&gt;SSLContext&lt;/code&gt;. Request methods in &lt;code&gt;http-kit&lt;/code&gt; have an &lt;code&gt;opts&lt;/code&gt; map that
you can pass a &lt;code&gt;:sslengine&lt;/code&gt; object to. Given an &lt;code&gt;SSLContext&lt;/code&gt;, you just
need to do &lt;code&gt;(.createSSLEngine ctx)&lt;/code&gt; to get the engine object you want.&lt;/p&gt;
&lt;p&gt;Another major library, &lt;a href="https://github.com/dakrone/clj-http"&gt;&lt;code&gt;clj-http&lt;/code&gt;&lt;/a&gt;, uses lower-level
APIs. Specifically, it requires [&lt;code&gt;KeyStore&lt;/code&gt;][keystore] instances for
its &lt;code&gt;:key-store&lt;/code&gt; and &lt;code&gt;:trust-store&lt;/code&gt; options. That requires diving deep
into Java's cryptographic APIs, which, as mentioned before, might be
something you want to avoid. While &lt;code&gt;clj-http&lt;/code&gt; is probably the most
popular library, if you want to do fancy TLS tricks, you probably want
to use &lt;code&gt;http-kit&lt;/code&gt; instead for now.&lt;/p&gt;
&lt;p&gt;My favorite HTTP library is &lt;a href="http://aleph.io/"&gt;&lt;code&gt;aleph&lt;/code&gt;&lt;/a&gt; by
&lt;a href="http://ideolalia.com/"&gt;Zach Tellman&lt;/a&gt;.  It uses Netty instead of the usual Java IO
components. Fortunately, Netty's API is at least marginally friendlier
than the one in &lt;code&gt;javax.net.ssl&lt;/code&gt;. Unfortunately, there's no
&lt;code&gt;less-awful-ssl&lt;/code&gt; for Aleph. Plus, since I'm using &lt;a href="https://github.com/ptaoussanis/sente"&gt;&lt;code&gt;sente&lt;/code&gt;&lt;/a&gt; for
asynchronous client-server communication, which doesn't have support
for &lt;code&gt;aleph&lt;/code&gt; yet. So, I'm comfortably stuck with &lt;code&gt;http-kit&lt;/code&gt; for now.&lt;/p&gt;
&lt;p&gt;In conclusion, API design &lt;em&gt;is&lt;/em&gt; UX design. The library that "won" for
us was simply the one that was easiest to use.&lt;/p&gt;
&lt;p&gt;For a deeper dive in how TLS and its building blocks work, you should
watch my talk, &lt;a href="https://www.youtube.com/watch?v=3rmCGsCYJF8"&gt;Crypto 101&lt;/a&gt;, or the matching &lt;a href="https://www.crypto101.io"&gt;book&lt;/a&gt;. It's
free! Oh, and if you're looking for information security positions
(that includes entry-level!) in an inclusive and friendly environment
that puts a heavy emphasis on teaching and personal development, you
should get in touch with me at &lt;code&gt;_@lvh.io&lt;/code&gt;.&lt;/p&gt;&lt;/div&gt;</description><category>security</category><guid>https://www.lvh.io/posts/https-requests-with-client-certificates-in-clojure.html</guid><pubDate>Thu, 02 Jul 2015 15:53:20 GMT</pubDate></item><item><title>Conflicting threat models</title><link>https://www.lvh.io/posts/conflicting-threat-models.html</link><dc:creator>lvh</dc:creator><description>&lt;div&gt;&lt;p&gt;As I mentioned &lt;a href="http://www.lvh.io/posts/were-just-getting-started.html"&gt;in my previous post&lt;/a&gt;, we have a long way to go when it comes to information security. I'll be presenting &lt;a href="https://us.pycon.org/2015/schedule/presentation/342/"&gt;a talk on building secure systems&lt;/a&gt; at PyCon 2015 next month, and I hope to blog more about interesting bits of comprehensible security.&lt;/p&gt;
&lt;p&gt;I'm a strong believer in the importance of threat models. A threat model is your idea of what you're protecting against. It may seem obvious that you can't effectively protect anything without knowing what you're protecting it from. Sadly, simply contemplating your threat model puts you ahead of the curve in today's software industry.&lt;/p&gt;
&lt;p&gt;Threat models often simply deal with how much effort you're willing to spend to prevent something from happening. In a world with finite resources, we have to make choices. Some models are unrealistic or prohibitively expensive to defend against. These questions aren't all strictly technical: perhaps some risk is adequately covered by insurance. Perhaps you have a legal or a compliance requirement to do something, even if the result is technically inferior. These questions are also not just about &lt;em&gt;how much&lt;/em&gt; you're going to do: different threat models can lead to mutually exclusive resolutions, each a clear security win.&lt;/p&gt;
&lt;p&gt;Consider your smartphone. Our phones have a lot of important, private information; it makes sense to protect them. The iPhone 6 provides two options for the lock screen: a passcode and a fingerprint sensor. Passcodes have been around for about as long as smartphones have, while fingerprint sensors are new and exciting. It's clear that either of them is more secure than not protecting your phone at all. But which one is more secure?&lt;/p&gt;
&lt;p&gt;Most people instinctively feel the fingerprint sensor is the way to go. Biometric devices feel advanced; up until recently, they only existed in Hollywood. Fingerprints have their share of issues. It's impossible to pick a new key or have separate keys for separate capabilities; you're stuck with the keys you have. A fingerprint is like a password that you involuntarily leave on everything you touch. That said, turning a fingerprint into something that will unlock your iPhone is out of reach for most attackers.&lt;/p&gt;
&lt;p&gt;Passcodes aren't perfect either. People generally pick poor codes: important dates and years are common, but typically not kept secret in other contexts. If you know someone's birthday, there's a decent chance you can unlock their phone. At least with a passcode, you have the option of picking a good one. Even if you do, a passcode provides little protection against shoulder surfing. Most people unlock their phone dozens of times per day, and spend most of that day in the presence of other people. A lot of those people could see your passcode inconspicuously.&lt;/p&gt;
&lt;p&gt;Two options. Neither is perfect. How do you pick one? To make an informed choice, you need to formalize your threat models.&lt;/p&gt;
&lt;p&gt;In the United States, under the Fifth Amendment, you don't have to divulge information that might incriminate you. I am not a lawyer, and courts have provided conflicting rulings, &lt;a href="https://en.wikipedia.org/wiki/Fifth_Amendment_to_the_United_States_Constitution#Computer_passwords"&gt;but currently it appears that this includes computer passwords.&lt;/a&gt; &lt;a href="http://hamptonroads.com/2014/10/police-can-require-cellphone-fingerprint-not-pass-code?wpisrc=nl-swbd&amp;amp;wpmm=1#"&gt;However, a court has ruled that a fingerprint doesn't count as secret information.&lt;/a&gt; If you can unlock your phone with your fingerprint, they can force you to unlock it.&lt;/p&gt;
&lt;p&gt;If your threat models include people snooping, the fingerprint sensor is superior. If your threat model includes law enforcement, the passcode is superior. So, which do you pick? It depends on your threat model.&lt;/p&gt;
&lt;p&gt;Disclaimer: this is an illustration of how threat models can conflict. It is &lt;em&gt;not&lt;/em&gt; operational security advice; in which case I would point out other options. It is not legal advice, which I am not at all qualified to dispense.&lt;/p&gt;&lt;/div&gt;</description><category>security</category><guid>https://www.lvh.io/posts/conflicting-threat-models.html</guid><pubDate>Sat, 07 Mar 2015 16:56:05 GMT</pubDate></item><item><title>We're just getting started</title><link>https://www.lvh.io/posts/were-just-getting-started.html</link><dc:creator>lvh</dc:creator><description>&lt;div&gt;&lt;p&gt;Most conference talks are transactional. The speaker has a point to
make. After the presentation, it's "over"; only spoken about in
perfect tenses. You've communicated your thoughts, perhaps had a
conversation or two, but, mostly, moved on.&lt;/p&gt;
&lt;p&gt;I've given talks like these. However, about two years ago, I gave a
talk that had a deep impact on my life. That talk was Crypto 101.&lt;/p&gt;
&lt;p&gt;Right before the presentation, cryptanalytic research was released
that popped RC4. I couldn't have asked for a better setup. Turns out
it wasn't &lt;em&gt;just&lt;/em&gt; luck; eventually our systemic failure as an industry
in taking security seriously was bound to catch up with us. Since
then, the proverbial piper has been well-paid. We've seen a plethora
of serious security bugs. Huge corporations have been the victims of
attacks in the billions of dollars a pop. As I'm writing this blog
post, there's an article on a new TLS attack in my reading list.&lt;/p&gt;
&lt;p&gt;It quickly became clear that this wasn't just a one-off thing. I
started writing &lt;a href="http://crypto101.github.io"&gt;Crypto 101, the book,&lt;/a&gt; not too long after
giving the talk. We were, unwittingly, at the crest of a wave that's
still growing. Projects like PyCA and LibreSSL started fighting
tirelessly to make the software we use better. Security talks became a
mandatory part of the programming conference food pyramid. My friends
Hynek and Ying gave fantastic talks. They, too, got "lucky" with a
security bombshell: Heartbleed happened mere days before the
conference.&lt;/p&gt;
&lt;p&gt;Last week, I presented Crypto 101 again at rax.io, Rackspace's
internal conference. It was well-received, and I think I provided
value for people's time. One thing, more than anything, it
crystallized where we are. We're not done yet. There's still a huge
audience left to reach. Interest in information security has done
nothing but grow.  With a total of just over 100,000 downloads for the
book and about half as many for the recording of the presentation,
people are definitely listening. We've made real impact, and we have
people's attention, but we need to keep going.&lt;/p&gt;
&lt;p&gt;One of the two talks I'll be giving at PyCon is a more high-level
overview of how we can build secure systems. More friends of mine will
talk in about TLS there too. Within Rackspace, I'm focusing on
information security. There are awesome things brewing here, and I
hope that we can continue the great work we've been doing so far.&lt;/p&gt;
&lt;p&gt;We've accomplished a lot, but we're just getting started.&lt;/p&gt;&lt;/div&gt;</description><category>crypto</category><category>security</category><guid>https://www.lvh.io/posts/were-just-getting-started.html</guid><pubDate>Tue, 03 Mar 2015 01:06:41 GMT</pubDate></item><item><title>Securing APIs with shims</title><link>https://www.lvh.io/posts/securing-apis-with-shims.html</link><dc:creator>lvh</dc:creator><description>&lt;div&gt;&lt;p&gt;Imagine that you had a capability URL, except instead of giving you
the ability to perform a specific &lt;em&gt;action&lt;/em&gt;, it gave you the ability to
perform a (limited) set of operations on a third party API,
e.g. OpenStack. The capability URL wouldn't just be something you
exercise or revoke; it'd be an API endpoint, mostly indistinguishable
from the real API. Incoming requests would be inspected, and based on
a set of rules, either be rejected or forwarded to the API being
shimmed.&lt;/p&gt;
&lt;h2&gt;Proof of concept&lt;/h2&gt;
&lt;p&gt;At my day job, we had a programming task that I thought logic
programming would be well-suited for. Unfortunately, logic programming
is kind of weird and esoteric. Even programmers with otherwise broad
experiences professed to not being quite sure how it worked, or what
to do with it.&lt;/p&gt;
&lt;p&gt;Therefore, I used up my hack day (a day where we get to hack on random
projects) to cook up some cool stuff using logic programming. I demoed
the usual suspects (&lt;a href="https://github.com/lvh/shimmer/blob/master/src/shimmer/monkey.clj"&gt;the monkey with the banana&lt;/a&gt;, and a
&lt;a href="https://github.com/lvh/shimmer/blob/master/src/shimmer/sudoku.clj"&gt;sudoku solver&lt;/a&gt;), illustrating the difference between the
relational nature of the logic programs and the imperative nature of
the algorithms you might otherwise write to solve the same problems.
Finally, I demoed the aforementioned proxying API shim. The proof of
concept, codenamed shimmer, is &lt;a href="https://github.com/lvh/shimmer/"&gt;up on Github&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Let's take a look at the handler function, which takes incoming
requests and modifies them slightly so they can be passed on:&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kd"&gt;defn &lt;/span&gt;&lt;span class="nv"&gt;build-handler&lt;/span&gt;
  &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;target-host&lt;/span&gt; &lt;span class="nv"&gt;target-port&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
  &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;fn &lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;incoming-request&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;if &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;match&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;spy&lt;/span&gt; &lt;span class="nv"&gt;incoming-request&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
      &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;let &lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;modified-request&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;-&amp;gt; &lt;/span&gt;&lt;span class="nv"&gt;incoming-request&lt;/span&gt;
                                 &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;dissoc &lt;/span&gt;&lt;span class="ss"&gt;:scheme&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;;; hack&lt;/span&gt;
                                 &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;assoc &lt;/span&gt;&lt;span class="ss"&gt;:host&lt;/span&gt; &lt;span class="nv"&gt;target-host&lt;/span&gt;
                                        &lt;span class="ss"&gt;:port&lt;/span&gt; &lt;span class="nv"&gt;target-port&lt;/span&gt;
                                        &lt;span class="ss"&gt;:throw-exceptions&lt;/span&gt; &lt;span class="nv"&gt;false&lt;/span&gt;&lt;span class="p"&gt;))]&lt;/span&gt;
        &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;spy&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;request&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;spy&lt;/span&gt; &lt;span class="nv"&gt;modified-request&lt;/span&gt;&lt;span class="p"&gt;))))&lt;/span&gt;
      &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="ss"&gt;:status&lt;/span&gt; &lt;span class="mi"&gt;403&lt;/span&gt; &lt;span class="c1"&gt;;; Forbidden&lt;/span&gt;
       &lt;span class="ss"&gt;:headers&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s"&gt;"content-type"&lt;/span&gt; &lt;span class="s"&gt;"text/plain"&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
       &lt;span class="ss"&gt;:body&lt;/span&gt; &lt;span class="s"&gt;"Doesn't match!"&lt;/span&gt;&lt;span class="p"&gt;})))&lt;/span&gt;
&lt;/pre&gt;


&lt;p&gt;(Those &lt;code&gt;spy&lt;/code&gt; calls are from the excellent &lt;a href="https://github.com/ptaoussanis/timbre"&gt;&lt;code&gt;timbre&lt;/code&gt;&lt;/a&gt;
library. They make it easy to log values without cluttering up your
code; a godsend while developing with some libraries you're not
terribly familiar with.)&lt;/p&gt;
&lt;p&gt;The matching function looks like this:&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kd"&gt;defn &lt;/span&gt;&lt;span class="nv"&gt;match&lt;/span&gt;
  &lt;span class="s"&gt;"Checks if the request is allowed."&lt;/span&gt;
  &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;req&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
  &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;not= &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;l/run&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;q&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
          &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;l/conde&lt;/span&gt;
           &lt;span class="p"&gt;[(&lt;/span&gt;&lt;span class="nf"&gt;l/featurec&lt;/span&gt; &lt;span class="nv"&gt;req&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="ss"&gt;:request-method&lt;/span&gt; &lt;span class="ss"&gt;:get&lt;/span&gt;&lt;span class="p"&gt;})]&lt;/span&gt;
           &lt;span class="p"&gt;[(&lt;/span&gt;&lt;span class="nf"&gt;l/featurec&lt;/span&gt; &lt;span class="nv"&gt;req&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="ss"&gt;:request-method&lt;/span&gt; &lt;span class="ss"&gt;:post&lt;/span&gt;
                             &lt;span class="ss"&gt;:headers&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s"&gt;"x-some-header"&lt;/span&gt;
                                       &lt;span class="s"&gt;"the right header value"&lt;/span&gt;&lt;span class="p"&gt;}})]&lt;/span&gt;
           &lt;span class="p"&gt;[(&lt;/span&gt;&lt;span class="nf"&gt;l/featurec&lt;/span&gt; &lt;span class="nv"&gt;req&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="ss"&gt;:request-method&lt;/span&gt; &lt;span class="ss"&gt;:post&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;
            &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;l/featurec&lt;/span&gt; &lt;span class="nv"&gt;req&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="ss"&gt;:headers&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s"&gt;"x-some-header"&lt;/span&gt;
                                       &lt;span class="s"&gt;"another right header value"&lt;/span&gt;&lt;span class="p"&gt;}})]))&lt;/span&gt;
        &lt;span class="o"&gt;'&lt;/span&gt;&lt;span class="p"&gt;()))&lt;/span&gt;
&lt;/pre&gt;


&lt;h2&gt;Future work&lt;/h2&gt;
&lt;p&gt;Make this thing actually vaguely correct. That means e.g. also
inspecting the body for URL references, and changing those to go
through the proxy as well.&lt;/p&gt;
&lt;p&gt;Start collecting a library of short hand notations for specific API
functionality, e.g. if you're proxying an OpenStack API, you should be
able to just say you want to allow server creation requests, without
having to figure out exactly what those requests look like.&lt;/p&gt;
&lt;p&gt;The spec is hard-coded, it should be specified at runtime. That was
trickier than I had originally anticipated: the vast majority of
&lt;code&gt;core.logic&lt;/code&gt; behavior uses macros. While some functionality is fairly
easy to port, that's probably a red herring: I don't want to port a
gazillion macros. As an example, here's &lt;code&gt;conds&lt;/code&gt;, which is just&lt;code&gt;conde&lt;/code&gt;
as a function (except without support for logical conjunction per
disjunctive set of goals):&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kd"&gt;defn &lt;/span&gt;&lt;span class="o"&gt;^&lt;/span&gt;&lt;span class="ss"&gt;:private&lt;/span&gt; &lt;span class="nv"&gt;conds&lt;/span&gt;
  &lt;span class="s"&gt;"Like conde, but a function."&lt;/span&gt;
  &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;goals&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
  &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;if &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;empty?&lt;/span&gt; &lt;span class="nv"&gt;goals&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="nv"&gt;l/fail&lt;/span&gt;
    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;l/conde&lt;/span&gt; &lt;span class="p"&gt;[(&lt;/span&gt;&lt;span class="nb"&gt;first &lt;/span&gt;&lt;span class="nv"&gt;goals&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
             &lt;span class="p"&gt;[(&lt;/span&gt;&lt;span class="nf"&gt;conds&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;rest &lt;/span&gt;&lt;span class="nv"&gt;goals&lt;/span&gt;&lt;span class="p"&gt;))])))&lt;/span&gt;
&lt;/pre&gt;


&lt;p&gt;That's not the worst function, but let's just say I see a lot of
&lt;code&gt;macroexpand&lt;/code&gt; in my future if I'm going to take this seriously.&lt;/p&gt;
&lt;p&gt;URLs and bodies should be parsed, so that you can write assertions
against structured data, or against URL patterns, instead of specific
URLs.&lt;/p&gt;
&lt;p&gt;If I ever end up letting any of this be a serious part of my day job,
I'm going to invest a ton of time improving the documentation for both
&lt;code&gt;core.logic&lt;/code&gt; and &lt;code&gt;core.typed&lt;/code&gt;. They're &lt;em&gt;fantastic&lt;/em&gt; projects, but
they're harder to get started with than they could be, and that's a
shame.&lt;/p&gt;&lt;/div&gt;</description><category>crypto</category><category>security</category><guid>https://www.lvh.io/posts/securing-apis-with-shims.html</guid><pubDate>Sat, 21 Feb 2015 06:24:03 GMT</pubDate></item><item><title>On discussing software security improvements</title><link>https://www.lvh.io/posts/on-discussing-software-security-improvements.html</link><dc:creator>lvh</dc:creator><description>&lt;div&gt;&lt;p&gt;A common criticism of information security folks is that they tend to
advise people to not do any crypto. Through projects like &lt;a class="reference external" href="https://www.crypto101.io/"&gt;Crypto
101&lt;/a&gt;, I've attempted to make a small contribution towards fixing
that.&lt;/p&gt;
&lt;p&gt;In the open source world, various people often try to improve the
security of a project. Because designing secure systems is pretty
hard, they often produce flawed proposals. The aforemetioned tendency
for infosec-conscious people to tell them to stop doing crypto is
experienced as unwelcoming, even dismissive. Typically, the only thing
that's accomplished is that a lot of feelings get hurt; it seems to
only rarely result in improved software.&lt;/p&gt;
&lt;p&gt;I think that's quite unfortunate. I think open source is great, and we
should be not just welcoming and inclusive, but aiming to produce
secure software. Furthermore, even if a flawed proposal is
unsalvageable, a clear description of &lt;em&gt;why&lt;/em&gt; it is flawed will
presumably result in fewer negative interactions. Best case scenario,
the issues with a proposal can be discussed and potentially rectified.&lt;/p&gt;
&lt;p&gt;In an effort to improve this situation, I'm documenting what I believe
to be a useful way to discuss security changes and their tradeoffs. As
Zooko has taught me:&lt;/p&gt;
&lt;blockquote&gt;
Security isn't about perfect versus imperfect or about better versus
worse, it's about &lt;em&gt;this&lt;/em&gt; attack surface versus &lt;em&gt;that&lt;/em&gt; attack
surface.&lt;/blockquote&gt;
&lt;p&gt;This document aims to be the equivalent of an &lt;a class="reference external" href="http://www.sscce.org/"&gt;SSCCE&lt;/a&gt; for generic bug
reports: a blueprint for making suggestions likely to lead to
productive discourse, as long as we can agree that we're trying to
produce more secure software, as well as provide a welcoming
development environment.&lt;/p&gt;
&lt;div class="section" id="important-points"&gt;
&lt;h2&gt;Important points&lt;/h2&gt;
&lt;p&gt;A good proposal should contain:&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;A brief description of what you're suggesting.&lt;/li&gt;
&lt;li&gt;A description of the attack model you're considering, why the
current system does not address this issue, and why the suggested
system &lt;em&gt;does&lt;/em&gt; address this issue.&lt;/li&gt;
&lt;li&gt;A motivation of the attack model. Why is it important that this
issue is actually addressed?&lt;/li&gt;
&lt;li&gt;How does this change affect the attack surface (i.e. all of the
ways an attacker can attempt to attack a system)?&lt;/li&gt;
&lt;li&gt;What does the user experience for all users of the system look
like? Many cryptosystems fall over because they're simply unusable
for most users of the system.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div class="section" id="an-example"&gt;
&lt;h2&gt;An example&lt;/h2&gt;
&lt;p&gt;Wul (the widely underestimated language, pronounced &lt;em&gt;/wool/&lt;/em&gt;) is a
general purpose programming language. It has a package repository,
WuPI (the Wul package index, pronounced &lt;em&gt;/woopie/&lt;/em&gt;), the de facto
standard for distributing and installing Wul software.&lt;/p&gt;
&lt;p&gt;WuPI uses TLS as a secure transport. The WuF (Wul foundation,
pronounced &lt;em&gt;/woof/&lt;/em&gt;), maintains a root certificate, distributed with
Wul. Thanks to a well-managed system of intermediary CAs run by a
tireless army of volunteers, this means that both package authors and
consumers know they're talking to the real WuPI.&lt;/p&gt;
&lt;p&gt;Alice is the WuPI BDFL. Bob is a Wul programmer, and would like to
improve the security of WuPI.&lt;/p&gt;
&lt;p&gt;While consumers and authors know that they're talking to the real
WuPI, there is no protection against a malicious WuPI endpoint. (This
problem was recently made worse because WuPI introduced a CDN, greatly
increasing the number of people who could own a node.). You know that
you're talking to something with a WuF-signed certificate (presumably
WuPI, provided the WuF has done a good job managing that certificate),
but you have no idea if that thing is being honest about the packages
it serves you.&lt;/p&gt;
&lt;p&gt;Bob believes WuPI could solve this by using GPG signatures.&lt;/p&gt;
&lt;p&gt;He starts with a brief description of the suggestion:&lt;/p&gt;
&lt;blockquote&gt;
I would like to suggest that WuPI grows support for GPG signatures
of packages. These signatures would be created when a package author
uploads a package. They would optionally be verified when the user
downloads a package.&lt;/blockquote&gt;
&lt;p&gt;He continues with the attack model being considered:&lt;/p&gt;
&lt;blockquote&gt;
I believe this would secure WuPI consumers against a malicious WuPI
endpoints. A malicious WuPI endpoint (assuming it acquires an
appropriate certificate) is currently free to deliver whatever
packages it wants.&lt;/blockquote&gt;
&lt;p&gt;He explains why the current model doesn't address this:&lt;/p&gt;
&lt;blockquote&gt;
The current system assures authenticity and secrecy of the stream
(through TLS), and it ensures that the server authenticates itself
with a WuPI/WuF certificate. It does not ensure that the package is
what the author uploaded.&lt;/blockquote&gt;
&lt;p&gt;He explains why he believes his model does address this:&lt;/p&gt;
&lt;blockquote&gt;
Because the signatures are produced by the author's GPG key, a
malicious WuPI endpoint would not be able to forge them. Therefore,
a consumer is sure that a package with a valid signature is indeed
from the author.&lt;/blockquote&gt;
&lt;p&gt;He explains why this attack model is important:&lt;/p&gt;
&lt;blockquote&gt;
With the new CDN support, the number of people with access to such a
certificate has greatly increased. While I certainly trust all of
the volunteers involved, it would be nice if we didn't &lt;em&gt;have&lt;/em&gt; to.
Furthermore, the software on the servers can always be vulnerable to
attack; as a high-value target, it certainly isn't inconceivable
that an attacker would use an unknown vulnerability to take over a
WuPI endpoint.&lt;/blockquote&gt;
&lt;p&gt;He (believes to) address the attack surface:&lt;/p&gt;
&lt;blockquote&gt;
Because the signatures are optional, the attack surface remains the
same.&lt;/blockquote&gt;
&lt;p&gt;Finally, he addresses the user experience:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The weak point of this scheme is most likely the user experience,
because users historically seem to dislike using GPG.&lt;/p&gt;
&lt;p&gt;I am hopeful that this increased value of participating in the GPG
web of trust will mean that more people participate.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Alice reviews this, and notes a flaw in the proposal:&lt;/p&gt;
&lt;blockquote&gt;
This proposal aims to address a security flaw when the WuPI endpoint
is malicious by adding signatures. However, a malicious WuPI
endpoint can lie by omission, and claim a package was never signed
by the author.&lt;/blockquote&gt;
&lt;p&gt;Bob now realizes this issue, and suggests an improvement:&lt;/p&gt;
&lt;blockquote&gt;
This could be rectified if the user insists on a signature for
packages they expect to be signed.&lt;/blockquote&gt;
&lt;p&gt;As a side note, Alice notes that the attack surface does increase:&lt;/p&gt;
&lt;blockquote&gt;
This places trust in author's ability to manage private keys, which
has historically been shown to be problematic. That introduces a new
attack vector: an attacker can attempt to go after the author's
private key.&lt;/blockquote&gt;
&lt;p&gt;Regardless of the outcome of this conversation, there actually was a
conversation. I believe this to be an improvement over the overall
status quo.&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;</description><category>crypto</category><category>python</category><category>security</category><guid>https://www.lvh.io/posts/on-discussing-software-security-improvements.html</guid><pubDate>Tue, 29 Jul 2014 06:58:27 GMT</pubDate></item></channel></rss>